[
["index.html", "Lecture notes for M362M: Introduction to Stochastic Processes Preface", " Lecture notes for M362M: Introduction to Stochastic Processes Gordan Zitkovic last updated - 2020-08-12 Preface This is an always-evolving set of lecture notes for Introduction to Stochastic Processes (M362M). It should start with me explaining what stochastic proceses are. Instead, here is a list of several questions you will be able to give answers to upon completion of this course. Question 1 In a simplistic model, the price of a share of a stock goes either up or down by \\(\\$1\\) each day, with probability \\(1/2\\). You own a single share whose value today is \\(\\$100\\), so that its tomorrow’s price will be \\(\\$101\\) or \\(\\$99\\) with probability \\(1/2\\), etc. Your strategy is to hold onto your share until one of the following two things happen: you go bankrupt (the stock price hits \\(0\\)), or you make a \\(\\$50\\) dollar profit (the stock price hits \\(\\$150\\).) How likely is it that you will make a profit before you go bankrupt? How long will it take? Is it possible that it takes forever, i.e., that the stock price hovers beween \\(\\$1\\) and \\(\\$149\\) forever? Question 2. A person carying a certain disease infects a random number of people in a week, and then stops being infectious. Each of the infected people transmits the disease in the same way, etc. Suppose that the number of people each (infectious individual) infects is either \\(0\\), \\(1\\) or \\(2\\) or \\(3\\), each with probability \\(1/4\\); different infectious individuals may infect different number of people and behave independently of each other. What is the probability that the disease will every be eradicated? What is the probability that every single individual in the population of \\(300,000,000\\) will eventually be infected? Question 3. In a game of tennis, Player \\(1\\) wins against Player \\(2\\) in each rally (the smallest chunk of the match that leads to point, i.e., to a score change from \\(15-30\\) to \\(30-30\\), for example) with probability \\(p\\). What is the probability that Player \\(1\\) wins a game (the chunk of the match that leads to a score change such as \\(5-3\\) to \\(6-3\\) within a set)? a set? the entire match? Is the game of tennis set up in such a way that is amplifies or reduces the difference in skill between players? Question 4. A knight starts in the lower left corner of the chess board and starts moving ``randomly’’. That means that from any position, it chooses one of the possible (legal) moves and takes it, with all legal moves having the same probability. It keeps doing the same thing until it comes back to the square it started from. What is the expected number of moves the knight will make before it returns to “square one”? How about the same problem, but using a different chess piece? Which one do you think will come back is the smallest (expected) number of steps? (*) How about the same problem, but until all squares have been visited at least once? Question 5. How does Google search work? "],
["intro.html", "Chapter 1 An intro to R and RStudio 1.1 Setting up an R environment on your computer 1.2 Learning the very basics of R 1.3 R Notebooks or How to submit your HW assignments and exams 1.4 Problems", " Chapter 1 An intro to R and RStudio 1.1 Setting up an R environment on your computer Learning basic R is an important part of this course, and the first order of business is to download and install an R distribution on your personal computer. We will be using RStudio as an IDE (integrated development environment). Like R itself, it is free and readily available for all major platforms. To download R to your computer, go to https://cloud.r-project.org. Then, go to https://rstudio.com/products/rstudio/download/ to get RStudio. There are several versions to choose from - the one your are looking for is “RStudio desktop - Free”. 1.2 Learning the very basics of R Once R and RStudio are on your computer, it is time to get acquainted with the basics of R. This class is not about the finer points of R itself, and I will try to make your R experience as smooth as possible. After all, R is a tool that will help us explore and understand stochastic processes. Having said (written) that, it is important to realize that R is a powerful programming language specifically created for statistical and probabilistic applications. Some knowledge of R is a valuable skill to have in today’s job market, and you should take this opportunity to learn it. The best way, of course, is by using it, but before you start, you need to know the very basics. Don’t worry, R is very user friendly and easy to get started in. In addition, it has been around for a long time (its predecessor S appeared in 1976) and is extremely well documented (just google introduction to R or something like that). I like all books written by R guru Hadley Wickham. In particular, R for Data Science features a nice and elementary introduction to R (albeit from the point of view of a data scientist). If you want to learn more, check the webpage of the bookdown package. It is a tool for writing books which easily integrates R computation, and its webpage features a list of books on R and other topics (including many of Wickham’s books) - they are all free. Btw, this very set of lecture notes was written in Rmarkdown using the bookdown package! 1.3 R Notebooks or How to submit your HW assignments and exams This is also a good time to explain how to submit your work. All submissions of computational work have to be produced using an R Notebook. There is nothing to worry - it is probably the most painless way, both for the student and for the grader. Let me explain. An R notebook is nothing other than an enhanced text file, but with the ability to intersperse so-called chunks or R code with regular text. It can also be transformed into a good looking pdf file by a press of a button. When you open an R notebook in Rstudio, you will be able to write your problem solutions, perform R computations and even plot graphs in one fell swoop. To create an R notebook, go to File -&gt; New File -&gt; R Notebook in RStudio, and a shiny new Rmd file will appear in the editor pane. It will even have text already filled in with hints about text formatting. You should not worry about text markup, or how to produce hyperlinks, or anything like that, though. In fact, the only important thing to know is that the following notation ```{r} ``` ```` marks a code chunk. Btw, the apostrophe-like symbols above are backticks - they live on the key below the escape key. You should not worry about counting how many there are or how to type them in - pressing Cmd+Option+I (on a Mac) or Ctrl+Alt+I (on Windows or Linux) will insert the whole thing for you. To see all available keyboard shortcuts available on your platform, press Option+Shift+K (on a Mac) or Alt+Shift+K (on Windows or Linux) in RStudio. To get more information about a typical workflow (how to run code chunks in place, for example) make sure to check the the following short book chapter (R notebooks)[https://bookdown.org/yihui/rmarkdown/notebook.html] - feel free to skip sections 3.2.2 and 3.2.3. Anything between ```{r} and the closing ``` is treated as R code and that you can evaluate on the spot. The result of the evaluation will automatically become a part of the notebook. Code chunks can also be used to produce graphs, and by simply evaluating it, R will embed the graph into your document. For those of you familiar with the Python ecosystem, Rmd documents are R’s version of Jupyter notebooks. After your R notebook is ready to be submitted, simply upload the file itself. Do not convert is into a pdf, html or any other format. The extension of an R-notebook file is .Rmd, and only .Rmd files will be accepted by the system. Your submissions should be structured as follows: the top should look like this (with obvious substitutions). It is important to include your EID, as well as the course number and the semester. --- title: &quot;HW 7; M362M, Fall 2020&quot; author: &quot;Jane B. Student, eid:jbs123 &quot; date: &quot;2020-10-14&quot; --- Each problem needs to start with the problem number preceeded by ##, and each part of a problem (e.g., if there are subquestions a,b,c or 1., 2., 3) needs to start with ### ## Problem 7.23 ### Part b. I computed the sum of 2 and 3 by running the following code ```{r} 2+3 ``` That is it. A sample Rmd file named HW_template.Rmd with solutions to parts of some of the problems below can be downloaded here. Theoretical problems (i.e., those that do not involve any computation in R) can be submitted in a separate document (hand written and then scanned into a pdf, for example). If you know some LaTeX, you can use an R notebook and simply type the formulas in LaTeX notation. This way, if the assignment comes with both computational and theoretical problems, you can submit both in a single file. I am not assuming that you do know LaTeX and I am not going to require you to learn it, but if you do, use it to make your submissions nicer and easier to read (the grader will thank you!). 1.4 Problems Here are several simple problems. Their goal is to give you an idea of exactly how much R is required to get started in this course. Virtually any “introduction to R” should contain enough information to get you through all of these. Problem 1.1 Compute the following (your answer should be a decimal number): \\(1/238746238746\\) \\(2^{45}\\) \\(3^{28}\\) \\(\\sqrt{15}\\) \\(\\cos(\\pi/8)\\) \\(e^2\\) \\(\\log(2)\\) (the base is \\(e\\)) \\(\\log_{10}(2)\\) (the base is \\(10\\)) \\(\\sqrt[3]{ \\frac{1342.16-2.18}{(3 \\pi +4.12)^2}}\\) Note: some of the answers will look like this 3.14e+13. If you do not know what that means, google E notation. Problem 1.2 1. Define two variables \\(a\\) and \\(b\\) with values \\(3\\) and \\(4\\) and “put” their product into a variable called \\(c\\). Output the value of \\(c\\). Define two vectors \\(x\\) and \\(y\\) of length \\(3\\), such that the components of \\(x\\) are \\(1,2,3\\) and the components of \\(y\\) are \\(8,9,0\\). Ouput their (componentwise) sum. Define a \\(2\\times 2\\) matrix \\(A=\\begin{pmatrix} 1 &amp; 2 \\\\ -1 &amp; 3 \\end{pmatrix}\\). Use the command matrix and make sure to understand how it works; you can access its documentation directly from \\(R\\) by writing ?matrix. Make sure you understand what the parameters ncol, nrow and byrow do. Compute the matrix square \\(A^2\\) (Careful! A*A will output a \\(2\\times 2\\) matrix, but it will not be \\(A^2\\) - it will be the matrix whose entries are the squares of the entries of \\(A\\). You need to use different notation.) Problem 1.3 Write a function that takes an argument \\(x\\) and returns \\(5\\) if \\(x\\geq 5\\) and \\(x\\) itself otherwise. Write a function that returns TRUE (a boolean value) if its argument is between \\(2\\) and \\(3\\) and FALSE otherwise. Problem 1.4 Construct a vector \\(x\\) which contains all numbers from \\(1\\) to \\(100\\). Construct a vector \\(y\\) which contains squares of all numbers between \\(20\\) and \\(30\\). Construct a vector \\(z\\) the same size as \\(y\\) with boolean entries (i.e., TRUE or FALSE) that indicates whether the entry at the same position in \\(y\\) is above \\(600\\) or not. (This is much easier to do than it sounds.) "],
["simulation-of-random-variables-and-monte-carlo.html", "Chapter 2 Simulation of Random Variables and Monte Carlo 2.1 How to simulate some common probability distributions 2.2 Monte Carlo 2.3 Conditional probability and independence.", " Chapter 2 Simulation of Random Variables and Monte Carlo In the spirit of “learn by doing”, these lecture notes are composed mostly of problems. Those with solutions usually introduce new concepts and feature a Comments section right after the solution. These comments are subdivided into R and Math comments focusing on the computational or coneceptual features, respectively. Note that you are not expected to be able to do the solved problems before reading their solutions and comments, so don’t worry if you cannot. It is a good practice to try, though. Problems unsolved in these notes, however, do not feature any new ideas and are there to help you practice the skills presented before. 2.1 How to simulate some common probability distributions Problem 2.1 ‘’Draw’’ 50 simulations from the geometric distribution with parameter \\(p=0.4\\). Solution: rgeom(50,prob=0.4) #R&gt; [1] 1 0 3 4 1 2 0 0 2 2 0 1 5 0 1 0 2 1 1 0 2 2 2 1 0 0 1 3 2 2 1 1 1 3 5 0 1 1 #R&gt; [39] 0 0 0 1 2 0 1 1 1 0 1 0 Comments R: R makes it very easy to simulate draws from one of the named distributions, such as geometric, binomial, uniform, normal, etc. For a list of all available distributions, run help(&quot;distributions&quot;) Each available distribution has an R name; the uniform is unif the normal is norm and the binomial is binom, etc. If you want to simulate \\(n\\) draws (aka a sample of size \\(n\\)) from a distribution, you form a full command by appending the letter r to its R name and use \\(n\\) as an argument. That is how we arrived to rgeom(50) in the solution above. The additional arguments of rgeom have to do with the parameters of that distribution. Which parameters go with which distributions, and how to input them as arguments to rgeom or rnorm is best looked up in R’s extensive documentation. Try help(&quot;rnorm&quot;), for example. Math: You might have encountered a geometric distribution before. A random variable with that distribution can take any positive integer value or \\(0\\). As you can see from the output above, the value \\(0\\) appears more often than the value \\(3\\), and the value \\(23\\) does not appear at all. The probability of seeing the value \\(k\\in \\{0,1,2,3,\\dots\\}\\) as a result of a single draw is given by \\((1-p)^k p\\), where \\(p\\) is called the parameter of the distribution. That corresponds to the following interpretation of the geometric distribution: keep tossing a biased coin (with probability \\(p\\) of obtaining H) until you see the first H; the number of tosses yielding T before that is your geometric random variable. If we put these probabilities in a single table (and choose \\(p=0.4\\), for example) it is going to look like this: 0 1 2 3 4 5 6 7 … Prob. 0.4 0.24 0.144 0.086 0.052 0.031 0.019 0.011 … Of course, the possible values our random variable can take do not stop at \\(7\\). In fact, there are infinitely many possible values, but we do not have infinite space. Note that, even though, the value \\(17\\) does not appear in the output of the command rgeom above, it probably would if we simulated more than \\(50\\) values. Let’s try it with \\(500\\) draws: X &lt;- rgeom(500, prob = 0.4) knitr::kable(t(table(X))) 0 1 2 3 4 5 6 7 8 9 10 208 132 62 43 23 16 8 3 2 1 2 Still no luck, but we do observe values above 5 more often. By trial and error, we arrive at about \\(1,000,000\\) as the required number of simulations: 0 1 2 3 … 23 24 25 26 400616 238946 144274 86489 … 3 3 3 3 Problem 2.2 Compute the probability that among \\(1,000,000\\) draws of a geometric random variable with parameter \\(p=0.4\\), we never see a number greater than \\(22\\). Solution: First, we compute the probability that the value seen in a single draw does not exceed \\(22\\): pgeom(22,prob=0.4) #R&gt; [1] 0.9999921 Different draws are independent of each other, so we need to raise this to the power \\(1,000,000\\). (pgeom(22,prob=0.4))^(1000000) #R&gt; [1] 0.0003717335 Comments: R. The command we used here is pgeom which is a cousin of rgeom. In general R commands that involve named probability distributions consist of two parts. The prefix, i.e., the initial letter (p in this case) stands for the operation you want to perform, and the rest is the R name of the distribution. There are 4 prefixes, and the commands they produce are Prefix Description r Simulate random draws from the distribution. p Compute the cumulative probability distribution function (cdf) d Compute the probability density (pdf) or the probability mass function (pmf) q Compute the quantile function (see the Math section below for the reminder of what these things are. ) In this problem, we are dealing with a geometric random variable \\(X\\), which has a discrete distribution, with support \\(0,1,2,3,\\dots\\). Therefore, the R name is geom. We are interested in the probability \\({\\mathbb{P}}[ X\\leq 22]\\), which corresponds to the cdf of \\(X\\) at \\(x=22\\), so we use the the prefix p. Finally, we used the named parameter p and gave it the value p = 0.4, because the geometric distribution has a single parameter \\(p\\). This problem also gives us a chance to discuss precision. As you can see, the probability of a single draw not exceeding \\(22\\) is very close to \\(1\\). In fact, it is equal to it to 5 decimal places. By default, R displays 7 significant digits of a number. That is enough for most applications, but sometimes we need more precision. For example, let’s try to compute the probability of seeing no T (tails) in 10 tosses of a biased coin, where the probability of H (heads) is 0.9. 1-0.1^10 #R&gt; [1] 1 While small, this probability is clearly not equal to \\(1\\), as suggested by the output above. The culprit is the default precision. We can increase the precision (up to \\(22\\) digits) by running options(digits=14) options(digits=17) 1-0.1^10 #R&gt; [1] 0.99999999989999999 Problems like this should not appear in this course, but they will out there “in the wild”, so it might be a good idea to be aware of them. Math. If you forgot all about pdfs, cdfs and such things here is a little reminder: And here is a reminder what these quantities are: cdf \\(F(x) = {\\mathbb{P}}[X\\leq x]\\) pdf \\(f(x)\\) such that \\(\\int_a^b f(x) \\, dx = {\\mathbb{P}}[X \\in [a,b]]\\) for all \\(a&lt;b\\) pmf \\(p(x)\\) such that \\({\\mathbb{P}}[X=a_n] = p(a_n)\\) for some sequence \\(a_n\\) qf \\(q(p)\\) is a number such that \\({\\mathbb{P}}[ X \\leq q(p)] = p\\) Those random variables that admit a pdf are called continuous. The prime examples are the normal, or the exponential distribution. The ones where a pmf exists are called discrete. The sequence \\(a_n\\) is simply the sequence of all values that such a, discrete, random variable can take. Most often, \\(a_n\\) is either the set of all natural numbers \\(0,1,2,\\dots\\) or a finite subset such that \\(0,1,2,3,4,5\\), called the support of the distribution. Btw, this probability we obtained is quite small. Since \\(1/0.000372\\) is about \\(2690\\), we would have to run about \\(2690\\) rounds of \\(1,000,000\\) simulations before the largest number falls below \\(23\\). Problem 2.3 Compute the \\(0.05\\), \\(0.1\\), \\(0.4\\), \\(0.6\\) and \\(0.95\\) quantiles of the normal distribution with mean \\(1\\) and standard deviation \\(2\\). Solution: qnorm( c(0.05, 0.1, 0.4, 0.6, 0.95), mean = 1, sd = 2) #R&gt; [1] -2.2897073 -1.5631031 0.4933058 1.5066942 4.2897073 R. The function we used is qnorm, with the prefix q which computes the quantile function and the R name norm because we are looking for the quantiles of the normal distribution. The additinal (named) parameters are where the parameters of the distribution come in (the mean and the standard variation) in this case. Note how we plugged in the entire vector c(0.05, 0.1, 0.4, 0.6, 0.98) instead of a single value into qnorm. You can do that because this function is vectorized. That means that if you give it a vector as an argument, it will return a vector with values corresponding to each element of the input. Many (but not all) functions in R are vectorized. As a sanity check, let’s apply the cdf to these quantile values: p &lt;- qnorm( c(0.05, 0.1, 0.4, 0.6, 0.95), mean = 1, sd = 2) pnorm( p , mean = 1, sd = 2) #R&gt; [1] 0.05 0.10 0.40 0.60 0.95 As expected, we got the original values back - the normal quantile function and its cdf are inverses of each other. Problem 2.4 Simulate \\(60\\) throws of a fair \\(10\\)-sided die. Solution: sample( 1:10 , 60, replace = TRUE) #R&gt; [1] 2 8 9 8 4 7 7 7 2 3 3 10 6 1 9 7 4 7 6 2 2 3 10 1 9 #R&gt; [26] 7 3 2 8 4 1 2 8 1 4 9 1 9 10 10 6 1 8 6 1 10 5 1 6 9 #R&gt; [51] 8 3 8 9 4 6 1 6 7 8 Comments: Math. Let \\(X\\) denote the outcome of a single throw of this strange die. The distribution of \\(X\\) is discrete (it can only take the values \\(1,2,\\dots, n\\)) but it is not one of the named distributions. The way we describe such distribution is by a distribution table, which is really just a list of possible values a random variable can take, together with their, respective, probabilities. 1 2 3 4 5 6 7 8 9 10 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 R. The command used to draw a sample from a (finite) collection is, of, course sample. The first argument is a vector, and it contains the “bag” from which you are drawing. If we are interested in repeated, random samples, we also need to specity replace = FALSE otherwise, you could draw any signle number am most once: sample(1:10, 8, replace = FALSE) #R&gt; [1] 1 5 6 7 8 10 3 4 With more than 10 draws, we would run out of numbers to draw: sample(1:10, 12, replace = FALSE) #R&gt; Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39; The bag you draw from can contain objects other than numbers: sample( c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;), 8, replace = TRUE) #R&gt; [1] &quot;Charlie&quot; &quot;Bob&quot; &quot;Alice&quot; &quot;Charlie&quot; &quot;Bob&quot; &quot;Charlie&quot; &quot;Charlie&quot; #R&gt; [8] &quot;Charlie&quot; So far, each object in the bag had the same probability of being drawn. You can use the sample command to produce a weighted sample, too. For example, if we wanted to simulate \\(10\\) draws from the following distribution 1 2 3 0.2 0.7 0.1 we would use the additional argument prob: sample( c(1,2,3), 10, replace = TRUE, prob = c(0.2,0.7, 0.1)) #R&gt; [1] 2 2 2 2 1 2 2 2 2 1 Note how it is mostly \\(2\\)s. Problem 2.5 Draw a sample of \\(n=10\\) from the normal distribution with parameters \\(\\mu=1\\), \\(\\sigma = 2\\). Plot a historgram of the obtained values. Repeat for \\(n=100\\) and \\(n=100000\\). Solution: x = rnorm(10, mean = 1, sd = 2) hist(x) x = rnorm(100, mean = 1, sd = 2) hist(x) x = rnorm(100000, mean = 1, sd = 2) hist(x) Comments: R. It cannot be simpler! You use the command hist, feed it a vector of values, and it produces a histogram. It will even labels the axes. If you want to learn how to tweak various features of your histogram, type ?hist. Esthetically, the built-in histograms leave something to be desired. We can do better, using the package ggplot2. You don’t have to use it in this class, but if you want to, you install it first by running install.packages(&quot;ggplot2&quot;) (you have to do this only once). Then, every time you want to use it, you run library(ggplot2) to notify R that you are aobut to use a function from that package. It would take a whole semester to learn everything there is to know about ggplot2; I will only show what a histogram looks like in it: library(ggplot2) x = rnorm(100000, mean = 1, sd = 2) qplot(x, bins=40) Math. Mathematically, histogram can be produced for any (finite) sequence of numbers: we divide the range into several bins, count how many of the points in the sequence falls into which bin, and then draw a bar above that bin whose height is equal (or proportional to) that count. The picture tells use about how the sequence we started from is “distributed”. The order of the points does not matter - you would get exactly the same picture if you presorted the points. If the sequence of points you draw the histogram comes from, say, normal distribution, the histogram will resemble the shape of the pdf of a normal distribution. I say resemble, because its shape is ultimately random. If the number of points is small (like in the second part of this problem) the histogram may look nothing like the normal pdf. However, when the number of points gets larger and larger, the shape of the histogram gets closer and closer to the underlying pdf (if it exists). I keep writing “shape” because the the three histograms above have very different scales on the \\(y\\) axis. That is because we used counts to set the sizes of bins. A more natural choice is to use the proportions, i.e. relative frequencies (i.e. counts divided by the total number of points) for bar heights. In R, we would need to add an additional option to hist: x = rnorm(100000, mean = 1, sd = 2) hist(x,freq = FALSE) With such a normalization, the histogram of \\(x\\) can be directly compared to the probability density of a normal distribution. Here is a picture. Its R code is a little bit too advanced for now, so I am skipping it: 2.2 Monte Carlo Problem 2.6 Use Monte Carlo to estimate the expected value of the exponential random variable with parameter \\(\\lambda= 4\\) using \\(n=10\\), \\(n=1,000\\) and \\(1,000,000\\) simulations. Compare to the exact value. Solution: x &lt;- rexp(10, rate=4) mean(x) #R&gt; [1] 0.1779768 For an exponential random variable with parameter \\(\\lambda\\), the expected value is \\(1/\\lambda\\); in this case \\(0.25\\). The error made was 0.072023 for \\(n=10\\) simulations. We increase the numbner of simulations to \\(n=1000\\) and get a better result x &lt;- rexp(1000, rate=4) mean(x) #R&gt; [1] 0.2564643 with (smaller) error -0.0064643. Finally, let’s try \\(n=1,000,000\\): x &lt;- rexp(1000000, rate=4) mean(x) #R&gt; [1] 0.250381 The error is even smaller -0.00038101. This can be obtained quite easily by integration (by parts): \\[ {\\mathbb{E}}[X] = \\int_{-\\infty}^{\\infty} x f(x)\\, dx = \\int_0^{\\infty} x \\lambda e^{-\\lambda x}\\, dx = \\tfrac{1}{\\lambda}\\] Comments: R. The only new thing here is the command mean which computes the mean of vector. Math. There is a lot going on here conceptually. This is the first time we used the Monte Carlo method. It is an incredibly method, as you will keep being reminded throughout this class. The idea behind it is simple, and it is based on the Law of large numbers: Theorem Let \\(X_1,X_2, \\dots\\) be an independent sequence of random variables with the same distribution, for which the expected value can be computed. Then \\[ \\tfrac{1}{n} \\Big( X_1+X_2+\\dots+X_n\\Big) \\to {\\mathbb{E}}[X_1] \\text{ as } n\\to\\infty\\] The idea behind Monte Carlo is to turn this theorem “upside down”. The goal is to compute \\({\\mathbb{E}}[X_1]\\), and uses a supply of random numbers, each of which comes from the same distribution to accomplish that. The random number generator inside rexp gives us a supply of numbers (stored in the vector x) and all we have to do is compute their average. This gives us the left-hand side of the formula above, and, if \\(n\\) is large enough, we hope that the this average does not differ too much from its theoretical limit. As \\(n\\) gets larger, we expect better and better results. That is why your error above gets smaller as \\(n\\) increases. It looks like Monte Carlo can only be used to compute the expected value of a random variable, which does not seem like such a bit deal. It is! You will see in the sequel that almost anything can be written as the expected value of some random variable. Problem 2.7 Use Monte Carlo to estimate \\({\\mathbb{E}}[X^2]\\), where \\(X\\) is a standard normal random variable. Do the same thing when \\(X\\) is geometrically distributed with prameter \\(p=0.5\\). Solution. When \\(X\\) is standard normal \\(Y=X^2\\) has a \\(\\chi^2\\) distribution with one degree of freedom: y = rchisq(100000,df=1) mean(x) #R&gt; [1] 0.250381 In the second case, we do not know the distribution of \\(X^2\\), but can still do the following: x &lt;- rgeom(100000, prob=0.5) y &lt;- x^2 mean(y) #R&gt; [1] 2.99152 Comments: Math+R. We are asked to compute \\({\\mathbb{E}}[ X^2]\\), which can be interpreted in two ways. First, we can think of \\(Y=X^2\\) as a random variable in its own and try to take draws from the distribution of \\(Y\\). In the case of the normal distribution, the distribution of \\(Y\\) is known - it happens to be a \\(\\chi^2\\)-distribution with a single degree of freedom (don’t worry if you never heard of it). We can simulate it in R by using its R name chisq and get a number close to the exact value of \\(1\\). The case of a geometric distribution is seemingly more difficult, because we do not know what the distribution of \\(Y=X^2\\) is and there is no corresponding R name to put the prefix r in front of. What makes the simulation possible is the fact that \\(Y\\) is a transformation of a random variable we know how to simulate. In that case, we simply simulate the required number of draws x from the geometric distribution (using rgeom) and then apply the transformation \\(x \\mapsto x^2\\) to the result. The transformed vector y is then nothing but the sequence of draws from the distribution of \\(X^2\\). Btw, we could have done the same thing in the case of the normal random variable, too - there was no need to recoqnize its square as a \\(\\chi^2\\) random variable: x &lt;- rnorm(100000) y &lt;- x^2 mean(y) #R&gt; [1] 1.008819 The idea described above is one of main advantages of the Monte Carlo technique: if you know how to simulated a random variable, you also know how to simulated any (deterministic) function of it. That fact will come into its own a bit later when we start working with several random variables and stochastic processes, but it can be very helpful even in the case of a single random variable, as you will see in the next problem. Problem 2.8 Let \\(X\\) be a standard normal random variable. Use Monte Carlo to estimate the probability \\({\\mathbb{P}}[ X &gt; 1 ]\\). Compare to the exact value. Solution: The estimated probability: x &lt;- rnorm(100000) y &lt;- x &gt; 1 (p_est = mean(y)) #R&gt; [1] 0.15732 The exact probability: 1 - pnorm(1) #R&gt; [1] 0.1586553 The error is -0.0013353. Comments: R. In R, the symbol &gt; is an operation, which returns a boolean (TRUE or FALSE) value. For example: 1&gt;2 #R&gt; [1] FALSE 5^2&gt;20 #R&gt; [1] TRUE It works with vectors, too, but now the output is a vector of boolean values: x &lt;- c(1,2,4) y &lt;- c(5,-4,3) x&gt;y #R&gt; [1] FALSE TRUE TRUE You can even compare a vector and a scalar: x &lt;- 1:10 x&gt;5 #R&gt; [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE Therefore, the vector y in the solution is a vector of length \\(100000\\) whose elements are either TRUE or FALSE; here are the first 5 rows of the “spreadsheet” (called a data frame or a tibble in R) with columns x and y from our solution: x y 1.9493 FALSE -1.1015 TRUE 1.0448 TRUE -0.1384 TRUE -0.2573 TRUE Finally, z contains the mean of y. How do you compute a mean of boolean values? In R (and many other languages) TRUE and FALSE have default numerical values, usually \\(1\\) and \\(0\\). This way, when \\(R\\) is asked to compute the sum of a boolean vector it will effectively count the number of values which are TRUE. Similarly, the mean is the relative proportion of TRUE values. Math. We computed the proportion of the “times” \\(X&gt;1\\) (among many simulations of \\(X\\)) and used it to approximate the probability \\({\\mathbb{P}}[ X&gt;1]\\). More formally, we started from a random variable \\(X\\) with a normal distribution and then transformed it into another random variable, \\(Y\\), by setting \\(Y=1\\) whenever \\(X&gt;1\\) and \\(0\\) otherwise. This is often written as follows \\[ Y = \\begin{cases} 1, &amp; X&gt;1 \\\\ 0, &amp; X\\leq 1.\\end{cases}\\] The random variable \\(Y\\) is very special - it can only take values \\(0\\) and \\(1\\) (i.e., its support is \\(\\{0,1\\}\\)). Such random variables are called indicator random variables, and their distribution, called the Bernoulli distribution, always looks like this: 0 1 1-p p for some \\(p \\in [0,1]\\). The parameter \\(p\\) is nothing but the probability \\({\\mathbb{P}}[Y=1]\\). So why did we decide to transform \\(X\\) into \\(Y\\)? Because of the following simple fact: \\[ {\\mathbb{E}}[ Y] = 1 \\times p + 0 \\times (1-p) = p.\\] The expected value of an indicator is the probability \\(p\\), and we know that we can use Monte Carlo whenever we can express the quantity we are computing as an expectated value of a random variable we know how to simulate. Problem 2.9 Use Monte Carlo to Estimate the value of \\(\\pi\\) and compute the error. Solution. nsim &lt;- 1000000 x &lt;- runif(nsim, -1, 1) y &lt;- runif(nsim, -1, 1) z &lt;- (x^2+y^2) &lt; 1 (pi_est &lt;- 4* mean(z)) #R&gt; [1] 3.141728 (err &lt;- pi_est - pi) #R&gt; [1] 0.0001353464 Since we know the “exact” value of \\(\\pi\\), we can compute the error {r} format(err, digits=5). Comments.: Math. As we learned in the previous problem, probabilities of events can be computed using Monte Carlo, as long as we know how to simulate the underlying indicator random variable. In this case, we want to compute \\(\\pi\\), so we would need to find a “situation” in which the probability of something is \\(\\pi\\). Of course, \\(\\pi&gt;1\\), so it cannot be a probability of anything, but \\(\\pi/4\\) can, and computing \\(\\pi/4\\) is as useful as computing \\(\\pi\\). To create the required probabilistic “situation” we think of the geometric meaning of \\(\\pi\\), and come up with the following scheme. Let \\(X\\) and \\(Y\\) be two independent uniform random variables each with values between \\(-1\\) and \\(1\\). We can think of the pair \\((X,Y)\\) as a random point in the squre \\([-1,1]\\times [-1,1]\\). This point will sometimes fall inside the unit circle, and sometimes it will not. What is the probability of hitting the circle? Well, since \\((X,Y)\\) is uniformly distributed everywhere inside the square, this probability shoule be equal to the portion of the area of our square which belongs to the unit circle. The area of the square is \\(4\\) and the area of the circle is \\(\\pi\\), so the required probability is \\(\\pi/4\\). Using the idea from the previous problem, we define the indicator random variable \\(Z\\) as follows \\[ Z = \\begin{cases} 1 &amp; (X,Y) \\text{ is inside the unit circle, } \\\\ 0 &amp; \\text{ otherwise.} \\end{cases} = \\begin{cases} 1&amp; X^2+Y^2 &lt; 1, \\\\ 0 &amp; \\text{ otherwise.} \\end{cases}\\] Problem 2.10 1. Write an R function cumavg which computes the sequence of running averages of a vector, i.e., if the input is \\(x=(x_1,x_2,x_3,\\dots, x_n)\\), the output should be \\[ \\Big(x_1, \\frac{1}{2} (x_1+x_2), \\frac{1}{3}(x_1+x_2+x_3), \\dots, \\frac{1}{n} (x_1+x_2+\\dots+x_n)\\Big).\\] Test it to check that it really works. Apply cumavg to the vector \\(4 z\\) from the previous problem and plot your results (use a smaller value for nsim. Maybe \\(1000\\).) Plot the values against their index. Add a read horizontal line at the level \\(\\pi\\). Rerun the same code (including the simulation part) several times. Solution cumavg &lt;- function(x) { c &lt;- cumsum(x) n &lt;- 1:length(x) return(c / n) } x &lt;- c(1, 3, 5, 3, 3, 9) cumavg(x) #R&gt; [1] 1 2 3 3 3 4 nsim &lt;- 1000 x &lt;- runif(nsim, -1, 1) y &lt;- runif(nsim, -1, 1) z &lt;- (x ^ 2 + y ^ 2) &lt; 1 pi_est = cumavg(4 * z) plot(1:nsim, pi_est, type = &quot;l&quot;, xlab = &quot;number of simulations&quot;, ylab = &quot;estimate of pi&quot;, main = &quot;Computing pi by Monte Carlo&quot; ) abline(pi, 0, col = &quot;red&quot;) Comments: R. Part 1: The function cumavg can be written in many ways; the one in the solution is sleek because it does not use any for loops. It is also faster than the other two implementations below. It relies on the fact that many natural operatrions in R are already vectorizes. The builtin function cumsum performs most of the work - once we have partial sums, we simply need to divide each one of them by the index of its position in the vector. If you wanted to use for loops and not rely on the function cumsum, you could write something like this: cumavg &lt;- function(x) { out &lt;- numeric(length(x)) for (i in 1:length(x)) { p &lt;- 0 for (j in 1:i) { p = p + x[j] } out[i] = p / i } return(out) } The statement out &lt;- numeric(length(x)) is there to ask R to reserve enough room (memory) for a numeric vector the same size as x. It is not neccessary, but it is a good idea to get into the habit of doing it because is makes your code both faster and easier to read. Here is another way, using the sum function: cumavg &lt;- function(x) { out = numeric(0) for (i in 1:length(x)) { out = c(out, sum(x[1:i]) / i) } return(out) } The line out = c(out, sum(x[1:i]) / i) appends the value sum(x[1:i]) / i to the end of the vector out. Note that x[1:i] is the vector x indexed by the sequence 1:i (i.e., 1,2,...,i), which is nothing other than the vector containing the first i elements of x. We also “pre-declared” the vector out in the statement `out = numeric(0)’. This time we only made sure that out is an empty numerical vector because of the way we build it incrementally. This is, in general, not a very efficient way of doing things, but it will do just fine for small vectors. Part 2. This course is not about R graphics, but I think it is a good idea to teach you how to make basic plots in R. This is what the functions plot and abline (and some others) do. I have already mentioned the fact that R has a number of high-quality graphics packages (like ggplot2), but the builtin R graphics is certainly good enough for “quick-and-dirty” plots. The main purpose of the function plot is to plot scatterplots: x &lt;- c(1,3,4,7) y &lt;- c(2,1,5,5) plot(x,y) The corresponding entries of vectors x and y are paired (into \\((1,2)\\), \\((3,1)\\), \\((4,5)\\) and \\((7,5)\\) in this case) and these pairs are used as coordinates of points. By default, each point is marked by a small circle, but this, and many other things, can be adjusted by numerous additional arguments. One of such arguments is type which determins the type of the plot. We used type=&quot;l&quot; which tells R to join the points with straight lines: x &lt;- c(1,3,4,7) y &lt;- c(2,1,5,5) plot(x,y, type=&quot;l&quot;) The other arguments, xlab, ylab and main determine labels for axes and the entire plot. The function abline(a,b) adds a line \\(y = a x + b\\) to an already existing plot. It is very useful in statistics if one wants to show the regression line superimposed on the scatterplot of data. Finally, the argument col, of course, determines the color of the line. To learn about various graphical parameters, type ?par. Math. The conceptual reason for this exercise is to explore (numerically) the kinds of errors we make when we use Monte Carlo. Unlike the deterministic numerical procedures, Monte Carlo has a strange property that no bound on the error can be made with absolute certainty. Let me give you an example. Suppose that you have a biased coin, with the probabilty \\(0.6\\) of heads and \\(0.4\\) of tails. You don’t know this probability, and use a Monte Carlo technique to estimate it - you toss your coin \\(1000\\) times and record the number of times you observe \\(H\\). The law of large numbers suggests that the relative frequency of heads is close to the true probability of \\(H\\). Indeed, you run a simulation set.seed(1234) x &lt;- sample( c(&quot;T&quot;,&quot;H&quot;), 1000, prob = c(0.4, 0.6), replace = TRUE) y &lt;- x == &quot;H&quot; mean(y) #R&gt; [1] 0.594 and get a pretty accurate estimate of \\(0.594\\). If you run the same code a few more times, you will get different estimates, but all of them will be close to \\(0.6\\). Theoretically, however, your simulation could have yielded \\(1000\\) Hs, which would lead you to report \\(p=1\\) as the Monte-Carlo estimate. The point is that even though such disasters are theoretically possible, they are exceedingly unlikely. The probability of getting all \\(H\\) in \\(1000\\) tosses of this coin is a number with more than \\(500\\) zeros after the decimal point. The take-home message is that even though there are no guarantees, Monte Carlo performs well vast majority of the time. The crucial ingredient, however, is the number of simulations. The plot you were asked to make illustrates exactly that. The function cumavg gives you a whole sequence of Monte-Carlo estimates of the same thing (the number \\(\\pi\\)) with different numbers of simulations nsim. For small values of nsim the error is typically very large (and very random). As the number of simulations grows, the situations stabilizes and the error decreases. Without going into the theory behind it, let me only mention is that in the majority of practical applications we have the following relationship: \\[ error \\sim \\frac{1}{\\sqrt{n}}.\\] In words, if you want to double the precision, you need to quadruple the number of simulations. If you want an extra digit in your estimate, you need to multiply the number of simulations by \\(100\\). Instead of going further into the theory, here is an image where I superimposed \\(40\\) plots like the one you were asked to produce (the red lines are \\(\\pm \\frac{4}{\\sqrt{n}}\\)): 2.3 Conditional probability and independence. Problem 2.11 Let \\(X\\) and \\(Y\\) be two independent geometric random variables with parameters \\(p=0.5\\), and let \\(Z=X+Y\\). Compute \\({\\mathbb{P}}[ X = 3| Z = 5]\\) using simulation. Compare to the exact value. Solution: nsim &lt;- 100000 X &lt;- rgeom(nsim, prob = 0.5) Y &lt;- rgeom(nsim, prob = 0.5) Z &lt;- X+Y X_cond = X[ Z == 5 ] mean(X_cond == 3) #R&gt; [1] 0.1690112 We have \\[ {\\mathbb{P}}[ X = 3 | Z= 5 ] = \\frac{{\\mathbb{P}}[ X=3 \\text{ and }Z=5]}{{\\mathbb{P}}[Z=5]} = \\frac{{\\mathbb{P}}[X=3 \\text{ and }Y = 2]}{{\\mathbb{P}}[Z=5]} \\] Since \\(X\\) and \\(Y\\) are independent, we have \\({\\mathbb{P}}[ X=3 \\text{ and }Y=2 ] = {\\mathbb{P}}[X=3] {\\mathbb{P}}[ Y=2] = 2^{-4} 2^{-3} = 2^{-7}\\). To compute \\({\\mathbb{P}}[ Z = 5]\\) we need to split the event \\(\\{ Z = 5 \\}\\) into events we know how to deal with. Since \\(Z\\) is built from \\(X\\) and \\(Y\\), we write \\[ \\begin{align} {\\mathbb{P}}[ Z = 5 ] = &amp;{\\mathbb{P}}[X=0 \\text{ and }Y=5]+ {\\mathbb{P}}[ X=1 \\text{ and }Y=4] + {\\mathbb{P}}[ X=2 \\text{ and }Y=3] + \\\\ &amp; {\\mathbb{P}}[ X=3 \\text{ and }Y=2] + {\\mathbb{P}}[ X=4 \\text{ and }Y=1] + {\\mathbb{P}}[ X = 5 \\text{ and }Y=0]. \\end{align}\\] Each of the individual probabilities in the sum above is \\(2^{-7}\\), so \\({\\mathbb{P}}[ X = 3 | Z = 5] = \\frac{1}{6}\\). This gives us an error of 0.0023445. Comments: Math. Let us, first, recall what the conditional probability is. The definition we learn in the probability class is the following \\[ {\\mathbb{P}}[A | B] = \\frac{{\\mathbb{P}}[A \\text{ and }B]}{{\\mathbb{P}}[B]}\\], as long as \\({\\mathbb{P}}[B]&gt;0\\). The interpretation is that \\({\\mathbb{P}}[A|B]\\) is still the probability of \\(A\\), but now in the world where \\(B\\) is guaranteed to happen. Conditioning usually happens when we receive new information. If someone tells us that \\(B\\) happened, we can disregard everyhting in the complement of \\(B\\) and adjust our probability to account for that fact. First we remove from \\(A\\) anything that belongs to the complement of \\(B\\), and recompute the probability \\({\\mathbb{P}}[A \\cap B]\\). We also have to divide by \\({\\mathbb{P}}[B]\\) because we want the total probability to be equal to \\(1\\). Our code starts as usual, but simulating \\(X\\) and \\(Y\\) from the required distribution, and constructing a new vector \\(Z\\) as their sum. The variable X_cond is new; we build it from \\(X\\) by removing all the elements whose corresponding \\(Z\\) is not equal to \\(5\\). This is an example of what is sometimes called the rejection method in simulation. We simply “reject” all simulations which do not satify the condition we are conditioning on. We can think of X_cond as bunch of simulations of \\(X\\), but in the world where \\(Z=5\\) is guaranteed to happen. Once we have X_cond, we proceed as usual by computing the relative frequency of the value \\(3\\) among all possible values \\(X\\) can take. Note that the same X_cond can also be used to compute the conditional probability \\({\\mathbb{P}}[ X=1| Z=5]\\). In fact, X_cond contains the information about the entire conditional distribution of \\(X\\) given \\(Z=5\\); if we draw a histogram of X_cond, we will get a good idea of what this distribution looks like: The histogram above suggests that the distribution of \\(X\\), given \\(Z=5\\), is uniform on \\(\\{0,1,2,3,4,5\\}\\). It is - a calculation almost identical to the one we performed above gives that \\({\\mathbb{P}}[ X= i| Z=5] = \\frac{1}{6}\\) for each \\(i=0,1,2,3,4,5\\). "]
]
