[
["markov-chains.html", "Chapter 5 Markov Chains 5.1 The Markov property 5.2 First Examples 5.3 Chapman-Kolmogorov equations 5.4 How to simulate Markov chains 5.5 Additional problems for Chapter 5", " Chapter 5 Markov Chains 5.1 The Markov property Simply put, a stochastic process has the Markov property if probabilities governing its future evolution depend only on its current position, and not on how it got there. Here is a more precise, mathematical, definition. It will be assumed throughout this course that any stochastic process \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) takes values in a countable set \\(S\\) called the state space. \\(S\\) will always be either finite, or countable, and a generic element of \\(S\\) will be denoted by \\(i\\) or \\(j\\). A stochastic process \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) taking values in a countable state space \\(S\\) is called a Markov chain if \\[\\begin{equation} {\\mathbb{P}}[ X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1},\\dots, X_1=i_1, X_0=i_0]= {\\mathbb{P}}[ X_{n+1}=j|X_n=i], \\tag{5.1} \\end{equation}\\] for all times \\(n\\in{\\mathbb{N}}_0\\), all states \\(i,j,i_0, i_1, \\dots, i_{n-1} \\in S\\), whenever the two conditional probabilities are well-defined, i.e., when \\[\\begin{equation} {\\mathbb{P}}[ X_n=i, \\dots, X_1=i_1, X_0=i_0]&gt;0. \\tag{5.2} \\end{equation}\\] The Markov property is typically checked in the following way: one computes the left-hand side of (5.1) and shows that its value does not depend on \\(i_{n-1},i_{n-2}, \\dots, i_1, i_0\\) (why is that enough?). The condition (5.2) will be assumed (without explicit mention) every time we write a conditional expression like to one in (5.1). All chains in this course will be homogeneous, i.e., the conditional probabilities \\({\\mathbb{P}}[X_{n+1}=j|X_{n}=i]\\) will not depend on the current time \\(n\\in{\\mathbb{N}}_0\\), i.e., \\({\\mathbb{P}}[X_{n+1}=j|X_{n}=i]={\\mathbb{P}}[X_{m+1}=j|X_{m}=i]\\), for \\(m,n\\in{\\mathbb{N}}_0\\). Markov chains are (relatively) easy to work with because the Markov property allows us to compute all the probabilities, expectations, etc. we might be interested in by using only two ingredients. The initial distribution: \\({a}^{(0)}= \\{ {a}^{(0)}_i\\, : \\, i\\in S\\}\\), \\({a}^{(0)}_i={\\mathbb{P}}[X_0=i]\\) - the initial probability distribution of the process, and Transition probabilities: \\(p_{ij}={\\mathbb{P}}[X_{n+1}=j|X_n=i]\\) - the mechanism that the process uses to jump around. Indeed, if you know \\({a}^{(0)}_i\\) and \\(p_{ij}\\) for all \\(i,j\\in S\\) and want to compute a joint distribution \\({\\mathbb{P}}[X_n=i_n, X_{n-1}=i_{n-1}, \\dots, X_0=i_0]\\), you can use the definition of conditional probability and the Markov property several times (the multiplication theorem from your elementary probability course) as follows: \\[\\begin{align} {\\mathbb{P}}[X_n=i_n, \\dots, X_0=i_0] &amp;= {\\mathbb{P}}[X_n=i_n| X_{n-1}=i_{n-1}, \\dots,X_0=i_0] \\cdot {\\mathbb{P}}[X_{n-1}=i_{n-1}, \\dots,X_0=i_0] \\\\ &amp; = {\\mathbb{P}}[X_n=i_n| X_{n-1}=i_{n-1}] \\cdot {\\mathbb{P}}[X_{n-1}=i_{n-1}, \\dots,X_0=i_0]\\\\ &amp;= p_{i_{n-1} i_{n}} {\\mathbb{P}}[X_{n-1}=i_{n-1}, \\dots,X_0=i_0] \\end{align}\\] If we repeat the same procedure \\(n-2\\) more times (and flip the order of factors), we get \\[\\begin{align} {\\mathbb{P}}[X_n=i_n, \\dots, X _0=i_0] &amp;= {a}^{(0)}_{i_0} \\cdot p_{i_0 i_1} \\cdot p_{i_1 i_2}\\cdot \\ldots \\cdot p_{i_{n-1} i_{n}} \\end{align}\\] Think of it this way: the probability of the process taking the trajectory \\((i_0, i_1, \\dots, i_n)\\) is: the probability of starting at \\(i_0\\) (which is \\({a}^{(0)}_{i_0}\\)), multiplied by the probability of transitioning from \\(i_0\\) to \\(i_1\\) (which is \\(p_{i_0 i_1}\\)), multiplied by the probability of transitioning from \\(i_1\\) to \\(i_2\\) (which is \\(p_{i_1 i_2}\\)), etc. When \\(S\\) is finite, there is no loss of generality in assuming that \\(S=\\{1,2,\\dots, n\\}\\), and then we usually organize the entries of \\({a}^{(0)}\\) into a row vector \\[{a}^{(0)}=({a}^{(0)}_1,{a}^{(0)}_2,\\dots, {a}^{(0)}_n),\\] and the transition probabilities \\(p_{ij}\\) into a square matrix \\({\\mathbf P}\\), where \\[{\\mathbf P}=\\begin{bmatrix} p_{11} &amp; p_{12} &amp; \\dots &amp; p_{1n} \\\\ p_{21} &amp; p_{22} &amp; \\dots &amp; p_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ p_{n1} &amp; p_{n2} &amp; \\dots &amp; p_{nn} \\\\ \\end{bmatrix}\\] In the general case (\\(S\\) possibly infinite), one can still use the vector and matrix notation as before, but it becomes quite clumsy. For example, if \\(S={\\mathbb{Z}}\\), then \\({\\mathbf P}\\) is an infinite matrix \\[{\\mathbf P}=\\begin{bmatrix} \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\\\ \\dots &amp; p_{-1\\, -1} &amp; p_{-1\\, 0} &amp; p_{-1\\, 1} &amp; \\dots \\\\ \\dots &amp; p_{0\\, -1} &amp; p_{0\\, 0} &amp; p_{0\\, 1} &amp; \\dots \\\\ \\dots &amp; p_{1\\, -1} &amp; p_{1\\, 0} &amp; p_{1\\, 1} &amp; \\dots \\\\ &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\\\ \\end{bmatrix}\\] 5.2 First Examples Here are some examples of Markov chains - you will see many more in problems and later chapters. 5.2.1 Random walks Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a simple (possibly biased) random walk. Let us show that it indeed has the Markov property (5.1). Remember, first, that \\[X_n=\\sum_{k=1}^n \\delta_k \\text{ where }\\delta_k \\text{ are independent (possibly biased) coin-tosses.}\\] For a choice of \\(i_0, \\dots, i_n, j=i_{n+1}\\) (such that \\(i_0=0\\) and \\(i_{k+1}-i_{k}=\\pm 1\\)) we have \\[%\\label{equ:} \\nonumber \\begin{split} {\\mathbb{P}}[ X_{n+1}=i_{n+1}&amp;|X_n=i_n, X_{n-1}=i_{n-1},\\dots, X_1=i_1, X_0=i_0]\\\\ = &amp; {\\mathbb{P}}[ X_{n+1}-X_{n}=i_{n+1}-i_{n}|X_n=i_n, X_{n-1}=i_{n-1},\\dots, X_1=i_1, X_0=i_0] \\\\ = &amp; {\\mathbb{P}}[ \\delta_{n+1}=i_{n+1}-i_{n}|X_n=i_n, X_{n-1}=i_{n-1},\\dots, X_1=i_1, X_0=i_0] \\\\= &amp; {\\mathbb{P}}[ \\delta_{n+1}=i_{n+1}-i_n], \\end{split}\\] where the last equality follows from the fact that the increment \\(\\delta_{n+1}\\) is independent of the previous increments, and, therefore, also of the values of \\(X_1,X_2, \\dots, X_n\\). The last line above does not depend on \\(i_{n-1}, \\dots, i_1, i_0\\), so \\(X\\) indeed has the Markov property. The state space \\(S\\) of \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) is the set \\({\\mathbb{Z}}\\) of all integers, and the initial distribution \\({a}^{(0)}\\) is very simple: we start at \\(0\\) with probability \\(1\\) (so that \\({a}^{(0)}_0=1\\) and \\({a}^{(0)}_i=0\\), for \\(i\\not= 0\\).). The transition probabilities are simple to write down \\[p_{ij}= \\begin{cases} p, &amp; j=i+1 \\\\ q, &amp; j=i-1 \\\\ 0, &amp; \\text{otherwise.} \\end{cases}\\] If you insist, these can be written down in an infinite matrix, \\[{\\mathbf P}=\\begin{bmatrix} \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\\\ \\dots &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; \\dots \\\\ \\dots &amp; q &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; \\dots \\\\ \\dots &amp;0 &amp;q &amp; 0 &amp; p &amp; 0 &amp; \\dots \\\\ \\dots &amp;0 &amp;0 &amp; q&amp; 0 &amp; p&amp; \\dots \\\\ \\dots &amp;0 &amp; 0 &amp;0 &amp; q&amp; 0&amp; \\dots \\\\ \\dots &amp;0 &amp; 0 &amp;0 &amp; 0&amp; q&amp; \\dots \\\\ &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\\\ \\end{bmatrix}\\] but this representation is typically not as useful as in the finite case. 5.2.2 Gambler’s ruin In Gambler’s ruin, a gambler starts with \\(\\$x\\), where \\(0\\leq x \\leq a\\in{\\mathbb{N}}\\) and in each play wins a dollar (with probability \\(p\\in (0,1)\\)) and loses a dollar (with probability \\(q=1-p\\)). When the gambler reaches either \\(0\\) or \\(a\\), the game stops. For mathematical convenience, it is usually a good idea to keep the chain defined, even after the modeled phenomenon stops. This is usually accomplished by simply assuming that the process “stays alive” but remains “frozen in place” instead of disappearing. In our case, once the gambler reaches either of the states \\(0\\) and \\(a\\), he/she simply stays there forever. Therefore, the transition probabilities are similar to those of a random walk, but differ from them at the boundaries \\(0\\) and \\(a\\). The state space is finite \\(S=\\{0,1,\\dots, a\\}\\) and the matrix \\({\\mathbf P}\\) is given by \\[{\\mathbf P}=\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 \\\\ q &amp; 0 &amp; p &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; q &amp; 0 &amp; p &amp; \\dots &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; q &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; p &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; q &amp; 0 &amp; p \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix}\\] The initial distribution is deterministic: \\[{a}^{(0)}_i= \\begin{cases} 1,&amp; i=x,\\\\ 0,&amp; i\\not= x. \\end{cases}\\] 5.2.3 Regime Switching Consider a system with two different states; think about a simple weather forecast (rain/no rain), high/low water level in a reservoir, high/low volatility regime in a financial market, high/low level of economic growth, the political party in power, etc. Suppose that the states are called \\(1\\) and \\(2\\) and the probabilities \\(p_{12}\\) and \\(p_{21}\\) of switching states are given. The probabilities \\(p_{11}=1-p_{12}\\) and \\(p_{22}=1-p_{21}\\) correspond to the system staying in the same state. The transition matrix for this Markov chain with \\(S=\\{1,2\\}\\) is \\[{\\mathbf P}= \\begin{bmatrix} p_{11} &amp; p_{12} \\\\ p_{21} &amp; p_{22}. \\end{bmatrix}\\] When \\(p_{12}\\) and \\(p_{21}\\) are large (close to \\(1\\)) the system nervously jumps between the two states. When they are small, there are long periods of stability (staying in the same state). One of the assumptions behind regime-switching models is that the transitions (switches) can only happen in regular intervals (once a minute, once a day, once a year, etc.). This is a feature of all discrete-time Markov chains. One would need to use a continuous-time model to allow for the transitions between states at any point in time. 5.2.4 Deterministically monotone Markov chain A stochastic process \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) with state space \\(S={\\mathbb{N}}_0\\) such that \\(X_n=n\\) for \\(n\\in{\\mathbb{N}}_0\\) (no randomness here) is called Deterministically monotone Markov chain (DMMC). The transition matrix looks like this \\[{\\mathbf P}= \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; \\dots \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; \\dots \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\dots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\] It is a pretty boring chain; its main use is as a counterexample. 5.2.5 Not a Markov chain Consider a frog jumping from a lily pad to a lily pad in a small forest pond. Suppose that there are \\(N\\) lily pads so that the state space can be described as \\(S=\\{1,2,\\dots, N\\}\\). The frog starts on lily pad 1 at time \\(n=0\\), and jumps around in the following fashion: at time \\(0\\) it chooses any lily pad except for the one it is currently sitting on (with equal probability) and then jumps to it. At time \\(n&gt;0\\), it chooses any lily pad other than the one it is sitting on and the one it visited immediately before (with equal probability) and jumps to it. The position \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) of the frog is not a Markov chain. Indeed, we have \\[{\\mathbb{P}}[X_3=1|X_2=2, X_1=3]= \\frac{1}{N-2},\\] while \\[{\\mathbb{P}}[X_3=1|X_2=2, X_1=1]=0.\\] A more dramatic version of this example would be the one where the frog remembers all the lily pads it had visited before, and only chooses among the remaining ones for the next jump. 5.2.6 Turning a non-Markov chain into a Markov chain How can we turn the process the previous example into a Markov chain. Obviously, the problem is that the frog has to remember the number of the lily pad it came from in order to decide where to jump next. The way out is to make this information a part of the state. In other words, we need to change the state space. Instead of just \\(S=\\{1,2,\\dots, N\\}\\), we set \\(S= \\{ (i_1, i_2)\\, : \\, i_1,i_2 \\in\\{1,2,\\dots N\\}\\}\\). In words, the state of the process will now contain not only the number of the current lily pad (i.e., \\(i_2\\)) but also the number of the lily pad we came from (i.e., \\(i_1\\)). This way, the frog will be in the state \\((i_1,i_2)\\) if it is currently on the lily pad number \\(i_2\\), and it arrived here from \\(i_1\\). There is a bit of freedom with the initial state, but we simply assume that we start from \\((1,1)\\). Starting from the state \\((i_1,i_2)\\), the frog can jump to any state of the form \\((i_2, i_3)\\), \\(i_3\\not= i_1,i_2\\) (with equal probabilities). Note that some states will never be visited (like \\((i,i)\\) for \\(i\\not = 1\\)), so we could have reduced the state space a little bit right from the start. It is important to stress that the passage to the new state space defines a whole new stochastic process. It is therefore, not quite accurate, as the title suggests, to say that we “turned” a non-Markov process into a Markov process. Rather, we replaced a non-Markovian model of a given situation by a different, Markovian, one. 5.2.7 Deterministic functions of Markov chains do not need to be Markov chains Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a Markov chain on the state space \\(S\\), and let \\(f:S\\to T\\) be a function. The stochastic process \\(Y_n= f(X_n)\\) takes values in \\(T\\); is it necessarily a Markov chain? We will see in this example that the answer is no. Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a simple symmetric random walk, with the usual state space \\(S = {\\mathbb{Z}}\\). With \\(r(m) = m\\ (\\text{mod } 3)\\) denoting the remainder after the division by \\(3\\), we first define the process \\(R_n = r(X_n)\\) so that \\[R_n=\\begin{cases} 0, &amp; \\text{ if $X_n$ is divisible by 3,}\\\\ 1, &amp; \\text{ if $X_n-1$ is divisible by 3,}\\\\ 2, &amp; \\text{ if $X_n-2$ is divisible by 3.} \\end{cases}\\] Using \\(R_n\\) we define \\(Y_n = (X_n-R_n)/3\\) to be the corresponding quotient, so that \\(Y_n\\in{\\mathbb{Z}}\\) and \\[3 Y_n \\leq X_n &lt;3 (Y_n+1).\\] The process \\(Y\\) is of the form \\(Y_n = f(X_n)\\), where \\(f(i)= \\lfloor i/3 \\rfloor\\), and \\(\\lfloor x \\rfloor\\) is the largest integer not exceeding \\(x\\). To show that \\(Y\\) is not a Markov chain, let us consider the the event \\(A=\\{Y_2=0, Y_1=0\\}\\). The only way for this to happen is if \\(X_1=1\\) and \\(X_2=2\\) or \\(X_1=1\\) and \\(X_2=0\\), so that \\(A=\\{X_1=1\\}\\). Also \\(Y_3=1\\) if and only if \\(X_3=3\\). Therefore \\[{\\mathbb{P}}[ Y_3=1|Y_2=0, Y_1=0]={\\mathbb{P}}[ X_3=3| X_1=1]= 1/4.\\] On the other hand, \\(Y_2=0\\) if and only if \\(X_2=0\\) or \\(X_2=2\\), so \\({\\mathbb{P}}[Y_2=0]= 3/4\\). Finally, \\(Y_3=1\\) and \\(Y_2=0\\) if and only if \\(X_3=3\\) and so \\({\\mathbb{P}}[Y_3=1, Y_2=0]= 1/8\\). Hence, \\[{\\mathbb{P}}[ Y_3=1|Y_2=0]={\\mathbb{P}}[Y_3=1, Y_2=0]/{\\mathbb{P}}[Y_2=0]= \\frac{1/8}{3/4}= \\frac{1}{6}.\\] Therefore, \\(Y\\) is not a Markov chain. If you want a more dramatic example, try to modify this example so that one of the probabilities above is positive, but the other is zero. The important property of the function \\(f\\) we applied to \\(X\\) is that it is not one-to-one. In other words, \\(f\\) collapses several states of \\(X\\) into a single state of \\(Y\\). This way, the “present” may end up containing so little information that the past suddenly becomes relevant for the dynamics of the future evolution. 5.2.8 A game of tennis In a game of tennis, the scoring system is as follows: both players start with the score of \\(0\\). Each time player 1 wins a point (a.k.a. a rally), her score moves a step up in the following hierarchy \\[0 \\mapsto 15 \\mapsto 30 \\mapsto 40.\\] Once she reaches \\(40\\) and scores a point, three things can happen: if the score of player 2 is \\(30\\) or less, player 1 wins the game. if the score of player 2 \\(40\\), the score of player 1 moves up to “advantage”, and if the score of player 2 is “advantage”, nothing happens to the score of player 1 but the score of player 2 falls back to \\(40\\). Finally, if the score of player 1 is “advantage” and she wins a point, she wins the game. The situation is entirely symmetric for player 2. We suppose that the probability that player 1 wins each point is \\(p\\in (0,1)\\), independently of the current score. A situation like this is a typical example of a Markov chain in an applied setting. What are the states of the process? We obviously need to know both players’ scores and we also need to know if one of the players has won the game. Therefore, a possible state space is the following: \\[\\begin{align} S= \\Big\\{ &amp;(0,0), (0,15), (0,30), (0,40), (15,0), (15,15), (15,30), (15,40), (30,0), (30,15),\\\\ &amp; (30,30), (30,40), (40,0), (40,15), (40,30), (40,40), (40,A), (A,40), W_1, W_2 \\Big\\} \\end{align}\\] where \\(A\\) stands for “advantage” and \\(W_1\\) (resp., \\(W_2\\)) denotes the state where player 1 (resp., player 2) wins. It is not hard to assign probabilities to transitions between states. Once we reach either \\(W_1\\) or \\(W_2\\) the game stops. We can assume that the chain remains in that state forever, i.e., the state is absorbing. The initial distribution is quite simple - we always start from the same state \\((0,0)\\), so that \\({a}^{(0)}_{(0,0)}=1\\) and \\({a}^{(0)}_i=0\\) for all \\(i\\in S\\setminus\\{(0,0)\\}\\). How about the transition matrix? When the number of states is big (\\(\\# S=20\\) in this case), transition matrices are useful in computer memory, but not so much on paper. Just for the fun of it, here is the transition matrix for our game-of-tennis chain (I am going to leave it up to you to figure out how rows/columns of the matrix match to states) \\[\\tiny {\\mathbf P}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 \\\\ p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; 0 \\\\ p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; p \\\\ 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 \\\\ p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] Does the structure of a game of tennis make is easier or harder for the better player to win? In other words, if you had to play against the best tennis player in the world (I am rudely assuming that he or she is better than you), would you have a better chance of winning if you only played a point (rally), or if you played the whole game? We will give a precise answer to this question in a little while. In the meantime, try to guess. 5.3 Chapman-Kolmogorov equations The transition probabilities \\(p_{ij}\\), \\(i,j\\in S\\) tell us how a Markov chain jumps from one state to another in a single step. Think of it as a description of the local behavior of the chain. This is the information one can usually obtain from observations and modeling assumptions. On the other hand, it is the global (long-time) behavior of the model that provides the most interesting insights. In that spirit, we turn our attention to probabilities like this: \\[{\\mathbb{P}}[X_{k+n}=j|X_k=i] \\text{ for } n = 1,2,\\dots.\\] Since we are assuming that all of our chains are homogeneous (transition probabilities do not change with time), this probability does not depend on the time \\(k\\), so we can define the multi-step transition probabilities \\(p^{(n)}_{ij}\\) as follows: \\[p^{(n)}_{ij}={\\mathbb{P}}[X_{k+n}=j|X_{k}=i]={\\mathbb{P}}[ X_{n}=j|X_0=i].\\] We allow \\(n=0\\) under the useful convention that \\[p^{(0)}_{ij}=\\begin{cases} 1, &amp; i=j,\\\\ 0,&amp; i\\not = j. \\end{cases}\\] We note right away that the numbers \\(p^{(n)}_{ij}\\), \\(i,j\\in S\\) naturally fit into an \\(N\\times N\\)-matrix which we denote by \\({\\mathbf P}^{(n)}\\). We note right away that \\[\\begin{equation} {\\mathbf P}^{(0)}= \\operatorname{Id}\\text{ and } {\\mathbf P}^{(1)}= {\\mathbf P}, \\tag{5.3} \\end{equation}\\] where \\(\\operatorname{Id}\\) denotes the \\(N\\times N\\) identity matrix. The central result of this section is the following sequence of equalities connecting \\({\\mathbf P}^{(n)}\\) for different values of \\(n\\), know as the Chapman-Kolmogorov equations: \\[\\begin{equation} {\\mathbf P}^{(m+n)} = {\\mathbf P}^{(m)} {\\mathbf P}^{(n)}, \\text{ for all } m,n \\in {\\mathbb{N}}_0. \\tag{5.4} \\end{equation}\\] To see why this is true we start by computing \\({\\mathbb{P}}[ X_{n+m} = j, X_0=i]\\). Since each trajectory from \\(i\\) to \\(j\\) in \\(n+m\\) steps has be somewhere at time \\(n\\), we can write \\[\\begin{equation} {\\mathbb{P}}[ X_{n+m}= j, X_0 = i] = \\sum_{k\\in S} {\\mathbb{P}}[X_{n+m} = j, X_{n} = k, X_0 = i]. \\tag{5.5} \\end{equation}\\] By the multiplication rule, we have \\[\\begin{multline} {\\mathbb{P}}[X_{n+m} = j, X_{n} = k, X_0 = i] = {\\mathbb{P}}[ X_{n+m} = j | X_{n}=k, X_{0}=i] {\\mathbb{P}}[X_{n}=k, X_0 = i], \\tag{5.6} \\end{multline}\\] and then, by the Markov property: \\[\\begin{equation} {\\mathbb{P}}[ X_{n+m} = j | X_{n}=k, X_{0}=i] = {\\mathbb{P}}[ X_{n+m} = j | X_n = k]. \\tag{5.7} \\end{equation}\\] Combining (5.5), (5.6) and (5.7) we obtain the following equality: \\[\\begin{equation} {\\mathbb{P}}[ X_{n+m}= j, X_0 = i] = \\sum_{k\\in S} {\\mathbb{P}}[ X_{n+m} = j | X_n = k] {\\mathbb{P}}[X_{n}=k, X_0 = i]. \\end{equation}\\] which is nothing but (5.4); to see that, just remember how matrices are multiplied. The punchline is that (5.4), together with (5.3) imply that \\[\\begin{equation} {\\mathbf P}^{(n)}= {\\mathbf P}^n, \\tag{5.8} \\end{equation}\\] where the left-hand side is the matrix composed of the \\(n\\)-step transition probabilities, and the right hand side is the \\(n\\)-th (matrix) power of the (\\(1\\)-step) transition matrix \\({\\mathbf P}\\). Using (5.8) allows us to write a simple expression for the distribution of the random variable \\(X_n\\), for \\(n\\in{\\mathbb{N}}_0\\). Remember that the initial distribution (the distribution of \\(X_0\\)) is denoted by \\({a}^{(0)}=({a}^{(0)}_i)_{i\\in S}\\). Analogously, we define the vector \\({a}^{(n)}=({a}^{(n)}_i)_{i\\in S}\\) by \\[{a}^{(n)}_i={\\mathbb{P}}[X_n=i],\\ i\\in S.\\] Using the law of total probability, we have \\[{a}^{(n)}_i={\\mathbb{P}}[X_n=i]=\\sum_{k\\in S} {\\mathbb{P}}[ X_0=k] {\\mathbb{P}}[ X_n=i|X_0=k]= \\sum_{k\\in S} {a}^{(0)}_k p^{(n)}_{ki}.\\] We usually interpret \\({a}^{(0)}\\) as a (row) vector, so the above relationship can be expressed using vector-matrix multiplication \\[{a}^{(n)}={a}^{(0)}{\\mathbf P}^n.\\] Find an explicit expression for \\({\\mathbf P}^{(n)}\\) in the case of the regime-switching chain introduced above. Feel free to assume that \\(p_{ij}&gt;0\\) for all \\(i,j\\). It is often difficult to compute \\({\\mathbf P}^n\\) for a general transition matrix \\({\\mathbf P}\\) and a large \\(n\\). We will see later that it will be easier to find the limiting values \\(\\lim_{n\\to\\infty}p^{(n)}_{ij}\\). The regime-switching chain is one of the few examples where everything can be done by hand. By (5.8), we need to compute the \\(n\\)-th matrix power of the transition matrix \\({\\mathbf P}\\). To make the notation a bit nicer, let us write \\(a\\) for \\(p_{12}\\) and \\(b\\) for \\(p_{21}\\), so that we can write \\[{\\mathbf P}= \\begin{bmatrix} 1-a &amp; a \\\\ b &amp; 1-b \\end{bmatrix}\\] The winning idea is to use diagonalization, and for that we start by writing down the characteristic equation \\(\\det (\\lambda I-{\\mathbf P})=0\\) of the matrix \\({\\mathbf P}\\): \\[\\label{equ:} \\nonumber \\begin{split} 0&amp;=\\det(\\lambda I-{\\mathbf P})= \\begin{vmatrix} \\lambda-1+a &amp; -a \\\\ -b &amp; \\lambda-1+b \\end{vmatrix}\\\\ &amp; =((\\lambda-1)+a)((\\lambda-1)+b)-ab =(\\lambda-1)(\\lambda-(1-a-b)). \\end{split}\\] The eigenvalues are, therefore, \\(\\lambda_1=1\\) and \\(\\lambda_2=1-a-b\\), and the corresponding eigenvectors are \\(v_1=\\binom{1}{1}\\) and \\(v_2=\\binom{a}{-b}\\). Therefore, if we define \\[V= \\begin{bmatrix} 1 &amp; a \\\\ 1 &amp; -b \\end{bmatrix} \\text{ and }D= \\begin{bmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{bmatrix}= \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; (1-a-b) \\end{bmatrix}\\] we have \\[{\\mathbf P}V = V D,\\text{ i.e., } {\\mathbf P}= V D V^{-1}.\\] This representation is very useful for taking (matrix) powers: \\[\\label{equ:60C4} \\begin{split} {\\mathbf P}^n &amp;= (V D V^{-1})( V D V^{-1}) \\dots (V D V^{-1})= V D^n V^{-1} \\\\ &amp; = V \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; (1-a-b)^n \\end{bmatrix} V^{-1} \\end{split}\\] We assumed that all \\(p_{ij}\\) are positive which means, in particular, that \\(a+b&gt;0\\) and \\[V^{-1} = \\tfrac{1}{a+b} \\begin{bmatrix} b &amp; a \\\\ 1 &amp; -1 \\end{bmatrix},\\] and so \\[\\begin{align} {\\mathbf P}^n &amp;= V D^n V^{-1}= \\begin{bmatrix} 1 &amp; a \\\\ 1 &amp; -b \\end{bmatrix} \\ \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; (1-a-b)^n \\end{bmatrix} \\ \\tfrac{1}{a+b} \\begin{bmatrix} b &amp; a \\\\ 1 &amp; -1 \\end{bmatrix}\\\\ &amp;= \\frac{1}{a+b} \\begin{bmatrix} b &amp; a \\\\ b &amp; a \\end{bmatrix} + \\frac{(1-a-b)^n}{a+b} \\begin{bmatrix} a &amp; -a \\\\ b &amp; -b \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} \\frac{b}{a+b}+(1-a-b)^n \\frac{a}{a+b} &amp; \\frac{a}{a+b}-(1-a-b)^n \\frac{a}{a+b}\\\\ \\frac{b}{a+b}+(1-a-b)^n \\frac{b}{a+b} &amp; \\frac{a}{a+b}-(1-a-b)^n \\frac{b}{a+b} \\end{bmatrix} \\end{align}\\] The expression for \\({\\mathbf P}^n\\) above tells us a lot about the structure of the multi-step probabilities \\(p^{(n)}_{ij}\\) for large \\(n\\). Note that the second matrix on the right-hand side above comes multiplied by \\((1-a-b)^n\\) which tends to \\(0\\) as \\(n\\to\\infty\\) (under our assumptions that \\(p_{ij}&gt;0\\).) We can, therefore, write \\[{\\mathbf P}^n\\sim \\frac{1}{a+b} \\begin{bmatrix} a &amp; b \\\\ a &amp; b \\end{bmatrix} \\text{ for large } n.\\] The fact that the rows of the right-hand side above are equal points to the fact that, for large \\(n\\), \\(p^{(n)}_{ij}\\) does not depend (much) on the initial state \\(i\\). In other words, this Markov chain forgets its initial condition after a long period of time. This is a rule more than an exception, and we will study such phenomena in the following lectures. 5.4 How to simulate Markov chains COMING SOON 5.5 Additional problems for Chapter 5 Let \\(\\{Y_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a sequence of die-rolls, i.e., a sequence of independent random variables which take values \\(1,2,\\dots, 6\\), each with probability \\(1/6\\). Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a stochastic process defined by \\[X_n=\\max (Y_0,Y_1, \\dots, Y_n), \\ n\\in{\\mathbb{N}}_0.\\] In words, \\(X_n\\) is the maximal value rolled so far. Is \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) a Markov chain? If it is, find its transition matrix and the initial distribution. If it is not, give an example of how the Markov property is violated. Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a simple symmetric random walk. For \\(n\\in{\\mathbb{N}}_0\\), define \\(Y_n = 2X_n+1\\), and let \\(Z_n\\) be the amount of time \\(X_n\\) spent strictly above \\(0\\) up to (and including) time \\(n\\), i.e. \\[Z_0=0, Z_{n+1} - Z_n = \\begin{cases} 1, &amp; X_{n+1}&gt;0 \\\\ 0, &amp; X_ {n+1}\\leq 0 \\end{cases} , \\text{ for }n\\in{\\mathbb{N}}_0.\\] Is \\(Y\\) a Markov chain? Is \\(Z\\)? Let \\(\\{\\delta_n\\}_{n\\in{\\mathbb{N}}}\\) be a sequence of independent coin tosses (i.e., random variables with values \\(T\\) or \\(H\\) with equal probabilities). Let \\(X_0=0\\), and, for \\(n\\in{\\mathbb{N}}\\), let \\(X_n\\) be the number of times two consecutive \\(\\delta\\)s take the same value in the first \\(n+1\\) tosses. For example, if the outcome of the coin tosses is TTHHTTTH …, we have \\(X_0=0\\), \\(X_1=1\\), \\(X_2=1\\), \\(X_3=2\\), \\(X_4=2\\), \\(X_5=3\\), \\(X_6=4\\), \\(X_7=4\\), … Is \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) a Markov chain? If it is, describe its state space, the transition probabilities and the initial distribution. If it is not, show exactly how the Markov property is violated. Let \\(X\\) be a Markov chain on \\(N\\) states, with the \\(N\\times N\\) transition matrix \\(P\\). We construct a new Markov chain \\(Y\\) from the transition mechanism of \\(X\\) as follows: at each point in time, we toss a biased coin (probability of heads \\(p\\in (0,1)\\)), independently of everything else. If it shows heads we move according to the transition matrix of \\(X\\). If it shows tails, we remain in the same state. What is the transition matrix of \\(Y\\)? The red container has 100 red balls, and the blue container has 100 blue balls. In each step - a container is selected (with equal probabilities), - a ball is selected from it (all balls in the container are equally likely to be selected), and - the selected ball is placed in the other container. If the selected container is empty, no ball is transferred. Once there are 100 blue balls in the red container and 100 red balls in the blue container, the game stops. We decide to model the situation as a Markov chain. What is the state space \\(S\\) we can use? How large is it? What is the initial distribution? What are the transition probabilities between states? Don’t write the matrix, it is way too large; just write a general expression for \\(p_{ij}\\), \\(i,j\\in S\\). (Note: this is a version of the famous Ehrenfest Chain from statistical physics.) A “deck” of cards starts with 2 red and 2 black cards. A “move” consists of the following: - pick a random card from the deck (if the deck is empty, do nothing), - if the card is black and the card drawn on the previous move was also black, return it back to the deck, - otherwise, throw the card away (this, in particular, applies to any card drawn on the first move, since there is no “previous” move at that time). Model the situation using a Markov chain: find an appropriate state space, and sketch the transition graph with transition probabilities. How small can you make the state space? What is the probability that the deck will be empty after exactly \\(4\\) moves? What is the probability that the deck will be empty eventually? A country has \\(m+1\\) cities (\\(m\\in{\\mathbb{N}}\\)), one of which is the capital. There is a direct railway connection between each city and the capital, but there are no tracks between any two “non-capital” cities. A traveler starts in the capital and takes a train to a randomly chosen non-capital city (all cities are equally likely to be chosen), spends a night there and returns the next morning and immediately boards the train to the next city according to the same rule, spends the night there, …, etc. We assume that her choice of the city is independent of the cities visited in the past. Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be the number of visited non-capital cities up to (and including) day \\(n\\), so that \\(X_0=1\\), but \\(X_1\\) could be either \\(1\\) or \\(2\\), etc. Explain why \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) is a Markov chain on the appropriate state space \\({\\mathcal{S}}\\) and the find the transition probabilities of \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\), i.e., write an expression for \\[{\\mathbb{P}}[X_{n+1}=j|X_n=i], \\text{ for $i,j\\in S$.}\\] Let \\(\\tau_m\\) be the first time the traveler has visited all \\(m\\) non-capital cities, i.e. \\[\\tau_m=\\min \\{ n\\in{\\mathbb{N}}_0\\, : \\, X_n=m\\}.\\] What is the distribution of \\(\\tau_m\\), for \\(m=1\\) and \\(m=2\\). Compute \\({\\mathbb{E}}[\\tau_m]\\) for general \\(m\\in{\\mathbb{N}}\\). What is the asymptotic behavior of \\({\\mathbb{E}}[\\tau_m]\\) as \\(m\\to\\infty\\)? More precisely, find a simple function \\(f(m)\\) of \\(m\\) (like \\(m^2\\) or \\(\\log(m)\\)) such that \\({\\mathbb{E}}[\\tau_m] \\sim f(m)\\), i.e., \\(\\lim_{m\\to\\infty} \\frac{{\\mathbb{E}}[\\tau_m]}{f(m)} = 1\\). We start with two cups, call them \\(A\\) and \\(B\\). Cup \\(A\\) contains \\(12\\) oz of milk, and cup \\(B\\) \\(12\\) oz of water. The following procedure is then performed twice: first, half of the content of the glass \\(A\\) is transferred into class \\(B\\). Then, the contents of glass \\(B\\) are thoroughly mixed, and a third of its entire content transferred back to \\(A\\). Finally, the contents of the glass \\(A\\) are thoroughly mixed. What is the final amount of milk in glass A? What does this have to do with Markov chains? The state space of a Markov chain is \\(S = \\{1,2,3,4,5\\}\\), and the non-zero transition probabilities are given by \\(p_{11} = 1/2\\), \\(p_{12}=1/2\\), \\(p_{23}=p_{34}=p_{45}=p_{51}=1\\). Compute \\(p^{(6)}_{12}\\) without using software. In a Gambler’s ruin problem with the state space \\(S=\\{0,1,2,3,4\\}\\) and the probability \\(p=1/3\\) of winning in a single game, compute the \\(4\\)-step transition probabilities \\[p^{(4)}_{2 2} = {\\mathbb{P}}[ X_{n+4}=2| X_n =2] \\text{ and } p^{(4)}_{2 4} = {\\mathbb{P}}[ X_{n+4}=4| X_n =2].\\] A car-insurance company classifies drivers in three categories: bad, neutral and good. The reclassification is done in January of each year and the probabilities for transitions between different categories is given by \\[P= \\begin{bmatrix} 1/2 &amp; 1/2 &amp; 0 \\\\ 1/5 &amp; 2/5 &amp; 2/5 \\\\ 1/5 &amp; 1/5 &amp; 3/5\\end{bmatrix},\\] where the first row/column corresponds to the bad category, the second to neutral and the third to good. The company started in January 1990 with 1400 drivers in each category. Estimate the number of drivers in each category in 2090. Assume that the total number of drivers does not change in time and use R for your computations. "]
]
