[
["markov-chains.html", "Chapter 1 Markov Chains 1.1 The Markov property 1.2 First Examples 1.3 Chapman-Kolmogorov equations 1.4 How to simulate Markov chains 1.5 Additional problems for Chapter 5", " Chapter 1 Markov Chains 1.1 The Markov property Simply put, a stochastic process has the Markov property if probabilities governing its future evolution depend only on its current position, and not on how it got there. Here is a more precise, mathematical, definition. It will be assumed throughout this course that any stochastic process \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) takes values in a countable set \\(S\\) called the state space. \\(S\\) will always be either finite, or countable, and a generic element of \\(S\\) will be denoted by \\(i\\) or \\(j\\). A stochastic process \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) taking values in a countable state space \\(S\\) is called a Markov chain if \\[\\begin{equation} {\\mathbb{P}}[ X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1},\\dots, X_1=i_1, X_0=i_0]= {\\mathbb{P}}[ X_{n+1}=j|X_n=i], \\tag{1.1} \\end{equation}\\] for all times \\(n\\in{\\mathbb{N}}_0\\), all states \\(i,j,i_0, i_1, \\dots, i_{n-1} \\in S\\), whenever the two conditional probabilities are well-defined, i.e., when \\[\\begin{equation} {\\mathbb{P}}[ X_n=i, \\dots, X_1=i_1, X_0=i_0]&gt;0. \\tag{1.2} \\end{equation}\\] The Markov property is typically checked in the following way: one computes the left-hand side of (1.1) and shows that its value does not depend on \\(i_{n-1},i_{n-2}, \\dots, i_1, i_0\\) (why is that enough?). The condition (1.2) will be assumed (without explicit mention) every time we write a conditional expression like to one in (1.1). All chains in this course will be homogeneous, i.e., the conditional probabilities \\({\\mathbb{P}}[X_{n+1}=j|X_{n}=i]\\) will not depend on the current time \\(n\\in{\\mathbb{N}}_0\\), i.e., \\({\\mathbb{P}}[X_{n+1}=j|X_{n}=i]={\\mathbb{P}}[X_{m+1}=j|X_{m}=i]\\), for \\(m,n\\in{\\mathbb{N}}_0\\). Markov chains are (relatively) easy to work with because the Markov property allows us to compute all the probabilities, expectations, etc. we might be interested in by using only two ingredients. The initial distribution: \\({a}^{(0)}= \\{ {a}^{(0)}_i\\, : \\, i\\in S\\}\\), \\({a}^{(0)}_i={\\mathbb{P}}[X_0=i]\\) - the initial probability distribution of the process, and Transition probabilities: \\(p_{ij}={\\mathbb{P}}[X_{n+1}=j|X_n=i]\\) - the mechanism that the process uses to jump around. Indeed, if you know \\({a}^{(0)}_i\\) and \\(p_{ij}\\) for all \\(i,j\\in S\\) and want to compute a joint distribution \\({\\mathbb{P}}[X_n=i_n, X_{n-1}=i_{n-1}, \\dots, X_0=i_0]\\), you can use the definition of conditional probability and the Markov property several times (the multiplication theorem from your elementary probability course) as follows: \\[\\begin{align} {\\mathbb{P}}[X_n=i_n, \\dots, X_0=i_0] &amp;= {\\mathbb{P}}[X_n=i_n| X_{n-1}=i_{n-1}, \\dots,X_0=i_0] \\cdot {\\mathbb{P}}[X_{n-1}=i_{n-1}, \\dots,X_0=i_0] \\\\ &amp; = {\\mathbb{P}}[X_n=i_n| X_{n-1}=i_{n-1}] \\cdot {\\mathbb{P}}[X_{n-1}=i_{n-1}, \\dots,X_0=i_0]\\\\ &amp;= p_{i_{n-1} i_{n}} {\\mathbb{P}}[X_{n-1}=i_{n-1}, \\dots,X_0=i_0] \\end{align}\\] If we repeat the same procedure \\(n-2\\) more times (and flip the order of factors), we get \\[\\begin{align} {\\mathbb{P}}[X_n=i_n, \\dots, X _0=i_0] &amp;= {a}^{(0)}_{i_0} \\cdot p_{i_0 i_1} \\cdot p_{i_1 i_2}\\cdot \\ldots \\cdot p_{i_{n-1} i_{n}} \\end{align}\\] Think of it this way: the probability of the process taking the trajectory \\((i_0, i_1, \\dots, i_n)\\) is: the probability of starting at \\(i_0\\) (which is \\({a}^{(0)}_{i_0}\\)), multiplied by the probability of transitioning from \\(i_0\\) to \\(i_1\\) (which is \\(p_{i_0 i_1}\\)), multiplied by the probability of transitioning from \\(i_1\\) to \\(i_2\\) (which is \\(p_{i_1 i_2}\\)), etc. When \\(S\\) is finite, there is no loss of generality in assuming that \\(S=\\{1,2,\\dots, n\\}\\), and then we usually organize the entries of \\({a}^{(0)}\\) into a row vector \\[{a}^{(0)}=({a}^{(0)}_1,{a}^{(0)}_2,\\dots, {a}^{(0)}_n),\\] and the transition probabilities \\(p_{ij}\\) into a square matrix \\({\\mathbf P}\\), where \\[{\\mathbf P}=\\begin{bmatrix} p_{11} &amp; p_{12} &amp; \\dots &amp; p_{1n} \\\\ p_{21} &amp; p_{22} &amp; \\dots &amp; p_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ p_{n1} &amp; p_{n2} &amp; \\dots &amp; p_{nn} \\\\ \\end{bmatrix}\\] In the general case (\\(S\\) possibly infinite), one can still use the vector and matrix notation as before, but it becomes quite clumsy. For example, if \\(S={\\mathbb{Z}}\\), then \\({\\mathbf P}\\) is an infinite matrix \\[{\\mathbf P}=\\begin{bmatrix} \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\\\ \\dots &amp; p_{-1\\, -1} &amp; p_{-1\\, 0} &amp; p_{-1\\, 1} &amp; \\dots \\\\ \\dots &amp; p_{0\\, -1} &amp; p_{0\\, 0} &amp; p_{0\\, 1} &amp; \\dots \\\\ \\dots &amp; p_{1\\, -1} &amp; p_{1\\, 0} &amp; p_{1\\, 1} &amp; \\dots \\\\ &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\\\ \\end{bmatrix}\\] 1.2 First Examples Here are some examples of Markov chains - you will see many more in problems and later chapters. Markov chains with a small number of states are often depicted as weighted directed graphs, whose nodes are the chain’s states, and the weight of the directed edge between \\(i\\) and \\(j\\) is \\(p_{ij}\\). Such graphs are called transition graphs and are an excellent way to visualize a number of important properties of the chain. A transition graph is included for most of the examples below. Edges are color-coded according to the probability assigned to them. Black is always \\(1\\), while other colors are uniquely assigned to different probabilities (edges carrying the same probability get the same color). 1.2.1 Random walks Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a simple (possibly biased) random walk. Let us show that it indeed has the Markov property (1.1). Remember, first, that \\[X_n=\\sum_{k=1}^n \\delta_k \\text{ where }\\delta_k \\text{ are independent (possibly biased) coin-tosses.}\\] For a choice of \\(i_0, \\dots, i_n, j=i_{n+1}\\) (such that \\(i_0=0\\) and \\(i_{k+1}-i_{k}=\\pm 1\\)) we have \\[%\\label{equ:} \\nonumber \\begin{split} {\\mathbb{P}}[ X_{n+1}=i_{n+1}&amp;|X_n=i_n, X_{n-1}=i_{n-1},\\dots, X_1=i_1, X_0=i_0]\\\\ = &amp; {\\mathbb{P}}[ X_{n+1}-X_{n}=i_{n+1}-i_{n}|X_n=i_n, X_{n-1}=i_{n-1},\\dots, X_1=i_1, X_0=i_0] \\\\ = &amp; {\\mathbb{P}}[ \\delta_{n+1}=i_{n+1}-i_{n}|X_n=i_n, X_{n-1}=i_{n-1},\\dots, X_1=i_1, X_0=i_0] \\\\= &amp; {\\mathbb{P}}[ \\delta_{n+1}=i_{n+1}-i_n], \\end{split}\\] where the last equality follows from the fact that the increment \\(\\delta_{n+1}\\) is independent of the previous increments, and, therefore, also of the values of \\(X_1,X_2, \\dots, X_n\\). The last line above does not depend on \\(i_{n-1}, \\dots, i_1, i_0\\), so \\(X\\) indeed has the Markov property. The state space \\(S\\) of \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) is the set \\({\\mathbb{Z}}\\) of all integers, and the initial distribution \\({a}^{(0)}\\) is very simple: we start at \\(0\\) with probability \\(1\\) (so that \\({a}^{(0)}_0=1\\) and \\({a}^{(0)}_i=0\\), for \\(i\\not= 0\\).). The transition probabilities are simple to write down \\[p_{ij}= \\begin{cases} p, &amp; j=i+1 \\\\ q, &amp; j=i-1 \\\\ 0, &amp; \\text{otherwise.} \\end{cases}\\] If you insist, these can be written down in an infinite matrix, \\[{\\mathbf P}=\\begin{bmatrix} \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\\\ \\dots &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; \\dots \\\\ \\dots &amp; q &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; \\dots \\\\ \\dots &amp;0 &amp;q &amp; 0 &amp; p &amp; 0 &amp; \\dots \\\\ \\dots &amp;0 &amp;0 &amp; q&amp; 0 &amp; p&amp; \\dots \\\\ \\dots &amp;0 &amp; 0 &amp;0 &amp; q&amp; 0&amp; \\dots \\\\ \\dots &amp;0 &amp; 0 &amp;0 &amp; 0&amp; q&amp; \\dots \\\\ &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\\\ \\end{bmatrix}\\] but this representation is typically not as useful as in the finite case. Here is a (portion of) a transition graph for a simple random walk. Instead of writing probabilities on top of the edges, we color code them as follows: green is \\(p\\) and orange is \\(1-p\\). 1.2.2 Gambler’s ruin In Gambler’s ruin, a gambler starts with \\(\\$x\\), where \\(0\\leq x \\leq a\\in{\\mathbb{N}}\\) and in each play wins a dollar (with probability \\(p\\in (0,1)\\)) and loses a dollar (with probability \\(q=1-p\\)). When the gambler reaches either \\(0\\) or \\(a\\), the game stops. For mathematical convenience, it is usually a good idea to keep the chain defined, even after the modeled phenomenon stops. This is usually accomplished by simply assuming that the process “stays alive” but remains “frozen in place” instead of disappearing. In our case, once the gambler reaches either of the states \\(0\\) and \\(a\\), he/she simply stays there forever. Therefore, the transition probabilities are similar to those of a random walk, but differ from them at the boundaries \\(0\\) and \\(a\\). The state space is finite \\(S=\\{0,1,\\dots, a\\}\\) and the matrix \\({\\mathbf P}\\) is given by \\[{\\mathbf P}=\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 \\\\ q &amp; 0 &amp; p &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; q &amp; 0 &amp; p &amp; \\dots &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; q &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; p &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; q &amp; 0 &amp; p \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix}\\] In the picture below, green denotes the probability \\(p\\) and orange probability \\(1-p\\). As always, black is \\(1\\). The initial distribution is deterministic: \\[{a}^{(0)}_i= \\begin{cases} 1,&amp; i=x,\\\\ 0,&amp; i\\not= x. \\end{cases}\\] 1.2.3 Regime Switching Consider a system with two different states; think about a simple weather forecast (rain/no rain), high/low water level in a reservoir, high/low volatility regime in a financial market, high/low level of economic growth, the political party in power, etc. Suppose that the states are called \\(1\\) and \\(2\\) and the probabilities \\(p_{12}\\) and \\(p_{21}\\) of switching states are given. The probabilities \\(p_{11}=1-p_{12}\\) and \\(p_{22}=1-p_{21}\\) correspond to the system staying in the same state. The transition matrix for this Markov chain with \\(S=\\{1,2\\}\\) is \\[{\\mathbf P}= \\begin{bmatrix} p_{11} &amp; p_{12} \\\\ p_{21} &amp; p_{22}. \\end{bmatrix}\\] When \\(p_{12}\\) and \\(p_{21}\\) are large (close to \\(1\\)) the system nervously jumps between the two states. When they are small, there are long periods of stability (staying in the same state). One of the assumptions behind regime-switching models is that the transitions (switches) can only happen in regular intervals (once a minute, once a day, once a year, etc.). This is a feature of all discrete-time Markov chains. One would need to use a continuous-time model to allow for the transitions between states at any point in time. 1.2.4 Deterministically monotone Markov chain A stochastic process \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) with state space \\(S={\\mathbb{N}}_0\\) such that \\(X_n=n\\) for \\(n\\in{\\mathbb{N}}_0\\) (no randomness here) is called Deterministically monotone Markov chain (DMMC). The transition matrix looks like this \\[{\\mathbf P}= \\begin{bmatrix} 0 &amp; 1 &amp; 0 &amp; 0 &amp; \\dots \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; \\dots \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\dots \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix}\\] It is a pretty boring chain; its main use is as a counterexample. 1.2.5 Not a Markov chain Consider a frog jumping from a lily pad to a lily pad in a small forest pond. Suppose that there are \\(N\\) lily pads so that the state space can be described as \\(S=\\{1,2,\\dots, N\\}\\). The frog starts on lily pad 1 at time \\(n=0\\), and jumps around in the following fashion: at time \\(0\\) it chooses any lily pad except for the one it is currently sitting on (with equal probability) and then jumps to it. At time \\(n&gt;0\\), it chooses any lily pad other than the one it is sitting on and the one it visited immediately before (with equal probability) and jumps to it. The position \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) of the frog is not a Markov chain. Indeed, we have \\[{\\mathbb{P}}[X_3=1|X_2=2, X_1=3]= \\frac{1}{N-2},\\] while \\[{\\mathbb{P}}[X_3=1|X_2=2, X_1=1]=0.\\] A more dramatic version of this example would be the one where the frog remembers all the lily pads it had visited before, and only chooses among the remaining ones for the next jump. 1.2.6 Turning a non-Markov chain into a Markov chain How can we turn the process the previous example into a Markov chain. Obviously, the problem is that the frog has to remember the number of the lily pad it came from in order to decide where to jump next. The way out is to make this information a part of the state. In other words, we need to change the state space. Instead of just \\(S=\\{1,2,\\dots, N\\}\\), we set \\(S= \\{ (i_1, i_2)\\, : \\, i_1,i_2 \\in\\{1,2,\\dots N\\}\\}\\). In words, the state of the process will now contain not only the number of the current lily pad (i.e., \\(i_2\\)) but also the number of the lily pad we came from (i.e., \\(i_1\\)). This way, the frog will be in the state \\((i_1,i_2)\\) if it is currently on the lily pad number \\(i_2\\), and it arrived here from \\(i_1\\). There is a bit of freedom with the initial state, but we simply assume that we start from \\((1,1)\\). Starting from the state \\((i_1,i_2)\\), the frog can jump to any state of the form \\((i_2, i_3)\\), \\(i_3\\not= i_1,i_2\\) (with equal probabilities). Note that some states will never be visited (like \\((i,i)\\) for \\(i\\not = 1\\)), so we could have reduced the state space a little bit right from the start. It is important to stress that the passage to the new state space defines a whole new stochastic process. It is therefore, not quite accurate, as the title suggests, to say that we “turned” a non-Markov process into a Markov process. Rather, we replaced a non-Markovian model of a given situation by a different, Markovian, one. 1.2.7 Deterministic functions of Markov chains do not need to be Markov chains Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a Markov chain on the state space \\(S\\), and let \\(f:S\\to T\\) be a function. The stochastic process \\(Y_n= f(X_n)\\) takes values in \\(T\\); is it necessarily a Markov chain? We will see in this example that the answer is no. Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a simple symmetric random walk, with the usual state space \\(S = {\\mathbb{Z}}\\). With \\(r(m) = m\\ (\\text{mod } 3)\\) denoting the remainder after the division by \\(3\\), we first define the process \\(R_n = r(X_n)\\) so that \\[R_n=\\begin{cases} 0, &amp; \\text{ if $X_n$ is divisible by 3,}\\\\ 1, &amp; \\text{ if $X_n-1$ is divisible by 3,}\\\\ 2, &amp; \\text{ if $X_n-2$ is divisible by 3.} \\end{cases}\\] Using \\(R_n\\) we define \\(Y_n = (X_n-R_n)/3\\) to be the corresponding quotient, so that \\(Y_n\\in{\\mathbb{Z}}\\) and \\[3 Y_n \\leq X_n &lt;3 (Y_n+1).\\] The process \\(Y\\) is of the form \\(Y_n = f(X_n)\\), where \\(f(i)= \\lfloor i/3 \\rfloor\\), and \\(\\lfloor x \\rfloor\\) is the largest integer not exceeding \\(x\\). To show that \\(Y\\) is not a Markov chain, let us consider the the event \\(A=\\{Y_2=0, Y_1=0\\}\\). The only way for this to happen is if \\(X_1=1\\) and \\(X_2=2\\) or \\(X_1=1\\) and \\(X_2=0\\), so that \\(A=\\{X_1=1\\}\\). Also \\(Y_3=1\\) if and only if \\(X_3=3\\). Therefore \\[{\\mathbb{P}}[ Y_3=1|Y_2=0, Y_1=0]={\\mathbb{P}}[ X_3=3| X_1=1]= 1/4.\\] On the other hand, \\(Y_2=0\\) if and only if \\(X_2=0\\) or \\(X_2=2\\), so \\({\\mathbb{P}}[Y_2=0]= 3/4\\). Finally, \\(Y_3=1\\) and \\(Y_2=0\\) if and only if \\(X_3=3\\) and so \\({\\mathbb{P}}[Y_3=1, Y_2=0]= 1/8\\). Hence, \\[{\\mathbb{P}}[ Y_3=1|Y_2=0]={\\mathbb{P}}[Y_3=1, Y_2=0]/{\\mathbb{P}}[Y_2=0]= \\frac{1/8}{3/4}= \\frac{1}{6}.\\] Therefore, \\(Y\\) is not a Markov chain. If you want a more dramatic example, try to modify this example so that one of the probabilities above is positive, but the other is zero. The important property of the function \\(f\\) we applied to \\(X\\) is that it is not one-to-one. In other words, \\(f\\) collapses several states of \\(X\\) into a single state of \\(Y\\). This way, the “present” may end up containing so little information that the past suddenly becomes relevant for the dynamics of the future evolution. 1.2.8 A game of tennis In a game of tennis, the scoring system is as follows: both players start with the score of \\(0\\). Each time player 1 wins a point (a.k.a. a rally), her score moves a step up in the following hierarchy \\[0 \\mapsto 15 \\mapsto 30 \\mapsto 40.\\] Once she reaches \\(40\\) and scores a point, three things can happen: if the score of player 2 is \\(30\\) or less, player 1 wins the game. if the score of player 2 \\(40\\), the score of player 1 moves up to “advantage”, and if the score of player 2 is “advantage”, nothing happens to the score of player 1 but the score of player 2 falls back to \\(40\\). Finally, if the score of player 1 is “advantage” and she wins a point, she wins the game. The situation is entirely symmetric for player 2. We suppose that the probability that player 1 wins each point is \\(p\\in (0,1)\\), independently of the current score. A situation like this is a typical example of a Markov chain in an applied setting. What are the states of the process? We obviously need to know both players’ scores and we also need to know if one of the players has won the game. Therefore, a possible state space is the following: \\[\\begin{align} S= \\Big\\{ &amp;(0,0), (0,15), (0,30), (0,40), (15,0), (15,15), (15,30), (15,40), (30,0), (30,15),\\\\ &amp; (30,30), (30,40), (40,0), (40,15), (40,30), (40,40), (40,A), (A,40), W_1, W_2 \\Big\\} \\end{align}\\] where \\(A\\) stands for “advantage” and \\(W_1\\) (resp., \\(W_2\\)) denotes the state where player 1 (resp., player 2) wins. It is not hard to assign probabilities to transitions between states. Once we reach either \\(W_1\\) or \\(W_2\\) the game stops. We can assume that the chain remains in that state forever, i.e., the state is absorbing. The initial distribution is quite simple - we always start from the same state \\((0,0)\\), so that \\({a}^{(0)}_{(0,0)}=1\\) and \\({a}^{(0)}_i=0\\) for all \\(i\\in S\\setminus\\{(0,0)\\}\\). How about the transition matrix? When the number of states is big (\\(\\# S=20\\) in this case), transition matrices are useful in computer memory, but not so much on paper. Just for the fun of it, here is the transition matrix for our game-of-tennis chain (I am going to leave it up to you to figure out how rows/columns of the matrix match to states) \\[\\tiny {\\mathbf P}= \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 \\\\ p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; 0 \\\\ p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; p \\\\ 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 \\\\ p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 \\\\ \\end{bmatrix} \\] Does the structure of a game of tennis make is easier or harder for the better player to win? In other words, if you had to play against the best tennis player in the world (I am rudely assuming that he or she is better than you), would you have a better chance of winning if you only played a point (rally), or if you played the whole game? We will give a precise answer to this question in a little while. In the meantime, try to guess. 1.3 Chapman-Kolmogorov equations The transition probabilities \\(p_{ij}\\), \\(i,j\\in S\\) tell us how a Markov chain jumps from one state to another in a single step. Think of it as a description of the local behavior of the chain. This is the information one can usually obtain from observations and modeling assumptions. On the other hand, it is the global (long-time) behavior of the model that provides the most interesting insights. In that spirit, we turn our attention to probabilities like this: \\[{\\mathbb{P}}[X_{k+n}=j|X_k=i] \\text{ for } n = 1,2,\\dots.\\] Since we are assuming that all of our chains are homogeneous (transition probabilities do not change with time), this probability does not depend on the time \\(k\\), so we can define the multi-step transition probabilities \\(p^{(n)}_{ij}\\) as follows: \\[p^{(n)}_{ij}={\\mathbb{P}}[X_{k+n}=j|X_{k}=i]={\\mathbb{P}}[ X_{n}=j|X_0=i].\\] We allow \\(n=0\\) under the useful convention that \\[p^{(0)}_{ij}=\\begin{cases} 1, &amp; i=j,\\\\ 0,&amp; i\\not = j. \\end{cases}\\] We note right away that the numbers \\(p^{(n)}_{ij}\\), \\(i,j\\in S\\) naturally fit into an \\(N\\times N\\)-matrix which we denote by \\({\\mathbf P}^{(n)}\\). We note right away that \\[\\begin{equation} {\\mathbf P}^{(0)}= \\operatorname{Id}\\text{ and } {\\mathbf P}^{(1)}= {\\mathbf P}, \\tag{1.3} \\end{equation}\\] where \\(\\operatorname{Id}\\) denotes the \\(N\\times N\\) identity matrix. The central result of this section is the following sequence of equalities connecting \\({\\mathbf P}^{(n)}\\) for different values of \\(n\\), know as the Chapman-Kolmogorov equations: \\[\\begin{equation} {\\mathbf P}^{(m+n)} = {\\mathbf P}^{(m)} {\\mathbf P}^{(n)}, \\text{ for all } m,n \\in {\\mathbb{N}}_0. \\tag{1.4} \\end{equation}\\] To see why this is true we start by computing \\({\\mathbb{P}}[ X_{n+m} = j, X_0=i]\\). Since each trajectory from \\(i\\) to \\(j\\) in \\(n+m\\) steps has be somewhere at time \\(n\\), we can write \\[\\begin{equation} {\\mathbb{P}}[ X_{n+m}= j, X_0 = i] = \\sum_{k\\in S} {\\mathbb{P}}[X_{n+m} = j, X_{n} = k, X_0 = i]. \\tag{1.5} \\end{equation}\\] By the multiplication rule, we have \\[\\begin{multline} {\\mathbb{P}}[X_{n+m} = j, X_{n} = k, X_0 = i] = {\\mathbb{P}}[ X_{n+m} = j | X_{n}=k, X_{0}=i] {\\mathbb{P}}[X_{n}=k, X_0 = i], \\tag{1.6} \\end{multline}\\] and then, by the Markov property: \\[\\begin{equation} {\\mathbb{P}}[ X_{n+m} = j | X_{n}=k, X_{0}=i] = {\\mathbb{P}}[ X_{n+m} = j | X_n = k]. \\tag{1.7} \\end{equation}\\] Combining (1.5), (1.6) and (1.7) we obtain the following equality: \\[\\begin{equation} {\\mathbb{P}}[ X_{n+m}= j, X_0 = i] = \\sum_{k\\in S} {\\mathbb{P}}[ X_{n+m} = j | X_n = k] {\\mathbb{P}}[X_{n}=k, X_0 = i]. \\end{equation}\\] which is nothing but (1.4); to see that, just remember how matrices are multiplied. The punchline is that (1.4), together with (1.3) imply that \\[\\begin{equation} {\\mathbf P}^{(n)}= {\\mathbf P}^n, \\tag{1.8} \\end{equation}\\] where the left-hand side is the matrix composed of the \\(n\\)-step transition probabilities, and the right hand side is the \\(n\\)-th (matrix) power of the (\\(1\\)-step) transition matrix \\({\\mathbf P}\\). Using (1.8) allows us to write a simple expression for the distribution of the random variable \\(X_n\\), for \\(n\\in{\\mathbb{N}}_0\\). Remember that the initial distribution (the distribution of \\(X_0\\)) is denoted by \\({a}^{(0)}=({a}^{(0)}_i)_{i\\in S}\\). Analogously, we define the vector \\({a}^{(n)}=({a}^{(n)}_i)_{i\\in S}\\) by \\[{a}^{(n)}_i={\\mathbb{P}}[X_n=i],\\ i\\in S.\\] Using the law of total probability, we have \\[{a}^{(n)}_i={\\mathbb{P}}[X_n=i]=\\sum_{k\\in S} {\\mathbb{P}}[ X_0=k] {\\mathbb{P}}[ X_n=i|X_0=k]= \\sum_{k\\in S} {a}^{(0)}_k p^{(n)}_{ki}.\\] We usually interpret \\({a}^{(0)}\\) as a (row) vector, so the above relationship can be expressed using vector-matrix multiplication \\[{a}^{(n)}={a}^{(0)}{\\mathbf P}^n.\\] Find an explicit expression for \\({\\mathbf P}^{(n)}\\) in the case of the regime-switching chain introduced above. Feel free to assume that \\(p_{ij}&gt;0\\) for all \\(i,j\\). It is often difficult to compute \\({\\mathbf P}^n\\) for a general transition matrix \\({\\mathbf P}\\) and a large \\(n\\). We will see later that it will be easier to find the limiting values \\(\\lim_{n\\to\\infty}p^{(n)}_{ij}\\). The regime-switching chain is one of the few examples where everything can be done by hand. By (1.8), we need to compute the \\(n\\)-th matrix power of the transition matrix \\({\\mathbf P}\\). To make the notation a bit nicer, let us write \\(a\\) for \\(p_{12}\\) and \\(b\\) for \\(p_{21}\\), so that we can write \\[{\\mathbf P}= \\begin{bmatrix} 1-a &amp; a \\\\ b &amp; 1-b \\end{bmatrix}\\] The winning idea is to use diagonalization, and for that we start by writing down the characteristic equation \\(\\det (\\lambda I-{\\mathbf P})=0\\) of the matrix \\({\\mathbf P}\\): \\[\\label{equ:} \\nonumber \\begin{split} 0&amp;=\\det(\\lambda I-{\\mathbf P})= \\begin{vmatrix} \\lambda-1+a &amp; -a \\\\ -b &amp; \\lambda-1+b \\end{vmatrix}\\\\ &amp; =((\\lambda-1)+a)((\\lambda-1)+b)-ab =(\\lambda-1)(\\lambda-(1-a-b)). \\end{split}\\] The eigenvalues are, therefore, \\(\\lambda_1=1\\) and \\(\\lambda_2=1-a-b\\), and the corresponding eigenvectors are \\(v_1=\\binom{1}{1}\\) and \\(v_2=\\binom{a}{-b}\\). Therefore, if we define \\[V= \\begin{bmatrix} 1 &amp; a \\\\ 1 &amp; -b \\end{bmatrix} \\text{ and }D= \\begin{bmatrix} \\lambda_1 &amp; 0 \\\\ 0 &amp; \\lambda_2 \\end{bmatrix}= \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; (1-a-b) \\end{bmatrix}\\] we have \\[{\\mathbf P}V = V D,\\text{ i.e., } {\\mathbf P}= V D V^{-1}.\\] This representation is very useful for taking (matrix) powers: \\[\\label{equ:60C4} \\begin{split} {\\mathbf P}^n &amp;= (V D V^{-1})( V D V^{-1}) \\dots (V D V^{-1})= V D^n V^{-1} \\\\ &amp; = V \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; (1-a-b)^n \\end{bmatrix} V^{-1} \\end{split}\\] We assumed that all \\(p_{ij}\\) are positive which means, in particular, that \\(a+b&gt;0\\) and \\[V^{-1} = \\tfrac{1}{a+b} \\begin{bmatrix} b &amp; a \\\\ 1 &amp; -1 \\end{bmatrix},\\] and so \\[\\begin{align} {\\mathbf P}^n &amp;= V D^n V^{-1}= \\begin{bmatrix} 1 &amp; a \\\\ 1 &amp; -b \\end{bmatrix} \\ \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; (1-a-b)^n \\end{bmatrix} \\ \\tfrac{1}{a+b} \\begin{bmatrix} b &amp; a \\\\ 1 &amp; -1 \\end{bmatrix}\\\\ &amp;= \\frac{1}{a+b} \\begin{bmatrix} b &amp; a \\\\ b &amp; a \\end{bmatrix} + \\frac{(1-a-b)^n}{a+b} \\begin{bmatrix} a &amp; -a \\\\ b &amp; -b \\end{bmatrix}\\\\ &amp;= \\begin{bmatrix} \\frac{b}{a+b}+(1-a-b)^n \\frac{a}{a+b} &amp; \\frac{a}{a+b}-(1-a-b)^n \\frac{a}{a+b}\\\\ \\frac{b}{a+b}+(1-a-b)^n \\frac{b}{a+b} &amp; \\frac{a}{a+b}-(1-a-b)^n \\frac{b}{a+b} \\end{bmatrix} \\end{align}\\] The expression for \\({\\mathbf P}^n\\) above tells us a lot about the structure of the multi-step probabilities \\(p^{(n)}_{ij}\\) for large \\(n\\). Note that the second matrix on the right-hand side above comes multiplied by \\((1-a-b)^n\\) which tends to \\(0\\) as \\(n\\to\\infty\\) (under our assumptions that \\(p_{ij}&gt;0\\).) We can, therefore, write \\[{\\mathbf P}^n\\sim \\frac{1}{a+b} \\begin{bmatrix} b &amp; a \\\\ b &amp; a \\end{bmatrix} \\text{ for large } n.\\] The fact that the rows of the right-hand side above are equal points to the fact that, for large \\(n\\), \\(p^{(n)}_{ij}\\) does not depend (much) on the initial state \\(i\\). In other words, this Markov chain forgets its initial condition after a long period of time. This is a rule more than an exception, and we will study such phenomena in the following lectures. 1.4 How to simulate Markov chains One of the (many) reasons Markov chains are a popular modeling tool is the ease with which they can be simulated. When we simulated a random walk, we started at \\(0\\) and built the process by adding independent coin-toss-distributed increments. We obtained the value of the next position of the walk by adding the present position and the value of an independent random variable. For general Markov chain, this procedure works almost verbatim, except that the function that combines the present position and a value of an independent random variable may be something other than addition. In general, we collapse the two parts of the process - a simulation of an independent random variable and its combination with the present position - into one. Given our position, we pick the row of the transition matrix that corresponds to it and then use its elements as the probabilities that govern our position tomorrow. It will all be clear once you read through the solution of the following problem. Simulate \\(1000\\) trajectories of a gambler’s ruin Markov chain with \\(a=3\\), \\(p=2/3\\) and \\(x=1\\) (see subsection 1.2.2 above for the meaning of these constants). Use the Monte Carlo method to estimate the probability that the gambler will leave the casino with \\(\\$3\\) in her pocket in at most \\(T=100\\) time periods. # state space S = c(0, 1, 2, 3) # transition matrix P = matrix(c(1, 0, 0, 0, 1/3, 0, 2/3, 0, 0, 1/3, 0, 2/3, 0, 0, 0, 1), byrow=T, ncol=4) T = 100 # number of time periods nsim = 1000 # number of simulations # simulate the next position of the chain draw_next = function(s) { i = match(s, S) # the row number of the state s sample(S, prob = P[i, ], size = 1) } # simulate a single trajectory of length T # from the initial state single_trajectory = function(initial_state) { path = numeric(T) last = initial_state for (n in 1:T) { path[n] = draw_next(last) last = path[n] } return(path) } # simulate the entire chain simulate_chain = function(initial_state) { data.frame(X0 = initial_state, t(replicate( nsim, single_trajectory(initial_state) ))) } df = simulate_chain(1) (p = mean(df$X100 == 3)) ## [1] 0.591 R. The function draw_next is at the heart of the simulation. Given the current state s, it looks up the row of the transition matrix P which corresponds to s. This is where the function match comes in handy - match(s,S) gives you the position of th element s in the vector S. Of course, if \\(S = \\{ 1,2,3, \\dots, n\\}\\) then we don’t need to use match, as each state is “its own position”. In our case, S is a bit different, namely \\(S=\\{0,1,2,3\\}\\), and so match(s,S) is nothing by s+1. This is clearly an overkill in this case, but we still do it for didactical purposes. Once the row corresponding to the state s is identified, we use its elements as the probabilities to be fed into the command sample, which, in turn, returns our next state and we repeat the procedure over and over (in this case \\(T=100\\) times). 1.5 Additional problems for Chapter 5 Let \\(\\{Y_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a sequence of die-rolls, i.e., a sequence of independent random variables which take values \\(1,2,\\dots, 6\\), each with probability \\(1/6\\). Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a stochastic process defined by \\[X_n=\\max (Y_0,Y_1, \\dots, Y_n), \\ n\\in{\\mathbb{N}}_0.\\] In words, \\(X_n\\) is the maximal value rolled so far. Is \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) a Markov chain? If it is, find its transition matrix and the initial distribution. If it is not, give an example of how the Markov property is violated. It turns out that \\(\\{X_n\\}_{n\\in{\\mathbb{N}}}\\) is, indeed, a Markov chain. The value of \\(X_{n+1}\\) is either going to be equal to \\(X_n\\) if \\(Y_{n+1}\\) happens to be less than or equal to it, or it moves up to \\(Y_{n+1}\\), otherwise, i.e., \\(X_{n+1}=\\max(X_n,Y_{n+1})\\). Therefore, the distribution of \\(X_{n+1}\\) depends on the previous values \\(X_0,X_1,\\dots, X_n\\) only through \\(X_n\\), and, so, \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) is a Markov chain on the state space \\(S=\\{1,2,3,4,5,6\\}\\). The transition matrix is given by \\[P=\\begin{bmatrix} 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\\\ 0 &amp; 1/3 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\\\ 0 &amp; 0 &amp; 1/2 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\\\ 0 &amp; 0 &amp; 0 &amp; 2/3 &amp; 1/6 &amp; 1/6 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 5/6 &amp; 1/6 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\\\ \\end{bmatrix}\\] Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a simple symmetric random walk. For \\(n\\in{\\mathbb{N}}_0\\), define \\(Y_n = 2X_n+1\\), and let \\(Z_n\\) be the amount of time \\(X_n\\) spent strictly above \\(0\\) up to (and including) time \\(n\\), i.e. \\[Z_0=0, Z_{n+1} - Z_n = \\begin{cases} 1, &amp; X_{n+1}&gt;0 \\\\ 0, &amp; X_ {n+1}\\leq 0 \\end{cases} , \\text{ for }n\\in{\\mathbb{N}}_0.\\] Is \\(Y\\) a Markov chain? Is \\(Z\\)? \\(Y\\) is a Markov chain because it is just a random walk started at \\(1\\) with steps of size \\(2\\) (a more rigorous proof would follow the same line of reasoning as the proof that random walks are Markov chains). \\(Z\\) is not a Markov chain because the knowledge of far history (beyond the present position) affects the likelihood of the next transition as the following example shows: \\[\\begin{aligned} {\\mathbb{P}}[ Z_4=2| Z_0=0, Z_1=0, Z_2=0, Z_3=1]=1/2\\end{aligned}\\] but \\[\\begin{aligned} {\\mathbb{P}}[ Z_4=2| Z_0=0, Z_1=1, Z_2=1, Z_3=1]= 0.\\end{aligned}\\] Let \\(\\{\\delta_n\\}_{n\\in{\\mathbb{N}}}\\) be a sequence of independent coin tosses (i.e., random variables with values \\(T\\) or \\(H\\) with equal probabilities). Let \\(X_0=0\\), and, for \\(n\\in{\\mathbb{N}}\\), let \\(X_n\\) be the number of times two consecutive \\(\\delta\\)s take the same value in the first \\(n+1\\) tosses. For example, if the outcome of the coin tosses is TTHHTTTH …, we have \\(X_0=0\\), \\(X_1=1\\), \\(X_2=1\\), \\(X_3=2\\), \\(X_4=2\\), \\(X_5=3\\), \\(X_6=4\\), \\(X_7=4\\), … Is \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) a Markov chain? If it is, describe its state space, the transition probabilities and the initial distribution. If it is not, show exactly how the Markov property is violated. Yes, the process \\(X\\) is a Markov chain, on the state space \\(S={\\mathbb{N}}_0\\). To show that we make the following simple observation: we have \\(X_{n}-X_{n-1}=1\\) if \\(\\delta_n=\\delta_{n+1}\\) and \\(X_n-X_{n-1}=0\\), otherwise (for \\(n\\in{\\mathbb{N}}\\)). Therefore, \\[{\\mathbb{P}}[ X_{n+1}=i_n+1 | X_{n}=i_n, \\dots, X_1=i_1, X_0=0] = {\\mathbb{P}}[ \\delta_{n+2}=\\delta_{n+1} | X_{n}=i_n, \\dots, X_1=i_1,X_0=0].\\] Even if we knew the exact values of all \\(\\delta_1,\\dots, \\delta_n,\\delta_{n+1}\\), the (conditional) probability that \\(\\delta_{n+2}=\\delta_{n+1}\\) would still be \\(1/2\\), regardless of these values. Therefore, \\[{\\mathbb{P}}[ X_{n+1}=i_n+1| X_n=i_n,\\dots, X_1=i_1, X_0=0] = \\tfrac{1}{2},\\] and, similarly, \\[{\\mathbb{P}}[ X_{n+1}=i_n| X_n=i_n,\\dots, X_1=i_1, X_0=0] = \\tfrac{1}{2}.\\] Therefore, the conditional probability given all the past depends on the past only through the value of \\(X_n\\) (the current position), and we conclude that \\(X\\) is, indeed, a Markov process. Its initial distribution is deterministic \\({\\mathbb{P}}[X_0=0]=1\\), and the transition probabilities, as computed above, are \\[p_{ij}={\\mathbb{P}}[ X_{n+1}=j| X_n=i] = \\begin{cases} 1/2, &amp;\\text{ if } j=i+1, \\\\ 1/2, &amp;\\text{ if } j=i, \\\\ 0, &amp;\\text{ otherwise.} \\end{cases}\\] In fact, \\(2 X_n - n\\) is a simple symmetric random walk. Let \\(X\\) be a Markov chain on \\(N\\) states, with the \\(N\\times N\\) transition matrix \\(P\\). We construct a new Markov chain \\(Y\\) from the transition mechanism of \\(X\\) as follows: at each point in time, we toss a biased coin (probability of heads \\(p\\in (0,1)\\)), independently of everything else. If it shows heads we move according to the transition matrix of \\(X\\). If it shows tails, we remain in the same state. What is the transition matrix of \\(Y\\)? Let \\(Q=(q_{ij})\\) denote the transition probability for the chain \\(Y\\). When \\(i\\ne j\\), the chain \\(Y\\) will go from \\(i\\) to \\(j\\) in one step if and only if the coin shows heads and the chain \\(X\\) wants to jump from \\(i\\) to \\(j\\). Since the two events are independent, the probability of the former is \\(p\\), and of the later is \\(p_{ij}\\), we have \\(q_{ij} = p p_{ij}\\). In the case \\(i=j\\), the chain \\(Y\\) will transition from \\(i\\) to \\(i\\) (i.e., stay in \\(i\\)) if either the coin shows heads, or if the coin shows tails and the chain \\(X\\) decides to stay in \\(i\\). Therefore, \\(q_{ii} = p + (1-p) p_{ij}\\), i.e., \\[ Q = p \\operatorname{Id}+(1-p) P,\\] where \\(\\operatorname{Id}\\) denotes \\(N\\times N\\) identity matrix. The red container has 100 red balls, and the blue container has 100 blue balls. In each step - a container is selected (with equal probabilities), - a ball is selected from it (all balls in the container are equally likely to be selected), and - the selected ball is placed in the other container. If the selected container is empty, no ball is transferred. Once there are 100 blue balls in the red container and 100 red balls in the blue container, the game stops. We decide to model the situation as a Markov chain. What is the state space \\(S\\) we can use? How large is it? What is the initial distribution? What are the transition probabilities between states? Don’t write the matrix, it is way too large; just write a general expression for \\(p_{ij}\\), \\(i,j\\in S\\). (Note: this is a version of the famous Ehrenfest Chain from statistical physics.) There are many ways in which one can solve this problem. Below is just one of them. 1. In order to describe the situation being modeled, we need to keep track of the number of balls of each color in each container. Therefore, one possibility is to take the set of all quadruplets \\((r,b,R,B)\\), \\(r,b,R,b\\in \\{0,1,2,\\dots, 100\\}\\) and this state space would have \\(101^4\\) elements. We know, however, that the total number of red balls, and the total number of blue balls is always equal to 100, so the knowledge of the composition of the red (say) container is enough to reconstruct the contents of the blue container. In other words, we can use the number of balls of each color in the red container only as our state, i.e. \\[S= \\{ (r,b)\\, : \\, r,b=0,1,\\dots, 100\\}.\\] This state space has \\(101\\times 101=10201\\) elements. 2. The initial distribution is deterministic: \\({\\mathbb{P}}[X_0=(100,0)]=1\\) and \\({\\mathbb{P}}[X_0=i]=0\\), for \\(i\\in S\\setminus\\{(100,0)\\}\\). In the vector notation, \\[{a}^{(0)}=(0,0, \\dots, 0, 1, 0, \\dots, 0),\\] where \\(1\\) is at the place corresponding to \\((100,0)\\). 3. Let us consider several separate cases, with the understanding that \\(p_{ij}=0\\), for all \\(i,j\\) not mentioned explicitly below: One of the containers is empty. In that case, we are either in \\((0,0)\\) or in \\((100,100)\\). Let us describe the situation for \\((0,0)\\) first. If we choose the red container - and that happens with probability \\(\\tfrac{1}{2}\\) - we stay in \\((0,0)\\): \\[p_{(0,0),(0,0)}=\\tfrac{1}{2}.\\] If the blue container is chosen, a ball of either color will be chosen with probability \\(\\tfrac{100}{200}=\\tfrac{1}{2}\\), so \\[p_{(0,0),(1,0)}=p_{(0,0),(0,1)}=\\tfrac{1}{4}.\\] By the same reasoning, \\[p_{(100,100),(0,0)}=\\tfrac{1}{2}\\text{ and } p_{(100,100),(99,100)}=p_{(100,100),(100,99)}=\\tfrac{1}{4}.\\] We are in the state \\((0,100)\\). By the description of the model, this is an absorbing state, so \\(p_{(0,100),(0,100)}=1.\\) All other cases Suppose we are in the state \\((r,b)\\) where \\((r,b)\\not\\in\\{(0,100),(0,0),(100,100)\\}\\). If the red container is chosen, then the probability of getting a red ball is \\(\\tfrac{r}{r+b}\\), so \\[p_{(r,b),(r-1,b)}= \\tfrac{1}{2}\\tfrac{r}{r+b}.\\] Similarly, \\[p_{(r,b),(r,b-1)}= \\tfrac{1}{2}\\tfrac{b}{r+b}.\\] In the blue container there are \\(100-r\\) red and \\(100-b\\) blue balls. Thus, \\[p_{(r,b),(r+1,b)}= \\tfrac{1}{2}\\tfrac{100-r}{200-r-b},\\] and \\[p_{(r,b),(r,b+1)}= \\tfrac{1}{2}\\tfrac{100-b}{200-r-b}.\\] A “deck” of cards starts with 2 red and 2 black cards. A “move” consists of the following: - pick a random card from the deck (if the deck is empty, do nothing), - if the card is black and the card drawn on the previous move was also black, return it back to the deck, - otherwise, throw the card away (this, in particular, applies to any card drawn on the first move, since there is no “previous” move at that time). Model the situation using a Markov chain: find an appropriate state space, and sketch the transition graph with transition probabilities. How small can you make the state space? What is the probability that the deck will be empty after exactly \\(4\\) moves? What is the probability that the deck will be empty eventually? 1. We need to keep track of the number of remaining cards of each color in the deck, as well as the color of the last card we picked (except at the beginning or when the deck is empty, when it does not matter). Therefore, the initial state will be \\((2,2)\\), the empty-deck state will be \\((0,0)\\) and the other states will be triplets of the form \\((\\#r, \\#b, c)\\), where \\(\\#r\\) and \\(\\#b\\) denote the number of cards (red and black) in the deck, and \\(c\\) is the color, \\(R\\) or \\(B\\), of the last card we picked. This way, the initial guess for the state space would be \\[\\begin{aligned} S_0 = \\{&amp;(2,2), (0,0),\\\\ &amp; (2,1,B), (2,1,R), (1,2,B), (1,2,R),\\\\ &amp; (1,1,B), (1,1,R), (0,2,B), (2,0,R), (2,0,B), (0,2,R),\\\\ &amp; (0,1,B), (0,1,R), (1,0,B), (1,0,R) \\} \\end{aligned}\\] In order to decrease the size of the state space, we start the chain at \\((2,2)\\) and consider all trajectories it is possible to take from there. It turns out that states \\((2,1,R), (1,2,B), (0,2,B), (2,0,R), (2,0,B)\\) and \\((1,0,R)\\) can never be reached from \\((2,2)\\), so we might as well leave them out of the state space. That reduces the initial guess \\(S_0\\) to a smaller \\(10\\)-state, version \\[\\begin{equation} S = \\{(2,2), (0,0), (2,1,B), (1,2,R), (1,1,B), (1,1,R), (0,2,R), (0,1,B), (0,1,R), (1,0,B) \\} \\end{equation}\\] with the following transition graph: You could further reduce the number of states to \\(9\\) by removing the initial state \\((2,2)\\) and choosing a non-deterministic distribution over the states that can be reached from them. There is something unsatisfying about that, though. 2. To get from \\((2,2)\\) to \\((0,0)\\) in exactly four steps, we need to follow one of the following three paths: \\[\\begin{aligned} &amp; (2,2) \\to (2,1,B) \\to (1,1,R) \\to (1,0,B) \\to (0,0), \\\\ &amp; (2,2) \\to (2,1,B) \\to (1,1,R) \\to (0,1,R) \\to (0,0), \\text{ or }\\\\ &amp; (2,2) \\to (1,2,R) \\to (1,1,B) \\to (0,1,R) \\to (0,0). \\\\ \\end{aligned}\\] Their respective probabilities happen to be the same, namely \\(\\tfrac{1}{2}\\times \\tfrac{2}{3} \\times \\tfrac{1}{2}\\times 1 = \\frac{1}{6}\\), so the probability of hitting \\((0,0)\\) in exactly \\(4\\) steps is \\(3 \\times \\frac{1}{6} = \\tfrac{1}{2}\\). To compute the probability of hitting \\((0,0)\\) eventually, we note that this is guaranteed to happen sooner or later (see the graph above) if the first card we draw is black. It is also guaranteed to happen is the first card we draw is red, but the second one is black. In fact, the only way for this not to happen is to draw two red cards on the first two draws. This happens with probability \\(\\tfrac{1}{2}\\times \\frac{1}{3} = \\frac{1}{6}\\), so the required probability of ending up with an empty deck is \\(1 - \\frac{1}{6} = \\frac{5}{6}\\). A country has \\(m+1\\) cities (\\(m\\in{\\mathbb{N}}\\)), one of which is the capital. There is a direct railway connection between each city and the capital, but there are no tracks between any two “non-capital” cities. A traveler starts in the capital and takes a train to a randomly chosen non-capital city (all cities are equally likely to be chosen), spends a night there and returns the next morning and immediately boards the train to the next city according to the same rule, spends the night there, …, etc. We assume that her choice of the city is independent of the cities visited in the past. Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be the number of visited non-capital cities up to (and including) day \\(n\\), so that \\(X_0=1\\), but \\(X_1\\) could be either \\(1\\) or \\(2\\), etc. Explain why \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) is a Markov chain on the appropriate state space \\({\\mathcal{S}}\\) and the find the transition probabilities of \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\), i.e., write an expression for \\[{\\mathbb{P}}[X_{n+1}=j|X_n=i], \\text{ for $i,j\\in S$.}\\] Let \\(\\tau_m\\) be the first time the traveler has visited all \\(m\\) non-capital cities, i.e. \\[\\tau_m=\\min \\{ n\\in{\\mathbb{N}}_0\\, : \\, X_n=m\\}.\\] What is the distribution of \\(\\tau_m\\), for \\(m=1\\) and \\(m=2\\). Compute \\({\\mathbb{E}}[\\tau_m]\\) for general \\(m\\in{\\mathbb{N}}\\). What is the asymptotic behavior of \\({\\mathbb{E}}[\\tau_m]\\) as \\(m\\to\\infty\\)? More precisely, find a simple function \\(f(m)\\) of \\(m\\) (like \\(m^2\\) or \\(\\log(m)\\)) such that \\({\\mathbb{E}}[\\tau_m] \\sim f(m)\\), i.e., \\(\\lim_{m\\to\\infty} \\frac{{\\mathbb{E}}[\\tau_m]}{f(m)} = 1\\). 1. The natural state space for \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) is \\(S=\\{1,2,\\dots, m\\}\\). It is clear that \\({\\mathbb{P}}[X_{n+1}=j|X_n=i]=0,\\) unless, \\(i=j\\) or \\(i=j+1\\). If we start from the state \\(i\\), the process will remain in \\(i\\) if the traveler visits one of the already-visited cities, and move to \\(i+1\\) is the visited city has never been visited before. Thanks to the uniform distribution in the choice of the next city, the probability that a never-visited city will be selected is \\(\\tfrac{m-i}{m}\\), and it does not depend on the (names of the) cities already visited, or on the times of their first visits; it only depends on their number. Consequently, the extra information about \\(X_1,X_2,\\dots, X_{n-1}\\) will not change the probability of visiting \\(j\\) in any way, which is exactly what the Markov property is all about. Therefore, \\(\\{X_n\\}_{n\\in{\\mathbb{N}}}\\) is Markov and its transition probabilities are given by \\[p_{ij}={\\mathbb{P}}[X_{n+1}=j|X_{n}=i]= \\begin{cases} 0, &amp; j\\not \\in \\{i,i+1\\}\\\\ \\tfrac{m-i}{m}, &amp; j=i+1\\\\ \\tfrac{i}{m}, &amp; j=i. \\end{cases}\\] (Note: the situation would not be nearly as nice if the distribution of the choice of the next city were non-uniform. In that case, the list of the (names of the) already-visited cities would matter, and it is not clear that the described process has the Markov property (does it?). ) 2. For \\(m=1\\), \\(\\tau_m=0\\), so its distribution is deterministic and concentrated on \\(0\\). The case \\(m=2\\) is only slightly more complicated. After having visited his first city, the visitor has a probability of \\(\\tfrac{1}{2}\\) of visiting it again, on each consecutive day. After a geometrically distributed number of days, he will visit another city and \\(\\tau_2\\) will be realized. Therefore the distribution \\(\\{p_n\\}_{n\\in {\\mathbb{N}}_0}\\) of \\(\\tau_2\\) is given by \\[p_0=0, p_1=\\tfrac{1}{2}, p_2=(\\tfrac{1}{2})^2, p_3=(\\tfrac{1}{2})^3,\\dots\\] 3. For \\(m&gt;1\\), we can write \\(\\tau_m\\) as \\[\\tau_m=\\tau_1+(\\tau_2-\\tau_1)+\\dots +(\\tau_m-\\tau_{m-1}),\\] so that \\[{\\mathbb{E}}[\\tau_m]={\\mathbb{E}}[\\tau_1]+{\\mathbb{E}}[\\tau_2-\\tau_1]+\\dots+{\\mathbb{E}}[\\tau_m-\\tau_{m-1}].\\] We know that \\(\\tau_1=0\\) and for \\(k=1,2,\\dots, m-1\\), the difference \\(\\tau_{k+1}-\\tau_{k}\\) denotes the waiting time before a never-before-visited city is visited, given that the number of already-visited cities is \\(k\\). This random variable is geometric with success probability given by \\(\\tfrac{m-k}{m}\\), so its expectation is given by \\[{\\mathbb{E}}[\\tau_{k+1}-\\tau_k]= \\frac{1}{ \\tfrac{m-k}{m}}=\\frac{m}{m-k}.\\] Therefore, \\[{\\mathbb{E}}[\\tau_m]=\\sum_{k=1}^{m-1} \\frac{m}{m-k}= m (1+\\tfrac{1}{2}+\\tfrac{1}{3}+\\dots+\\tfrac{1}{m-1}).\\] By comparing it with the integral \\(\\int_1^m \\frac{1}{x}\\, dx\\), it is possible to conclude that \\(H_m=1+\\tfrac{1}{2}+\\dots+\\tfrac{1}{m-1}\\) behaves like \\(\\log m\\), i.e., that \\[\\lim_{m\\to\\infty} \\frac{H_m}{\\log m} = 1.\\] Therefore \\({\\mathbb{E}}[\\tau_m] \\sim f(m)\\), where \\(f(m) = m \\log m\\). We start with two cups, call them \\(A\\) and \\(B\\). Cup \\(A\\) contains \\(12\\) oz of milk, and cup \\(B\\) \\(12\\) oz of water. The following procedure is then performed twice: first, half of the content of the glass \\(A\\) is transferred into class \\(B\\). Then, the contents of glass \\(B\\) are thoroughly mixed, and a third of its entire content transferred back to \\(A\\). Finally, the contents of the glass \\(A\\) are thoroughly mixed. What is the final amount of milk in glass A? What does this have to do with Markov chains? If there are \\(a\\) oz of milk and \\(b\\) oz of water in the glass \\(A\\) at time \\(n\\) (with \\(a+b=12\\)), then there are \\(b\\) oz of milk and \\(a\\) oz of water in the glass \\(B\\). After half of the content of glass \\(A\\) is moved to \\(B\\), it will contain \\(b+\\tfrac{1}{2}a\\) oz of milk and \\(a+\\tfrac{1}{2}b\\) oz of water. Transferring a third of that back to \\(a\\) leaves \\(B\\) with \\((2/3 b + 1/3 a)\\) oz of milk and \\((2/3 a + 1/3 b)\\) oz of water. Equivalently, \\(A\\) contains \\((2/3 a + 1/3 b)\\) oz of milk and \\((1/3 a + 2/3 b)\\) oz of water. This corresponds to the action of a Markov chain with the transition matrix \\(P = \\begin{bmatrix} 2/3 &amp; 1/3 \\\\ 1/3 &amp; 2/3 \\end{bmatrix}\\). We get the required amounts by computing \\[\\begin{aligned} (12,0) P^2 = (12,0) \\begin{bmatrix} 5/9 &amp; 4/9 \\\\ 4/9 &amp; 5/9\\end{bmatrix} = (20/3, 16/3).\\end{aligned}\\] The state space of a Markov chain is \\(S = \\{1,2,3,4,5\\}\\), and the non-zero transition probabilities are given by \\(p_{11} = 1/2\\), \\(p_{12}=1/2\\), \\(p_{23}=p_{34}=p_{45}=p_{51}=1\\). Compute \\(p^{(6)}_{12}\\) without using software. As you can see from the transition graph below You can go from \\(1\\) to \\(2\\) in \\(6\\) steps in exactly two ways: \\[1 \\to 2 \\to 3 \\to 4 \\to 5 \\to 1 \\to 2\\] and \\[1 \\to 1 \\to 1 \\to 1 \\to 1 \\to 1 \\to 2\\] The probability of the first path is \\(2^{-2}\\) and the probability of the second path is \\(2^{-6}\\) - they add up to \\(\\tfrac{17}{64}\\). In a Gambler’s ruin problem with the state space \\(S=\\{0,1,2,3,4\\}\\) and the probability \\(p=1/3\\) of winning in a single game, compute the \\(4\\)-step transition probabilities \\[p^{(4)}_{2 2} = {\\mathbb{P}}[ X_{n+4}=2| X_n =2] \\text{ and } p^{(4)}_{2 4} = {\\mathbb{P}}[ X_{n+4}=4| X_n =2].\\] There are four \\(4\\)-step trajectories that start in \\(2\\) and end in \\(2\\), with positive probabilities (remember, once you hit \\(0\\) or \\(4\\) you get stuck there), namely \\[\\begin{aligned} &amp; 2 \\to 1 \\to 2 \\to 1 \\to 2, \\quad 2 \\to 1 \\to 2 \\to 3 \\to 2, \\quad \\\\ &amp; 2 \\to 3 \\to 2 \\to 1 \\to 2, \\quad 2 \\to 3 \\to 2 \\to 3 \\to 2.\\end{aligned}\\] Each has probability \\((1/3)\\times(2/3)\\times(1/3)\\times(2/3) = 4/81\\) so the total probability is \\(16/81\\). The (possible) trajectories that go from \\(2\\) to \\(4\\) in exactly 4 steps are \\[\\begin{aligned} 2 \\to 1 \\to 2 \\to 3 \\to 4, \\quad 2 \\to 3 \\to 2 \\to 3 \\to 4\\ \\text{ and }\\ 2 \\to 3 \\to 4 \\to 4 \\to 4.\\end{aligned}\\] The first two have the same probability, namely \\((2/3)\\times(1/3)\\times(2/3)\\times(2/3) = 8/81\\), and the third one \\((1/3)\\times(2/3)\\times(1)\\times(1) = 18/81\\) so \\(p^{(4)}_{24} = 26/81\\). A car-insurance company classifies drivers in three categories: bad, neutral and good. The reclassification is done in January of each year and the probabilities for transitions between different categories is given by \\[P= \\begin{bmatrix} 1/2 &amp; 1/2 &amp; 0 \\\\ 1/5 &amp; 2/5 &amp; 2/5 \\\\ 1/5 &amp; 1/5 &amp; 3/5\\end{bmatrix},\\] where the first row/column corresponds to the bad category, the second to neutral and the third to good. The company started in January 1990 with 1400 drivers in each category. Estimate the number of drivers in each category in 2090. Assume that the total number of drivers does not change in time and use R for your computations. Equal numbers of drivers in each category corresponds to the uniform initial distribution, \\(a^{(0)}=(1/3,1/3,1/3)\\). The distribution of drivers in 2090 is given by the distribution \\(a^{(100)}\\) of \\(X_{100}\\) which is, in turn, given by \\[a^{(100)}= a^{(0)} P^{100}.\\] Finally, we need to compute the number of drivers in each category, so we multiply the result by the total number of drivers, i.e., \\(3 \\times 1400 = 4200\\): P = matrix( c(1/2 , 1/2 , 0, 1/5 , 2/5 , 2/5 , 1/5 , 1/5 , 3/5), byrow=T, ncol=3) # a0 needs to be a row matrix a0 = matrix(c(1/3, 1/3, 1/3), nrow=1) P100 = diag(3) # the 3x3 identity matrix for (i in 1:100) P100 = P100 %*% P (a0 %*% P100) * 4200 ## [,1] [,2] [,3] ## [1,] 1200 1500 1500 Note: if you think that computing matrix powers using for loops is in poor taste, there are several R packages you can use. Have a look at this post if you are curious. A zoologist, Dr. Gurkensaft, claims to have trained Basil the Rat so that it can avoid being shocked and find food, even in highly confusing situations. Another scientist, Dr. Hasenpfeffer does not agree. She says that Basil is stupid and cannot tell the difference between food and an electrical shocker until it gets very close to either of them. The two decide to see who is right by performing the following experiment. Basil is put in the compartment \\(3\\) of a maze that looks like this: Dr. Gurkensaft’s hypothesis is that, once in a compartment with \\(k\\) exits, Basil will prefer the exits that lead him closer to the food. Dr. Hasenpfeffer’s claim is that every time there are \\(k\\) exits from a compartment, Basil chooses each one with probability \\(1/k\\). After repeating the experiment 100 times, Basil got shocked before getting to food \\(52\\) times and he reached food before being shocked \\(48\\) times. Create an Markov chain that models this situation (draw a transition graph and mark the edges with their probabilities). Use Monte Carlo to estimate the probability of being shocked before getting to food, under the assumption that Basil is stupid (all exits are equally likely). Btw, who do you think is right? Whose side is the evidence (48 vs. 52) on? If you know how to perform an appropriate statistical test here, do it. If you don’t simply state what you think. 1. Basil’s behavior can be modeled by a Markov Chain with states corresponding to compartments, and transitions to their adjacency. The graph of such a chain, on the state space \\(S=\\{1,2,3,4,5,F,S\\}\\) would look like this (with black = \\(1\\), orange = \\(1/2\\) and green=\\(1/3\\)) 2. To be able to do Monte Carlo, we need to construct its transition matrix. Since there are far fewer transitions than pairs of states, it is a good idea to start with a matrix of \\(0\\)s and then fill in the non-zero values. We also decide that \\(F\\) and \\(S\\) will be given the last two rows/columns, i.e., numbers \\(6\\) and \\(7\\): P = matrix(0,nrow =7, ncol=7 ) P[1,2] = 1/2; P[1,3] = 1/2; P[2,1] = 1/3; P[2,4] = 1/3; P[2,6] = 1/3; P[3,1] = 1/3; P[3,4] = 1/3; P[3,7] = 1/3; P[4,2] = 1/3; P[4,3] = 1/3; P[4,5] = 1/3; P[5,4] = 1/2; P[5,6] = 1/2; P[6,6] = 1 P[7,7] = 1 We continue by simulating nsim = 1000 trajectories of this chain, starting from the state \\(3\\). We compress and reuse the code from section 1.4 above: set.seed(1000) T = 100 # number of time periods nsim = 1000 # number of simulations single_trajectory = function(i) { path = numeric(T) last = i for (n in 1:T) { path[n] = sample(1:7, prob = P[last, ], size = 1) last = path[n] } return(path) } df = data.frame(X0 = 3, t(replicate(nsim, single_trajectory(3)))) (p_shocked = mean(df$X100 == 7)) ## [1] 0.581 So, the probability of being shocked first is about \\(0.58\\). To be honest, what we computed up here is not \\({\\mathbb{P}}[X_{\\tau_{S,F}} = S]\\), as the problem required, but the probability \\({\\mathbb{P}}[ X_{100} = S]\\). In general, these are not the same, but because both \\(S\\) and \\(F\\) are absorbing states, the events \\(X_{100}=S\\) and \\(X_{\\tau_{S,F}} = S\\) differ only on the event where \\(\\tau_{F,S}&gt;100\\), i.e., when Basil has not been either shocked or fed after \\(100\\) steps. To see what kind of an error we are making, we can examine the empirical distribution of \\(X_{100}\\) across our \\(1000\\) samples: table(df$X100) ## ## 6 7 ## 419 581 and conclude that, on this particular set of simulations, \\(\\tau_{S,F}\\leq 100\\), so no error has been made at all. In general, approximations like this are very useful in cases where we can expect the probability of non-absorption within a given time interval to be negligible. On the other hand, if you examine a typical trajectory of df, you will see that most of the time it takes the value \\(6\\) of \\(7\\), so a lot of the computational effort goes to waste. But don’t worry about such things in this course. So, is this enough evidence to conclude that Basil is, in fact, a smart rat? On one hand, the obtained probability \\(0.58\\) is somewhat higher than Basil’s observed shock rate of \\(52\\%\\), but it is not clear just from those numbers are not due to simple luck of the draw, and not Basil’s alleged intelligence. Without doing any further statistical analysis, my personal guess would be “probably, but who knows”. For those of you who know a bit of statistics: one can apply the binomial test (or, more precisely, its large-sample approximation) to test against the null hypothesis that Basil is stupid. Under the null, the number of times Basil will get shocked in 100 experiments is binomial, with parameters \\(n=100\\) and \\(p=0.581\\). Its normal approximation is \\(N(np, \\sqrt{np(1-p)}) = N(58.1, 4.934)\\), so the \\(z\\)-score of the observed value, i.e., \\(52\\), is \\(z = \\tfrac{ 52 - 58.1}{ 4.934} = -1.236\\). The standard normal CDF at \\(z=-1.236\\) is about \\(0.11\\), i.e., the \\(p\\)-value is about \\(0.11\\). That means that a truly stupid rat would appear at least as smart as Basil in about \\(11\\%\\) of experiments identical to the one described above by chance alone. This kind of evidence is usually not considered sufficient to make a robust conclusion about Basil’s intelligence. A math professor has \\(4\\) umbrellas. He keeps some of them at home and some in the office. Every morning, when he leaves home, he checks the weather and takes an umbrella with him if it rains. In case all the umbrellas are in the office, he gets wet. The same procedure is repeated in the afternoon when he leaves the office to go home. The professor lives in a tropical region, so the chance of rain in the afternoon is higher than in the morning; it is \\(1/5\\) in the afternoon and \\(1/20\\) in the morning. Whether it rains of not is independent of whether it rained the last time he checked. On day \\(0\\), there are \\(2\\) umbrellas at home, and \\(2\\) in the office. Construct a Markov chain that models the situation. Use Monte Carlo to give an approximate answer the following questions: What is the expected number of trips the professor will manage before he gets wet? What is the probability that the first time he gets wet it is on his way home from the office? "],
["classification-of-states.html", "Chapter 2 Classification of States 2.1 The Communication Relation 2.2 Classes 2.3 Transience and recurrence 2.4 Class properties 2.5 A few examples 2.6 Additional problems for Chapter 6", " Chapter 2 Classification of States There will be a lot of definitions and some theory before we get to examples. You might want to peek ahead as notions are being introduced; it will help your understanding. 2.1 The Communication Relation Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a Markov chain on the state space \\(S\\). For a given set \\(B\\) of states, define the (first) hitting time \\(\\tau_B\\) (or \\(\\tau(B)\\) if subscripts are impractical) of the set \\(B\\) as \\[\\begin{equation} \\tau_B=\\min \\{ n\\in{\\mathbb{N}}_0\\, : \\, X_n\\in B\\}. \\end{equation}\\] We know that \\(\\tau_B\\) is, in fact, a stopping time with respect to \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\). When \\(B\\) consists of only one element , e.g. \\(B=\\{i\\}\\), we simply write \\(\\tau_{i}\\) for \\(\\tau_{\\{i\\}}\\); \\(\\tau_{i}\\) is the first time the Markov chain \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) “hits” the state \\(i\\). As always, we allow \\(\\tau_{B}\\) to take the value \\(\\infty\\); it means that no state in \\(B\\) is ever hit. The hitting times are important both for applications, and for better understanding of the structure of Markov chains in general. For example, let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be the chain which models a game of tennis (from the previous lecture). The probability of winning for Player 1 can be phrased in terms of hitting times: \\[{\\mathbb{P}}[ \\text{Player 1 wins}]={\\mathbb{P}}[ \\tau_{i_{1}}&lt;\\tau_{i_{2}}],\\] where \\(i_{1}=\\) “Player 1 wins” and \\(i_{2}=\\)“Player 2 wins” (the two absorbing states of the chain). We will learn how to compute such probabilities in the subsequent lectures. Having introduced the hitting times \\(\\tau_B\\), let us give a few more definitions. It will be very convenient to consider the same Markov chain with different initial distributions. Most often, these distributions will correspond to starting from a fixed state (as opposed to choosing the initial state at random). We use the notation \\({\\mathbb{P}}_i[A]\\) to mean \\({\\mathbb{P}}[A|X_0=i]\\) (for any event \\(A\\)), and \\({\\mathbb{E}}_i[A]={\\mathbb{E}}[A|X_0=i]\\) (for any random variable \\(X\\)). In practice, we use \\({\\mathbb{P}}_i\\) and \\({\\mathbb{E}}_i\\) to signify that we are starting the chain from the state \\(i\\), i.e., \\({\\mathbb{P}}_i\\) corresponds to a Markov chain whose transition matrix is the same as the one of \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\), but the initial distribution is given by \\({\\mathbb{P}}_i[X_0=j]=0\\) if \\(j\\not = i\\) and \\({\\mathbb{P}}_i[X_0=i]=1\\). Note also that \\({\\mathbb{P}}_i[X_1=j] = p_{ij}\\) and that \\({\\mathbb{P}}_i[X_n=j] =p^{(n)}_{ij}\\), for any \\(n\\). A state \\(i\\in S\\) is said to communicate with the state \\(j\\in S\\), denoted by \\(i\\to j\\) if \\[{\\mathbb{P}}_i[\\tau_{j}&lt;\\infty]&gt;0.\\] Intuitively, \\(i\\) communicates with \\(j\\) if there is a non-zero chance that the Markov chain \\(X\\) will eventually visit \\(j\\) if it starts from \\(i\\). Sometimes we also say that \\(j\\) is a consequent of \\(i\\), that \\(j\\) is accessible from \\(i\\), or that \\(j\\) follows \\(i\\). In the “tennis” example of the previous chapter, every state is accessible from \\((0,0)\\) (the fact that \\(p\\in (0,1)\\) is important here), but \\((0,0)\\) is not accessible from any other state. The consequents of \\((0,0)\\) are not only \\((15,0)\\) and \\((0,15)\\), but also \\((30,15)\\) or \\((40,40)\\). In fact, all states are consequents of \\((0,0)\\). The consequents of \\((40,40)\\) are \\((40,40)\\) itself, \\((40,Adv)\\), \\((Adv, 40)\\), “P1 wins” and “P2 wins”. Explain why \\(i \\to j\\) if and only if \\(p^{(n)}_{ij}&gt;0\\) for some \\(n\\in{\\mathbb{N}}_0\\). Leaving a rigorous mathematical proof aside, we note that the statement is intuitively easy to understand. If \\(i\\to j\\) then there must exist some time \\(n\\) such that \\({\\mathbb{P}}_i[\\tau_j = n]&gt;0\\). This, in turn, implies that it is possible to go from \\(i\\) to \\(j\\) in exactly \\(n\\) steps, where “possible” means “with positive probability”. In our notation, that is exactly what \\(p^{(n)}_{ij}&gt;0\\) means. Conversely, if \\(p^{(n)}_{ij}&gt;0\\) then \\({\\mathbb{P}}_i[ \\tau_j &lt;\\infty] \\geq {\\mathbb{P}}_i[\\tau_j \\leq n] \\geq {\\mathbb{P}}_i[ X_n = j]=p^{(n)}_{ij}&gt;0.\\) Two immediate properties of the relation \\(\\to\\) are listed in the problem below: Explain why the following statements are true for all states \\(i,j,k\\) of a Markov chain. \\(i\\to i\\), \\(i\\to j, j\\to k\\) implies \\(i \\to k\\). If we start from state \\(i\\in S\\) we are already there! More rigorously, note that \\(0\\) is allowed as a value for \\(\\tau_{B}\\) in its definition above, i.e., \\(\\tau_i=0\\) when \\(X_0=i\\). Intuitively, if you can follow a path (sequence of arrows) from \\(i\\) to \\(j\\), and then another path \\(j\\) to \\(k\\), you can do the same from \\(i\\) to \\(k\\) by concatenating two paths. More rigorously, by the previous problem, it will be enough to show that \\(p^{(n)}_{ik}&gt;0\\) for some \\(n\\in{\\mathbb{N}}\\). By the same Proposition, we know that \\(p^{(n_1)}_{ij}&gt;0\\) and \\(p^{(n_2)}_{jk}&gt;0\\) for some \\(n_1,n_2\\in{\\mathbb{N}}_0\\). By the Chapman-Kolmogorov relations, with \\(n=n_1+n_2\\), we have \\[\\begin{equation} p^{(n)}_{ik} =\\sum_{l\\in S} p^{(n_1)}_{il} p^{(n_2)}_{lk}\\geq p^{(n_1)}_{ij} p^{(n_2)}_{jk}&gt;0. \\end{equation}\\] Note that the inequality \\(p^{(n)}_{ik}\\geq p^{(n_1)}_{il}p^{(n_2)}_{lk}\\) is valid for all \\(i,l,k\\in S\\), as long as \\(n_1+n_2=n\\). It will come in handy later. Remember that the greatest common divisor (gcd) of a set \\(A\\) of natural numbers if the largest number \\(d\\in{\\mathbb{N}}\\) such that \\(d\\) divides each \\(k\\in A\\), i.e., such that each \\(k\\in A\\) is of the form \\(k=l d\\) for some \\(l\\in{\\mathbb{N}}\\). A period \\(d(i)\\) of a state \\(i\\in S\\) is the greatest common divisor of the return set \\[R(i)= \\{ n\\in{\\mathbb{N}}\\, : \\, p^{(n)}_{ii}&gt;0\\}\\] of the state \\(i\\). When \\(R(i)=\\emptyset\\), we set \\(d(i)=1\\). A state \\(i\\in S\\) is called aperiodic if \\(d(i)=1\\). Consider two Markov chains with three states and the transition matrices \\[P_1=\\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix}, \\quad P_2=\\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\tfrac{1}{2} &amp; 0 &amp; \\tfrac{1}{2} \\end{bmatrix}\\] Find return sets and periods of each state \\(i\\) of each chain. For the first chain, with transition graph the return set for each state \\(i\\in\\{1,2,3\\}\\) is given by \\(R(i)= \\{3,6,9,12,\\dots\\}\\), so \\(d(i)=3\\) for all \\(i\\in\\{1,2,3\\}\\). Even though the transition graph of the second chain looks very similar to the first one the situation changes drastically: \\[\\begin{align} R(1) &amp; =\\{ 3,4,5,6, \\dots \\},\\\\ R(2) &amp; =\\{ 2,3,4,5,6, \\dots \\},\\\\ R(3) &amp; =\\{ 1,2,3,4,5,6, \\dots \\}, \\end{align}\\] so that \\(d(i)=1\\) for \\(i\\in\\{1,2,3\\}\\). 2.2 Classes We say that the states \\(i\\) and \\(j\\) in \\(S\\) intercommunicate, denoted by \\(i\\leftrightarrow j\\) if \\(i\\to j\\) and \\(j\\to i\\). A set \\(B\\subseteq S\\) of states is called irreducible if \\(i\\leftrightarrow j\\) for all \\(i,j\\in S\\). Unlike the relation of communication, the relation of intercommunication is symmetric. Moreover, we have the following immediate property: the relation \\(\\leftrightarrow\\) is an equivalence relation on \\(S\\), i.e., for all \\(i,j,k\\in S\\), we have \\(i\\leftrightarrow i\\) (reflexivity) , \\(i\\leftrightarrow j\\) implies \\(j\\leftrightarrow i\\) (symmetry), and \\(i\\leftrightarrow j, j\\leftrightarrow k\\) implies \\(i\\leftrightarrow k\\) (transitivity). The fact that \\(\\leftrightarrow\\) is an equivalence relation allows us to split the state-space \\(S\\) into equivalence classes with respect to \\(\\leftrightarrow\\). In other words, we can write \\[S=S_1\\cup S_2\\cup S_3\\cup \\dots,\\] where \\(S_1, S_2, \\dots\\) are mutually exclusive (disjoint) and all states in a particular \\(S_n\\) intercommunicate, while no two states from different equivalence classes \\(S_n\\) and \\(S_m\\) do. The sets \\(S_1, S_2, \\dots\\) are called classes of the chain \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\). Equivalently, one can say that classes are maximal irreducible sets, in the sense that they are irreducible and no class is a subset of a (strictly larger) irreducible set. A cookbook algorithm for class identification would involve the following steps: Start from an arbitrary state (call it \\(1\\)). Identify all states \\(j\\) that intercommunicate with it (\\(1\\), itself, always does). That is your first class, call it \\(C_1\\). If there are no elements left, then there is only one class \\(C_1=S\\). If there is an element in \\(S\\setminus C_1\\), repeat the procedure above starting from that element. The notion of a class is especially useful in relation to another natural concept: A set \\(B\\subseteq S\\) of states is said to be closed if \\(i \\not\\to j\\) for all \\(i\\in B\\) and all \\(j\\in S\\setminus B\\). In words, \\(B\\) is closed if it is impossible to get out of. A state \\(i\\in S\\) such that the set \\(\\{i\\}\\) is closed is called absorbing. Show that a set \\(B\\) of states is closed if and only if \\(p_{ij}=0\\) for all \\(i\\in B\\) and all \\(j\\in B^c=S\\setminus B\\). Suppose, first, that \\(B\\) is closed. Then for \\(i\\in B\\) and \\(j\\in B^c\\), we have \\(i\\not\\to j\\), i.e., \\(p^{(n)}_{ij}=0\\) for all \\(n\\in{\\mathbb{N}}\\). In particular, \\(p_{ij}=0\\). Conversely, suppose that \\(p_{ij}=0\\) for all \\(i\\in B\\), \\(j\\in B^c\\). We need to show that \\(k\\not\\to l\\) (i.e. \\(p^{(n)}_{kl}=0\\) for all \\(n\\in{\\mathbb{N}}\\)) for all \\(k\\in B\\), \\(l\\in B^c\\). Suppose, to the contrary, that there exist \\(k\\in B\\) and \\(l\\in B^c\\) such that \\(p^{(n)}_{kl}&gt;0\\) for some \\(n\\in {\\mathbb{N}}\\). That means that we can find a sequence of states \\[k=i_0, i_1, \\dots, i_n=l \\text{ such that } p_{i_{m-1} i_{m}}&gt;0 \\text{ forall }m = 1,\\dots, n.\\] The first state, \\(k=i_0\\) is in \\(B\\) and the last one, \\(l=i_n\\), is in \\(B^c\\). Therefore there must exist an index \\(m\\) such that \\(i_{m-1}\\in B\\) but \\(i_{m}\\in B^c\\). We also know that \\(p_{i_m i_{m+1}}&gt;0\\), which is in contradiction with out assumption that \\(p_{ij}=0\\) for all \\(i\\in B\\) and \\(j\\in B^c\\). Intuitively, a set of states is closed if it has the property that the chain \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) stays in it forever, once it enters it. In general, if \\(B\\) is closed, it does not have to follow that \\(S\\setminus B\\) is closed. Also, a class does not have to be closed, and a closed set does not have to be a class. Here is an example - consider the following three sets of states in the tennis chain of the previous lecture and: \\(B=\\{\\text{``P1 wins&#39;&#39;}\\}\\): closed and a class, but \\(S\\setminus B\\) is not closed \\(B=S\\setminus \\{(0,0)\\}\\): closed, but not a class, and \\(B=\\{(0,0)\\}\\): class, but not closed. Not everything is lost as the following relationship always holds: Show that every closed set \\(B\\) is a union of one or more classes. Let \\(\\hat{B}\\) be the union of all classes \\(C\\) such that \\(C\\cap B\\not=\\emptyset\\). In other words, take all the elements of \\(B\\) and throw in all the states which intercommunicate with at least one of them. I claim that \\(\\hat{B}=B\\). Clearly, \\(B\\subset \\hat{B}\\), so we need to show that \\(\\hat{B}\\subseteq B\\). Suppose, to the contrary, that there exists \\(j\\in \\hat{B}\\setminus B\\). By construction, \\(j\\) intercommunicates with some \\(i\\in B\\). In particular \\(i\\to j\\). By the closedness of \\(B\\), we must have \\(j\\in B\\). This is a contradiction with the assumptions that \\(j\\in \\hat{B}\\setminus B\\). Note that the converse is not true: just take the set \\(B=\\{ (0,0), (0,15)\\}\\) in the “tennis” example. It is a union of two classes, but it is not closed. 2.3 Transience and recurrence It is often important to know whether a Markov chain will ever return to its initial state, and if so, how often. The notions of transience and recurrence are used to address this questions. We start by introducing a cousin \\(T_j(1)\\) of the first hitting time \\(\\tau_1\\). The (first) visit time to state \\(j\\), denoted by \\(T_j(1)\\) is defined as \\[T_j(1) = \\min \\{ n\\in{\\mathbb{N}}\\, : \\, X_n=j\\}.\\] As usual \\(T_j(1)=\\infty\\) if \\(X_n\\not = j\\) for all \\(n\\in{\\mathbb{N}}\\). Similarly, second, third, etc., visit times are defined as follows: \\[\\begin{aligned} T_j(2) &amp;= \\min \\{ n&gt;T_j(1)\\, : \\, X_n=j\\}, \\\\ T_j(3) &amp;= \\min \\{ n&gt;T_j(2)\\, : \\, X_n=j\\}, \\text{ etc., }\\end{aligned}\\] with the understanding that if \\(T_j(n)=\\infty\\), then also \\(T_j(m)=\\infty\\) for all \\(m&gt;n\\). Note that the definition of the random variable \\(T_j(1)\\) differs from the definition of \\(\\tau_j\\) in that the minimum here is taken over the set \\({\\mathbb{N}}\\) of natural numbers, while the set of non-negative integers \\({\\mathbb{N}}_0\\) is used for \\(\\tau_j\\). When \\(X_0\\not = j\\), the hitting time \\(\\tau_j\\) and the first visit time \\(T_j(1)\\) coincide. The important difference occurs only when \\(X_0=j\\). In that case \\(\\tau_j=0\\) (we are already there), but it is always true that \\(T_j(1)\\geq 1\\). It can even happen that \\({\\mathbb{P}}_j[T_j(1)=\\infty]=1\\). If you want an example, take any state in the deterministically monotone chain. A state \\(i\\in S\\) is said to be recurrent if \\({\\mathbb{P}}_i[T_i(1)&lt;\\infty]=1\\), positive recurrent if \\({\\mathbb{E}}_i[T_i(1)]&lt;\\infty\\) null recurrent if it is recurrent, but not positive recurrent, transient if it is not recurrent. A state is recurrent if we are sure we will come back to it eventually (with probability 1). It is positive recurrent if it is recurrent and the time between two consecutive visits has finite expectation. Null recurrence means the we will return, but the waiting time may be very long. A state is transient if there is a positive chance (however small) that the chain will never return to it. 2.3.1 The Return Theorem The definition of recurrence from above is conceptually simple, but it gives us no clue about how to actually go about deciding whether a particular state in a specific Markov chain is recurrent. A criterion stated entirely in terms of the transition matrix \\(P\\) would be nice. Before we give it, we need to introduce some notation. and prove an important theorem. Given a state \\(i\\), let \\(f_i\\) denote the probability that the chain will visit \\(i\\) again, if it starts there, i.e., \\[f_i = {\\mathbb{P}}_i[ T_i(1) &lt; \\infty].\\] Clearly, \\(i\\) is recurrent if and only if \\(f_i=1\\). The interesting thing is that every time our chain visits the state \\(i\\), its future evolution is independent of the past (except for the name of the current state) and it behaves exactly like a new and independent chain started from \\(i\\) would. This is a special case of so-called strong Markov property which states that the (usual) Markov property also holds at stopping times (and not only fixed times \\(n\\)). We will not prove this property it these notes, but we will gladly use it to prove the following dichotomy: Theorem 2.1 (The “Return” Theorem) Let \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) be a Markov chain on a countable state space \\(S\\), with the (deterministic) initial state \\(X_0=i\\). Then exactly one of the following two statements hold with probability 1: either the chain will return to \\(i\\) infinitely many times, or the chain will return to \\(i\\) a finite number \\(N_i\\) of times, where \\(N_i\\) is geometrically distributed random variable with parameter \\(f_i\\), where \\(f_i={\\mathbb{P}}_i[T_i(1)&lt;\\infty]\\). In the first case, \\(i\\) is recurrent and, in the second, it is transient. Proof. If \\(f_i=1\\), then \\(X\\) is guaranteed to return to \\(i\\) at least once. When that happens, however, the strong Markov property “deletes” the past, and the process “renews” itself. This puts us back in the original situation where we are looking at a chain which starts at \\(i\\) and is guaranteed to return there at least once. Continuing like that, we get a whole infinite sequence of stopping times \\[T_i(1) &lt; T_i(2) &lt; \\dots\\] at which \\(X\\) finds itself at \\(i\\). If \\(f_i&lt;1\\), a similar story can be told, but with a significant difference. Every time \\(X\\) returns to \\(i\\), there is a probability \\(1-f_i\\) that it will never come back to \\(i\\), and, this is independent of the past behavior. If we think of the return to \\(i\\) as a success, the number of successes before the first failure, i.e., the number of return visits to \\(i\\), is nothing but a geometrically distributed random variable with parameter \\(f_i\\). Q.E.D. The following interesting fact follows (almost) directly from the Return Theorem: Suppose that the state space \\(S\\) is finite. Show that there exists at least one recurrent state. We argue by contradiction and assume that all the states are transient. We claim that, in that case, the total number of visits \\(N_i\\) to each state \\(i\\) is always finite, no matter what state \\(i_0\\) we start from. Indeed, if \\(i=i_0\\) that is precisely the conclusion the Return Theorem above. For a state \\(i\\ne i_0\\), the number of visits is either \\(0\\) - if we never even get to \\(i\\), or \\(1+N_{i}\\) if we do. In either case, it is a finite number (not \\(\\infty\\)). Since \\(S\\) is finite, it follows that the sum \\(\\sum_{i\\in S} N_i\\) is also finite - a contradiction with the fact that there are infinitely many time instances \\(n\\in{\\mathbb{N}}_0\\), and the fact that the chain must be in some state in each one of them. If \\(S\\) is not finite, it is not true that recurrent states must exist. Just think of the Deterministically-Monotone Chain or the random walk with \\(p\\not=\\tfrac{1}{2}\\). All states are transitive there. 2.3.2 A recurrence criterion Perhaps the most important consequence of the Return Theorem is the following criterion for recurrence of Markov chains on finite or countable state spaces: Theorem 2.2 (The Recurrence Criterion) A state \\(i\\in S\\) is recurrent if and only if \\[\\sum_{n\\in{\\mathbb{N}}} p^{(n)}_{ii}=\\infty.\\] Proof. Let \\(N_i\\) denote the total number (finite or \\(\\infty\\)) of visits to the state \\(i\\), with the initial visit at time \\(0\\) not counted. We can write \\(N_i\\) as an infinite sum as follows \\[N_i = \\sum_{n=1}^{\\infty} \\mathbf{1}_{\\{X_n = i\\}}.\\] Taking the expectation yields \\[{\\mathbb{E}}[N_i] = {\\mathbb{E}}_i[ \\sum_{n=1}^{\\infty} \\mathbf{1}_{\\{X_n=i\\}}] = \\sum_{n=1}^{\\infty} {\\mathbb{E}}_i[ \\mathbf{1}_{\\{X_n=i\\}}] = \\sum_{n=1}^{\\infty} {\\mathbb{P}}_i[ X_n=i] = \\sum_{n=1}^{\\infty} p^{(n)}_{ii},\\] where we used the intuitively acceptable (but not rigorously proven) fact that \\({\\mathbb{E}}_i\\) and an infinite sum can be switched. If \\(i\\) is transient, i.e., if \\(f_i&lt;1\\), the Return Theorem and the formula for the expected value of a geometric distribution imply that \\[{\\mathbb{E}}_i[N_i] = \\frac{f_i}{1-f_i}&lt;\\infty, \\text{ and so } \\sum_{n=1}^{\\infty} p^{(n)}_{ii} = {\\mathbb{E}}_i[N_i]&lt;\\infty.\\] On the other hand, if \\(i\\) is recurrent, the Return Theorem states that \\(N_i=\\infty\\). Hence, \\[\\sum_{n=1}^{\\infty} p^{(n)}_{ii}={\\mathbb{E}}_i[N_i]=\\infty. \\text{ Q.E.D. }\\] Remark. The central idea behind the proof of the recurrence criterion is the following: we managed tell whether or not \\(N_i = \\infty\\) by checking whether \\({\\mathbb{E}}[N_i]=\\infty\\) or not. This is, however, not something that can be done for any old random variable taking values in \\({\\mathbb{N}}_0 \\cup \\{\\infty\\}\\). If \\({\\mathbb{E}}[N]&lt;\\infty\\), then, clearly \\({\\mathbb{P}}[N=\\infty]=0\\) so that \\(N\\) only takes values in \\({\\mathbb{N}}_0\\). On the other hand, it is not true that \\({\\mathbb{P}}[N=\\infty]=0\\) implies that \\({\\mathbb{E}}[N]&lt;\\infty\\). It suffices to take a random variable with the following distribution \\[{\\mathbb{P}}[ N = n] = c/n^2 \\text{ for }n\\in{\\mathbb{N}},\\] where the constant \\(c\\) is chosen so that \\(\\sum_n c/n^2 =1\\) (in fact, we can compute that \\(c=6/\\pi^2\\) explicitly in this case). The expected value of \\(N\\) is given by \\[{\\mathbb{E}}[N] = \\sum_{n=1}^{\\infty} n {\\mathbb{P}}[N=n] = c \\sum_{n=1}^{\\infty} \\frac{1}{n} = \\infty.\\] The message is that, in general, you cannot detect whether something happened infinitely many times or not based only on its expectation. Such a detection, however, becomes possible in the special case when \\(N=N_i\\) denotes the total number of returns to the state \\(i\\) of a Markov chain. This is exactly the content of proof of the Return Theorem above: each time the chain leaves \\(i\\), it comes back to it (or does not) with the same probability, independently of the past. This gives us extra information about the random variable \\(N\\) (namely that it is either infinite with probability \\(1\\) or geometrically distributed) and allows us to test its finiteness by using the expected value only. 2.3.3 Polya’s theorem Here is an application of our recurrence criterion - a beautiful and unexpected result of George Pólya from 1921. In addition to the simple symmetric random walk on the line (\\(d=1\\)) we studied before, one can consider random walks whose values are in the plane (\\(d=2\\)), the space (\\(d=3\\)), etc. These are usually defined as follows: the random walk in \\(d\\) dimensions is the Markov chain with the state space \\(S={\\mathbb{Z}}^d\\) and the following transitions: starting from the state \\((x_1,\\dots, x_d)\\), it picks one of its \\(2d\\) neighbors \\((x_1+1,\\dots, x_d)\\), \\((x_1-1,\\dots, x_d)\\), \\((x_1, x_2+1,\\dots, x_d)\\), \\((x_1, x_2-1,\\dots, x_d)\\), …, \\((x_1,\\dots, x_d+1)\\), \\((x_1,\\dots, x_d-1)\\) randomly and uniformly and moves there. For illustration, here is a picture of a path of a two-dimensional random walk; as time progresses, the color of the edges goes from black to orange, edges traversed multiple times are darker, dots mark the position of the walk at time \\(n=0\\) (the black round dot) and at time \\(n=1000\\) (orange square dot): Polya’s (and our) goal was to study the recurrence properties of the \\(d\\)-dimensional random walk. We already know that the simple symmetric random walk on \\({\\mathbb{Z}}\\) is recurrent (i.e., every \\(i\\in {\\mathbb{Z}}\\) is a recurrent state). The easiest way to proceed when \\(d\\geq 2\\) is to use the recurrence criterion we proved above. We start by estimating the values \\(p^{(n)}_{ii}\\), for \\(n\\in{\\mathbb{N}}\\). By symmetry, we can focus on the origin, i.e., it is enough to estimate, for each \\(n\\in{\\mathbb{N}}\\), the magnitude of \\[p^{(n)}= p^{(n)}_{00}= {\\mathbb{P}}_{0}[ X_n=(0,0,\\dots, 0)].\\] As we learned some time ago, this probability can be computed by counting all “trajectories” from \\((0,\\dots, 0)\\) that return to \\((0,\\dots, 0)\\) in \\(n\\) steps. First of all, it is clear that \\(n\\) needs to be even, i.e., \\(n=2m\\), for some \\(m\\in{\\mathbb{N}}\\). It helps if we think of any trajectory as a sequence of “increments” \\(\\xi_1,\\dots, \\xi_n\\), where each \\(\\xi_i\\) takes its value in the set \\(\\{1,-1,2,-2,\\dots, d, -d\\}\\). In words, \\(\\xi_i= +k\\) if the \\(k\\)-th coordinate increases by \\(1\\) on the \\(i\\)-th step, and \\(\\xi_i=-k\\), if the \\(k\\)-th coordinate decreases1 This way, the problem becomes combinatorial: In how many ways can we put one element of the set \\(\\{1,-1,2,-2, \\dots, d,-d\\}\\) into each of \\(n=2m\\) boxes so that the number of boxes with \\(k\\) in them equals to the number of boxes with \\(-k\\) in them? To get the answer, we start by fixing a possible “count” \\((i_1,\\dots, i_d)\\), satisfying \\(i_1+\\dots+i_d=m\\) of the number of times each of the values in \\(\\{1,2,\\dots, d\\}\\) occurs. These values have to be placed in \\(m\\) of the \\(2m\\) slots and their negatives (possibly in a different order) in the remaining \\(m\\) slots. So, first, we choose the “positive” slots (in \\(\\binom{2m}{m}\\) ways), and then distribute \\(i_1\\) “ones”, \\(i_2\\) “twos”, etc., in those slots; this can be done in2 \\[\\binom{ m }{ i_1 i_2 \\dots i_d}\\] ways. This is also the number of ways we can distribute the negative “ones”, “twos”, etc., in the remaining slots. All in all, for fixed \\(i_1,i_2,\\dots, i_d\\), all of this can be done in \\[\\binom{2m}{m} \\binom{ m }{ i_1 i_2 \\dots i_d}^2\\] ways. Remembering that each path has the probability \\((2d)^{-2m}\\), and summing over all \\(i_1,\\dots, i_d\\) with \\(i_1+\\dots+i_d=m\\), we get \\[\\begin{equation} p^{(2m)} = \\frac{1}{(2d)^{2m}} \\binom{2m}{m} \\sum_{i_1+\\dots+i_d=m} \\binom{ m }{ i_1 i_2 \\dots i_d}^2. \\tag{2.1} \\end{equation}\\] This expression looks so complicated that we better start examining is for particular values of \\(d\\): For \\(d=1\\), the expression above simplifies to \\(p^{(2m)} = \\frac{1}{4^{m}} \\binom{2m}{m}\\). It is still too complicated sum over all \\(m\\in{\\mathbb{N}}\\), but we can simplify it further by using Stirling’s formula \\[n! \\sim \\sqrt{2\\pi n} \\big(\\tfrac{n}{e}\\big)^n,\\] where \\(a_n \\sim b_n\\) means \\(\\lim_{n\\to\\infty} a_n/b_n=1\\). Indeed, from there, \\[\\label{equ:binom} \\begin{split} \\binom{2m}{m} \\sim \\frac{4^m}{ \\sqrt{\\pi m}}, \\end{split} \\text{ and so } p^{(2m)} \\sim \\frac{1}{\\sqrt{m\\pi}}.\\] That means that \\(p^{(m)}\\) behaves li a \\(p\\)-series with \\(p=1/2\\) which we know is divergent. Therefore, \\[\\sum_{m=1}^{\\infty} p^{(2m)} = \\infty,\\] and we recover our previous conclusion that the simple symmetric random walk is, indeed, recurrent. Moving on to the case \\(d= 2\\), we notice that the sum of the multinomial coefficients in (2.1) no longer equals \\(1\\); in fact it is given by3 \\[\\label{equ:Van} \\begin{split} \\sum_{i=0}^{m} \\binom{m}{i}^2 = \\binom{2m}{m}, \\end{split}\\] and, so, \\[p^{(2m)} = \\frac{1}{16^m} \\Big( \\frac{4^m}{\\sqrt{\\pi m}} \\Big)^2 \\sim \\frac{1}{\\pi m} \\text{ implying that } \\sum_{m=1}^{\\infty} p^{(2m)}=\\infty,\\] which which, in turn, implies that the two-dimensional random walk is also recurrent. How about \\(d\\geq 3\\)? Things are even more complicated now. The multinomial sum in (2.1) above does not admit a nice closed-form expression as in the case \\(d=2\\), so we need to do some estimates; these are a bit tedious so we skip them, but report the punchline, which is that \\[p^{(2m)} \\sim C \\Big( \\tfrac{3}{m} \\Big)^{3/2},\\] for some constant \\(C\\). This is where it gets interesting: this is a \\(p\\)-series which converges: \\[\\sum_{m=1}^{\\infty} p^{(2m)}&lt;\\infty,\\] and, so, the random walk is transient for \\(d=3\\). This is enough to conclude that the random walk is transient for all \\(d\\geq 3\\), too (why?). To summarize Theorem 2.3 (Polya) The simple symmetric random walk is recurrent for \\(d=1,2\\), but transient for \\(d\\geq 3\\). In the words of Shizuo Kakutani A drunk man will find his way home, but a drunk bird may get lost forever. 2.4 Class properties Certain properties of states are shared between all elements in a class. Knowing which properties have this feature is useful for a simple reason - if you can check them for a single class member, you know automatically that all the other elements of the class share it. A property is called a class property it holds for all states in its class, whenever it holds for any one particular state in the that class. Put differently, a property is a class property if and only if either all states in a class have it or none does. Show that transience and recurrence are class properties. We use the recurrence criterion proved above. Suppose that the state \\(i\\) is recurrent, and that \\(j\\) is in its class, i.e., that \\(i\\leftrightarrow j\\). Then, there exist natural numbers \\(m\\) and \\(k\\) such that \\(p^{(m)}_{ij}&gt;0\\) and \\(p^{(k)}_{ji}&gt;0\\). By the Chapman-Kolmogorov relations, for each \\(n\\in{\\mathbb{N}}\\), we have \\[p^{(n+m+k)}_{jj} =\\sum_{l_1\\in S} \\sum_{l_2\\in S} p^{(k)}_{j l_1} p^{(n)}_{l_1 l_2} p^{(m)}_{l_2 m}\\geq p^{(k)}_{ji} p^{(n)}_{ii} p^{(m)}_{ij}.\\] In other words, there exists a positive constant \\(c\\) (take \\(c=p^{(k)}_{ji}p^{(m)}_{ij}\\)), independent of \\(n\\), such that \\[p^{(n+m+k)}_{jj}\\geq c p^{(n)}_{ii}.\\] The recurrence of \\(i\\) implies that \\(\\sum_{n=1}^{\\infty}p^{(n)}_{ii}=\\infty\\), and so \\[\\sum_{n=1}^{\\infty} p^{(n)}_{jj}\\geq \\sum_{n=m+k+1}^{\\infty} p^{(n)}_{jj}= \\sum_{n=1}^{\\infty} p^{(n+m+k)}_{jj}\\geq c \\sum_{n=1}^{\\infty} p^{(n)}_{ii}=\\infty,\\] which implies that \\(j\\) is recurrent. Thus, recurrence is a class property, and since transience is just the opposite of recurrence, it is clear that transience is also a class property, too. Show that period is a class property, i.e., all elements of a class have the same period. Let \\(d=d(i)\\) be the period of the state \\(i\\), and let \\(j\\leftrightarrow i\\). Then, there exist natural numbers \\(m\\) and \\(k\\) such that \\(p^{(m)}_{ij}&gt;0\\) and \\(p^{(k)}_{ji}&gt;0\\). By Chapman-Kolmogorov, \\[p^{(m+k)}_{ii}\\geq p^{(m)}_{ij}p^{(k)}_{ji}&gt;0,\\] and so \\(m+k\\in R(i)\\). Similarly, for any \\(n\\in R(j)\\), \\[p^{(m+k+n)}_{ii}\\geq p^{(m)}_{ij} p^{(n)}_{jj} p^{(k)}_{ji}&gt;0,\\] so \\(m+k+n\\in R(i)\\). By the definition of the period, we see now that \\(d(i)\\) divides both \\(m+k\\) and \\(m+k+n\\), and, so, it divides \\(n\\). This works for each \\(n\\in R(j)\\), so \\(d(i)\\) is a common divisor of all elements of \\(R(j)\\); this, in turn, implies that \\(d(i)\\leq d(j)\\). The same argument with roles of \\(i\\) and \\(j\\) switched shows that \\(d(j)\\leq d(i)\\). Therefore, \\(d(i)=d(j)\\). 2.4.1 The Canonical Decomposition Now that we know that transience and recurrence are class properties, we can introduce the notion of the of a Markov chain. Let \\(S_1,S_2,\\dots\\) be the collection of all classes; some of them contain recurrent states and some transient ones. We learned in the previous section that if there is one recurrent state in a class, than all states in the class must be recurrent. Thus, it makes sense to call the whole class recurrent. Similarly, the classes which are not recurrent consist entirely of transient states, so we call them transient. There are at most countably many states, so the number of all classes is also at most countable. In particular, there are only countably (or finitely) many recurrent classes, and we usually denote them by \\(C_1, C_2, \\dots\\). Transient classes are denoted by \\(T_1,T_2, \\dots\\). There is no special rule for the choice of indices \\(1,2,3,\\dots\\) for particular classes. The only point is that they can be enumerated because there are at most countably many of them. The distinction between different transient classes is usually not very important, so we pack all transient states together in a set \\(T=T_1\\cup T_2\\cup \\dots\\). Definition. Let \\(S\\) be the state space of a Markov chain \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\). Let \\(C_1,C_2, \\dots\\) be its recurrent classes, \\(T_1,T_2,\\dots\\) the transient classes, and let \\(T=T_1\\cup T_2\\cup \\dots\\) be their union. The decomposition \\[S= T \\cup C_1 \\cup C_2 \\cup C_3 \\cup \\dots,\\] is called the canonical decomposition of the (state space of the) Markov chain \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\). The reason that recurrent classes are important is simple - they can be interpreted as Markov chains themselves. To see why, we start with the following problem: Show that recurrent classes are necessarily closed. We argue by contradiction and assume that that \\(C\\) is a recurrent class which is not closed. Then, there exist states \\(i\\in C\\) and \\(j\\in C^c\\) such that \\(i\\to j\\). On the other hand, since \\(j\\not\\in C\\) and \\(C\\) is a class, we cannot have \\(j\\to i\\). Started at \\(i\\), the chain will reach \\(j\\) with positive probability, and, since \\(j\\not\\to i\\), never return. That implies that the number of visits to \\(i\\) will be finite, with positive probability. That is in contradiction with the fact that \\(i\\) is recurrent and the statement of the Return Theorem above. The fact we just proved implies the following nice dichotomy, valid for every finite-state-space chain: A class of a Markov chain on a finite state space is recurrent if and only if it is closed. We know that recurrent classes are closed. In order to show the converse, we need to prove that transient classes are not closed. Suppose, to the contrary, the there exists a finite state-space Markov chain with a closed transient class \\(T\\). Since \\(T\\) is closed, we can see it as a state space of the restricted Markov chain. This, new, Markov chain has a finite number of states so there exists a recurrent state. This is a contradiction with the assumption that \\(T\\) consists only of transient states. The condition of finiteness is necessary for the above equivalent to hold. For a random walk on \\(\\mathbb Z\\), all states intercommunicate. In particular, there is only one class - \\(\\mathbb Z\\) itself - and it it trivially closed. If \\(p\\not=\\tfrac{1}{2}\\), however, all states are transient, and, so, \\(\\mathbb Z\\) is a closed and transient class. Together with the canonical decomposition, we introduce the of the transition matrix \\(P\\). The idea is to order the states in \\(S\\) with the canonical decomposition in mind. We start from all the states in \\(C_1\\), followed by all the states in \\(C_2\\), etc. Finally, we include all the states in \\(T\\). The resulting matrix looks like this \\[P= \\begin{bmatrix} P_1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; P_2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; P_3 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ Q_1 &amp; Q_2 &amp; Q_3 &amp; \\dots &amp; \\dots \\end{bmatrix},\\] where the entries should be interpreted as matrices: \\(P_1\\) is the transition matrix within the first class, i.e., \\(P_1=(p_{ij},i\\in C_1, j\\in C_1)\\), etc. \\(Q_k\\) contains the transition probabilities from the transient states to the states in the (recurrent) class \\(C_k\\). We learned, above, that recurrent classes are closed, which implies implies that each \\(P_k\\) is a stochastic matrix, or, equivalently, that all the entries in the row of \\(P_k\\) outside of \\(P_k\\) are zeros. 2.5 A few examples To help you internalize the notions introduced in this chapter, we classify the states, identify closed sets and discuss periodicity, transience and recurrence in some of the standard examples. In all examples below we assume that \\(0 &lt; p &lt; 1\\). 2.5.1 Random walks Communication and classes. Clearly, it is possible to go from any state \\(i\\) to either \\(i+1\\) or \\(i-1\\) in one step, so \\(i\\to i+1\\) and \\(i\\to i-1\\) for all \\(i\\in S\\). By transitivity of communication, we have \\(i\\to i+1\\to i+2\\to \\dots\\to i+k\\). Similarly, \\(i\\to i-k\\) for any \\(k\\in{\\mathbb{N}}\\). Therefore, \\(i\\to j\\) for all \\(i,j\\in S\\), and so, \\(i\\leftrightarrow j\\) for all \\(i,j\\in S\\), and the whole \\(S\\) is one big class. Closed sets. The only closed set is \\(S\\) itself. Transience and recurrence We studied transience and recurrence in the lectures about random walks (we just did not call them that). The situation highly depends on the probability \\(p\\) of making an up-step. If \\(p&gt;\\tfrac{1}{2}\\), there is a positive probability that the first step will be “up”, so that \\(X_1=1\\). Then, we know that there is a positive probability that the walk will never hit \\(0\\) again. Therefore, there is a positive probability of never returning to \\(0\\), which means that the state \\(0\\) is transient. A similar argument can be made for any state \\(i\\) and any probability \\(p\\not=\\tfrac{1}{2}\\). What happens when \\(p=\\tfrac{1}{2}\\)? In order to come back to \\(0\\), the walk needs to return there from its position at time \\(n=1\\). If it went up, the we have to wait for the walk to hit \\(0\\) starting from \\(1\\). We have shown that this will happen sooner or later, but that the expected time it takes is infinite. The same argument works if \\(X_1=-1\\). All in all, \\(0\\) (and all other states) are null-recurrent (recurrent, but not positive recurrent). Periodicity. Starting from any state \\(i\\in S\\), we can return to it after \\(2,4,6,\\dots\\) steps. Therefore, the return set \\(R(i)\\) is always given by \\(R(i)=\\{2,4,6,\\dots\\}\\) and so \\(d(i)=2\\) for all \\(i\\in S\\). 2.5.2 Gambler’s ruin Communication and classes. The winning state \\(a\\) and the losing state \\(0\\) are clearly absorbing, and form one-element classes. The other \\(a-1\\) states intercommunicate among each other, so they form a class of their own. This class is not closed (you can - and will - exit it and get absorbed sooner or later). Transience and recurrence. The absorbing states \\(0\\) and \\(a\\) are (trivially) positive recurrent. All the other states are transient: starting from any state \\(i\\in\\{1,2,\\dots, a-1\\}\\), there is a positive probability (equal to \\(p^{a-i}\\)) of winning every one of the next \\(a-i\\) games and, thus, getting absorbed in \\(a\\) before returning to \\(i\\). Periodicity. The absorbing states have period \\(1\\) since \\(R(0)=R(a)={\\mathbb{N}}\\). The other states have period \\(2\\) (just like in the case of a random walk). 2.5.3 Deterministically monotone Markov chain Communication and classes. A state \\(i\\) communicates with the state \\(j\\) if and only if \\(j\\geq i\\). Therefore \\(i\\leftrightarrow j\\) if and only if \\(i=j\\), and so, each \\(i\\in S\\) is in a class by itself. Closed sets. The closed sets are precisely the sets of the form \\(B={i,i+1,i+2,\\dots}\\), for \\(i\\in{\\mathbb{N}}\\). Transience and recurrence All states are transient. Periodicity. The return set \\(R(i)\\) is empty for each \\(i\\in S\\), so \\(d(i)=1\\), for all \\(i\\in S\\). 2.5.4 The game of tennis Communication and classes. All the states except for those in \\(E=\\{ (40,Adv), (40,40), (Adv,40),\\) \\(\\text{P1 wins}, \\,\\text{P2 wins}\\}\\) intercommunicate only with themselves, so each \\(i\\in S\\setminus E\\) is in a class by itself. The winning states P1 wins and P2 wins are absorbing, and, so, also form classes with one element. Finally, the three states in \\(\\{(40,Adv),(40,40),(Adv,40)\\}\\) intercommunicate with each other, so they form the last class. Periodicity. The states \\(i\\) in \\(S\\setminus E\\) have the property that \\(p^{(n)}_{ii}=0\\) for all \\(n\\in{\\mathbb{N}}\\), so \\(d(i)=1\\). The winning states are absorbing so \\(d(i)=1\\) for \\(i\\in \\{\\text{P1 wins, P2 wins}\\}\\). Finally, the return set for the remaining three states is \\(\\{2,4,6,\\dots\\}\\) so their period is \\(2\\). 2.6 Additional problems for Chapter 6 Let \\(C_1\\) and \\(C_2\\) be two (different) classes. For each of the following statements either explain why it is true, or give an example showing that it is false. \\(i\\to j\\) or \\(j\\to i\\), for all \\(i\\in C_1\\), and \\(j\\in C_2\\), \\(C_1\\cup C_2\\) is not a class, If \\(i\\to j\\) for some \\(i\\in C_1\\) and \\(j\\in C_2\\), then \\(k\\not\\to l\\) for all \\(k\\in C_2\\) and \\(l\\in C_1\\), If \\(i\\to j\\) for some \\(i\\in C_1\\) and \\(j\\in C_2\\), then \\(k\\to l\\) for some \\(k\\in C_2\\) and \\(l\\in C_1\\), Consider a Markov Chain whose transition graph is given below (with orange edges having probability \\(1/2\\), black \\(1\\), blue \\(3/4\\) and green \\(1/4\\)) Identify the classes. Find transient and recurrent states. Find periods of all states. Compute \\(f^{(n)}_{13}\\), for all \\(n\\in{\\mathbb{N}}\\), where \\(f^{(n)}_{ij} ={\\mathbb{P}}_i[T_j(1) = n]\\). Using software, we can get that, approximately, \\[ P^{20}=\\left[ \\begin{array}{llllllll} 0 &amp; 0 &amp; 0.15 &amp; 0.14 &amp; 0.07 &amp; 0.14 &amp; 0.21 &amp; 0.29 \\\\ 0 &amp; 0 &amp; 0.13 &amp; 0.15 &amp; 0.07 &amp; 0.15 &amp; 0.21 &amp; 0.29 \\\\ 0 &amp; 0 &amp; 0.3 &amp; 0.27 &amp; 0.15 &amp; 0.28 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.27 &amp; 0.3 &amp; 0.13 &amp; 0.29 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.29 &amp; 0.28 &amp; 0.15 &amp; 0.28 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0.28 &amp; 0.29 &amp; 0.14 &amp; 0.29 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.43 &amp; 0.57 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.43 &amp; 0.57 \\end{array} \\right],\\] where \\(P\\) is the transition matrix of the chain. Compute the probability \\({\\mathbb{P}}[X_{20}=3]\\), if the initial distribution (the distribution of \\(X_0\\)) is given by \\({\\mathbb{P}}[X_0=1]=1/2\\) and \\({\\mathbb{P}}[X_0=3]=1/2\\). A fair 6-sided die is rolled repeatedly, and for \\(n\\in{\\mathbb{N}}\\), the outcome of the \\(n\\)-th roll is denoted by \\(Y_n\\) (it is assumed that \\(\\{Y_n\\}_{n\\in{\\mathbb{N}}}\\) are independent of each other). For \\(n\\in{\\mathbb{N}}_0\\), let \\(X_n\\) be the remainder (taken in the set \\(\\{0,1,2,3,4\\}\\)) left after the sum \\(\\sum_{k=1}^n Y_k\\) is divided by \\(5\\), i.e. \\(X_0=0\\), and \\[%\\label{} \\nonumber \\begin{split} X_n= \\sum_{k=1}^n Y_k \\ (\\,\\mathrm{mod}\\, 5\\,),\\text{ for } n\\in{\\mathbb{N}}, \\end{split}\\] making \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) a Markov chain on the state space \\(\\{0,1,2,3,4\\}\\) (no need to prove this fact). Write down the transition matrix of the chain, classify the states, separate recurrent from transient ones, and compute the period of each state. The outcomes \\(1,2,3,4,5,6\\) leave remainders \\(1,2,3,4,0,1\\), when divided by \\(5\\), so the transition matrix \\(P\\) of the chain is given by \\[P=\\begin{bmatrix} \\tfrac{1}{6} &amp; \\tfrac{1}{3} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} \\\\ \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{3} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} \\\\ \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{3} &amp; \\tfrac{1}{6} \\\\ \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{3} \\\\ \\tfrac{1}{3} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} &amp; \\tfrac{1}{6} \\\\ \\end{bmatrix}\\] Since \\(p_{ij}&gt;0\\) for all \\(i,j\\in S\\), all the states belong to the same class, and, because there is at least one recurrent state in a finite-state-space Markov chain and because recurrence is a class property, all states are recurrent. Finally, \\(1\\) is in the return set of every state, so the period of each state is \\(1\\). Which of the following statements is true? Give a short explanation (or a counterexample where appropriate) for your choice. \\(\\{X_n\\}_{n\\in {\\mathbb{N}}_0}\\) is a Markov chain with state space \\(S\\). If states \\(i\\) and \\(j\\) intercommunicate, then there exists \\(n\\in{\\mathbb{N}}\\) such that \\(p^{(n)}_{ij}&gt;0\\) and \\(p^{(n)}_{ji}&gt;0\\). If all rows of the transition matrix are equal, then all states belong to the same class. If \\(P^n\\to I\\), then all states are recurrent. (Note: We say that a sequence \\(\\{A_n\\}_{n\\in{\\mathbb{N}}}\\) of matrices converges to the matrix \\(A\\), and we denote it by \\(A_n\\to A\\), if \\((A_n)_{ij}\\to A_{ij}\\), as \\(n\\to\\infty\\), for all \\(i,j\\).) FALSE. Consider a Markov chain with the transition matrix \\[\\begin{equation} P = \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix} \\end{equation}\\] All states intercommunicate, but \\(p^{(n)}_{12}&gt;0\\) if and only if \\(n\\) is of the form \\(n=3k+1\\), for \\(k\\in{\\mathbb{N}}_0\\). On the other hand \\(p^{(n)}_{21}&gt;0\\) if and only if \\(n=3k+2\\) , for some \\(k\\in{\\mathbb{N}}_0\\). Thus, \\(p^{(n)}_{12}\\) and \\(p^{(n)}_{21}\\) are never simultaneously positive. FALSE. Consider a Markov chain with the following transition matrix: \\[P= \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 0 \\end{bmatrix}.\\] Then \\(1\\) is an absorbing state and it is in a class of its own, so it is not true that all states belong to the same class. TRUE. Suppose that there exists a transient state \\(i\\in S\\). Then \\(\\sum_{n} p^{(n)}_{ii}&lt;\\infty\\), and, in particular, \\(p^{(n)}_{ii}\\to 0\\), as \\(n\\to\\infty\\). This is a contradiction with the assumption that \\(p^{(n)}_{ii}\\to 1\\), for all \\(i\\in S\\). Let \\(C\\) be a class in a Markov chain. For each of the following statements either explain why it is true, or give an example showing that it is false. \\(C\\) is closed, \\(C^c\\) is closed, At least one state in \\(C\\) is recurrent, For all states \\(i,j\\in C\\), \\(p_{ij}&gt;0\\), False. Take \\(C=\\{(0,0)\\}\\) in the “Tennis example”. False. Take \\(C=\\{\\text{Player 1 wins}\\}\\) in the “Tennis example”. False. It is enough to take any transient class in a finite-state Markov chain as a counterexample. For instance, the class \\(\\{ (0,0) \\}\\) consisting of a single element \\((0,0)\\) in the Tennis chain. False. This would be true if it read “for each pair of states \\(i,j\\in C\\), there exists \\(n\\in{\\mathbb{N}}\\) such that \\(p^{(n)}_{ij}&gt;0\\)”. Otherwise, we can use the “Tennis chain” and the states \\(i=(40,Adv)\\) and \\(j=(Adv,40)\\). They belong to the same class, but \\(p_{ij}=0\\) (you need to pass through \\((40,40)\\) to go from one to another). Consider a Markov chain whose state space has \\(n\\) elements (\\(n\\in{\\mathbb{N}}\\)). For each of the following statements either explain why it is true, or give an example showing that it is false. all classes are closed at least one state is transient, not more than half of all states are transient, there are at most \\(n\\) classes, False. In the “Tennis” example, there are classes that are not closed. False. Just take the Regime Switching with \\(0&lt;p_{01}, p_{10} &lt;1\\). Both of the states are recurrent there. Or, simply take a Markov chain with only one state (\\(n=1\\)). False. In the “Tennis” example, 18 states are transient, but \\(n=20\\). True. Classes form a partition of the state space, and each class has at least one element. Therefore, there are at most \\(n\\) classes. Let \\(i\\) be a recurrent state with period 5, and let \\(j\\) be another state. For each of the following statements either explain why it is true, or give an example showing that it is false. if \\(j\\to i\\), then \\(j\\) is recurrent, if \\(j\\to i\\), then \\(j\\) has period \\(5\\), if \\(i\\to j\\), then \\(j\\) has period \\(5\\), if \\(j\\not\\to i\\) then \\(j\\) is transient, We will use the following chain for all the counterexamples (green edges have probability \\(1/2\\) and black edges \\(1\\)) False. Take \\(j=0\\) and \\(i=1\\) in the chain in the picture. False. Take the same counterexample as above. True. We know that \\(i\\) is recurrent, and since all recurrent classes are closed, and \\(i\\to j\\), \\(h\\) must belong to the same class as \\(i\\). Period is a class property, so the period of \\(j\\) is also \\(5\\). False. Take \\(j=6\\), \\(i=0\\) in the chain in the picture. Let \\(i\\) and \\(j\\) be two states such that \\(i\\) is transient and \\(i\\leftrightarrow j\\). For each of the following statements either explain why it is true, or give an example showing that it is false. if \\(i\\to k\\), then \\(k\\) is transient, if \\(k\\to i\\), then \\(k\\) is transient, period of \\(i\\) must be \\(1\\), (extra credit) \\(\\sum_{n=1}^{\\infty} p^{(n)}_{jj} = \\sum_{n=1}^{\\infty} p^{(n)}_{ii}\\), Suppose there exists \\(n\\in{\\mathbb{N}}\\) such that \\(P^n=I\\), where \\(I\\) is the identity matrix and \\(P\\) is the transition matrix of a finite-state-space Markov chain. For each of the following statements either explain why it is true, or give an example showing that it is false. \\(P=I\\). All states belong to the same class. All states are recurrent. The period of each state is \\(n\\). False. Take the Regime-switching chain with \\[P= \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix}\\] Then \\(P^2=I\\), but \\(P\\not= I\\). False. If \\(P=I\\), all states are absorbing, and, therefore, each is in a class of its own. True. By the assumption \\(P^{kn}=(P^n)^k=I^k=I\\), for all \\(k\\in{\\mathbb{N}}\\). Therefore, \\(p^{(kn)}_{ii}=1\\) for all \\(k\\in{\\mathbb{N}}\\), and so \\(\\lim_{m\\to\\infty} p^{(m)}_{ii}\\not= 0\\) (maybe it doesn’t even exist). In any case, the series \\(\\sum_{m=1}^{\\infty} p^{(m)}_{ii}\\) cannot be convergent, and so, \\(i\\) is recurrent, for all \\(i\\in S\\). Alternatively, the condition \\(P^n=I\\) means that the chain will be coming back to where it started - with certainty - every \\(n\\) steps, and so, all states must be recurrent. False. Any chain satisfying \\(P^n=I\\), but with the property that the \\(n\\) above is not unique is a counterexample. For example, if \\(P=I\\), then \\(P^n=I\\) for any \\(n\\in{\\mathbb{N}}\\). Let \\(i\\) be a recurrent state with period 3, and let \\(j\\) be another state. For each of the following statements either explain why it is true, or give an example showing that it is false. If \\(j\\to i\\), then \\(j\\) is recurrent. If \\(j\\to i\\), then \\(j\\) has period \\(3\\). If \\(i\\to j\\), then \\(j\\) has period \\(3\\). If \\(j\\not\\to i\\) then \\(j\\) is transient. The only true statement is (c). Since \\(i\\) is recurrent and \\(i\\to j\\), we have \\(j\\to i\\), and so, \\(i\\) and \\(j\\) belong to the same class. Periodicity is a class property so the period of \\(i\\) and \\(j\\) are the same, namely \\(3\\). Let \\(C_1\\) and \\(C_2\\) be two (different) communication classes of a Markov chain. Exactly one of the statements below is necessarily true. Which one? \\(i\\to j\\) or \\(j\\to i\\), for all \\(i\\in C_1\\), and \\(j\\in C_2\\) \\(C_1\\cup C_2\\) is a class if \\(i\\to j\\) for some \\(i\\in C_1\\) and \\(j\\in C_2\\), then \\(k\\not\\to l\\) for all \\(k\\in C_2\\) and \\(l\\in C_1\\) if \\(i\\to j\\) for some \\(i\\in C_1\\) and \\(j\\in C_2\\), then \\(k\\to l\\) for some \\(k\\in C_2\\) and \\(l\\in C_1\\), Statements 1. - 4. are all false. The correct statement is 3. Suppose that \\(i\\to j\\) and \\(k\\to l\\) for some \\(k\\in C_2\\) and some \\(l\\in C_1\\). Since both \\(j\\) and \\(k\\) are in the same class (and so are \\(i\\) and \\(l\\)), we have \\(j\\to k\\) (and \\(l\\to i\\)), and then, by transitivity \\[i \\to j \\to k \\to l \\to i\\] which implies that \\(j\\leftrightarrow k\\). Therefore, \\(j\\) and \\(k\\) are in the same class, which is in contradiction with the assumption that \\(C_1\\) and \\(C_2\\) are different classes. Suppose that all classes of a Markov chain are recurrent, and let \\(i,j\\) be two states such that \\(i\\to j\\). For each of the 4 statements before, either explain why it is true, or give an example of a Markov chain in which it fails. for each state \\(k\\), either \\(i\\to k\\) or \\(j\\to k\\) \\(j\\to i\\) \\(p_{ji}&gt;0\\) or \\(p_{ij}&gt;0\\) \\(\\sum_{n=1}^{\\infty} p^{(n)}_{jj}&lt;\\infty\\) False. Take a chain with two states \\(1,2\\) where \\(p_{11}=p_{22}=1\\), and set \\(i=j=1\\), \\(k=2\\). True. Recurrent classes are closed, so \\(i\\) and \\(j\\) belong to the same class. Therefore \\(j\\to i\\). False. Take a chain with \\(4\\) states \\(1,2,3,4\\) where \\(p_{12}=p_{23}=p_{34}=p_{41}=1\\), and set \\(i=1\\), \\(j=3\\). False. That would mean that \\(j\\) is transient. ⬇︎ In case you were wondering, the text below belongs to footnotes from somewhere high above.⬇︎ For \\(d=2\\) we could have used the values “up”, “down”, “left” and\n“’right”, for \\(1,-1,2\\) or \\(-2\\), respectively. In dimension \\(3\\), we\ncould have added “forward” and “backward”, but we run out of words\nfor directions for larger \\(d\\).↩︎ \\(\\binom{m}{i_1 \\dots i_d}\\) is called the multinomial coefficient. It\ncounts the number of ways we can color \\(m\\) objects into one of \\(d\\)\ncolors such that there are \\(i_1\\) objects of color \\(1\\), \\(i_2\\) of\ncolor \\(2\\), etc. It is a generalization of the binomial coefficient\nand its value is given by\n\\[\\binom{ m }{ i_1 i_2 \\dots i_d} = \\frac{m!}{i_1! i_2!\\dots i_d!}.\\]↩︎ Why is this identity true? Can you give a counting argument?↩︎ "],
["dist.html", "A Probability Distributions A.1 Discrete distributions: A.2 Continuous distributions:", " A Probability Distributions Here are the basic facts about the probability distributions we will need in these lecture notes. For a much longer list of important distributions, check this wikipedia page. A.1 Discrete distributions: Note: \\((q=1-p)\\) Parameters Notation Support pmf \\({\\mathbb{E}}[X]\\) \\(\\operatorname{Var}[X]\\) Bernoulli \\(p\\in (0,1)\\) \\(B(p)\\) \\(\\{0,1\\}\\) \\((q,p,0,0,\\dots)\\) \\(p\\) \\(pq\\) Binomial \\(n\\in{\\mathbb{N}}, p\\in (0,1)\\) \\(b(n,p)\\) \\(\\{0,1,\\dots, n\\}\\) \\(\\binom{n}{k} p^k q^{n-k}\\) \\(np\\) \\(npq\\) Geometric \\(p\\in (0,1)\\) \\(g(p)\\) \\(\\{0,1,\\dots\\}\\) \\(p q^k\\) \\(q/p\\) \\(q/p^2\\) Poisson \\(\\lambda\\in(0,\\infty)\\) \\(P(\\lambda)\\) \\(\\{0,1,\\dots\\}\\) \\(e^{-\\lambda} \\tfrac{\\lambda^k}{k!}\\) \\(\\lambda\\) \\(\\lambda\\) A.2 Continuous distributions: Note: the pdf is given by the formula in the table only on its support. It is equal to \\(0\\) outside of it. Parameters Notation Support pdf \\({\\mathbb{E}}[X]\\) \\(\\operatorname{Var}[X]\\) Uniform \\(a\\lt b\\) \\(U(a,b)\\) \\((a,b)\\) \\(\\frac{1}{b-a}\\) \\(\\frac{a+b}{2}\\) \\(\\frac{(b-a)^2}{12}\\) Normal \\(\\mu\\in{\\mathbb R},\\sigma \\gt 0\\) \\(N(\\mu,\\sigma)\\) \\({\\mathbb R}\\) \\(\\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\tfrac{(x-\\mu)^2}{2 \\sigma^2}}\\) \\(\\mu\\) \\(\\sigma^2\\) Exponential \\(\\lambda\\gt 0\\) \\(\\operatorname{Exp}(\\lambda)\\) \\((0,\\infty)\\) \\(\\lambda e^{-\\lambda x}\\) \\(\\tfrac{1}{\\lambda}\\) \\(\\frac{1}{\\lambda^2}\\) "]
]
