<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Lecture notes for &quot;Introduction to Stochastic Processes&quot;</title>
  <meta name="description" content="A set of lecture notes for <em>M362M: Introduction to Stochastic Processes</em>" />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="Lecture notes for &quot;Introduction to Stochastic Processes&quot;" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A set of lecture notes for <em>M362M: Introduction to Stochastic Processes</em>" />
  <meta name="github-repo" content="gordanz/M362M" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Lecture notes for &quot;Introduction to Stochastic Processes&quot;" />
  
  <meta name="twitter:description" content="A set of lecture notes for <em>M362M: Introduction to Stochastic Processes</em>" />
  

<meta name="author" content="Gordan Zitkovic" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Lecture notes for "Introduction to Stochastic Processes"</h1>
<p class="author"><em>Gordan Zitkovic</em></p>
<p class="date"><em>last updated - 2020-10-30</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#preface">Preface</a></li>
<li><a href="#intro"><span class="toc-section-number">1</span> An intro to R and RStudio</a>
<ul>
<li><a href="#setting-up-an-r-environment-on-your-computer"><span class="toc-section-number">1.1</span> Setting up an R environment on your computer</a>
<ul>
<li><a href="#installing-r"><span class="toc-section-number">1.1.1</span> Installing R</a></li>
<li><a href="#installing-rstudio"><span class="toc-section-number">1.1.2</span> Installing RStudio</a></li>
<li><a href="#installing-basic-packages"><span class="toc-section-number">1.1.3</span> Installing basic packages</a></li>
</ul></li>
<li><a href="#learning-the-basics-of-r"><span class="toc-section-number">1.2</span> Learning the basics of R</a>
<ul>
<li><a href="#the-console-scripts-and-r-notebooks"><span class="toc-section-number">1.2.1</span> The console, Scripts and R Notebooks</a></li>
<li><a href="#asking-for-help"><span class="toc-section-number">1.2.2</span> Asking for help</a></li>
<li><a href="#vectors"><span class="toc-section-number">1.2.3</span> Vectors</a></li>
<li><a href="#matrices"><span class="toc-section-number">1.2.4</span> Matrices</a></li>
<li><a href="#functions"><span class="toc-section-number">1.2.5</span> Functions</a></li>
<li><a href="#if-else-statements"><span class="toc-section-number">1.2.6</span> If-else statements</a></li>
</ul></li>
<li><a href="#problems"><span class="toc-section-number">1.3</span> Problems</a></li>
</ul></li>
<li><a href="#simulation-of-random-variables-and-monte-carlo"><span class="toc-section-number">2</span> Simulation of Random Variables and Monte Carlo</a>
<ul>
<li><a href="#simulation-of-some-common-probability-distributions"><span class="toc-section-number">2.1</span> Simulation of some common probability distributions</a></li>
<li><a href="#multivariate-distributions"><span class="toc-section-number">2.2</span> Multivariate Distributions</a></li>
<li><a href="#monte-carlo"><span class="toc-section-number">2.3</span> Monte Carlo</a></li>
<li><a href="#conditional-distributions"><span class="toc-section-number">2.4</span> Conditional distributions</a></li>
<li><a href="#additional-problems-for-chapter-2"><span class="toc-section-number">2.5</span> Additional Problems for Chapter 2</a></li>
</ul></li>
<li><a href="#random-walks"><span class="toc-section-number">3</span> Random Walks</a>
<ul>
<li><a href="#what-are-stochastic-processes"><span class="toc-section-number">3.1</span> What are stochastic processes?</a></li>
<li><a href="#the-simple-symmetric-random-walk"><span class="toc-section-number">3.2</span> The Simple Symmetric Random Walk</a></li>
<li><a href="#how-to-simulate-random-walks"><span class="toc-section-number">3.3</span> How to simulate random walks</a></li>
<li><a href="#two-ways-of-looking-at-a-stochastic-proceses"><span class="toc-section-number">3.4</span> Two ways of looking at a stochastic proceses</a>
<ul>
<li><a href="#column-wise-distributionally"><span class="toc-section-number">3.4.1</span> Column-wise (distributionally)</a></li>
<li><a href="#row-wise-trajectorially-or-path-wise"><span class="toc-section-number">3.4.2</span> Row-wise (trajectorially or path-wise)</a></li>
</ul></li>
<li><a href="#the-path-space"><span class="toc-section-number">3.5</span> The path space</a></li>
<li><a href="#the-distribution-of-x_n"><span class="toc-section-number">3.6</span> The distribution of <span class="math inline">\(X_n\)</span></a></li>
<li><a href="#biased-random-walks"><span class="toc-section-number">3.7</span> Biased random walks</a></li>
<li><a href="#additional-problems-for-chapter-3"><span class="toc-section-number">3.8</span> Additional problems for Chapter 3</a></li>
</ul></li>
<li><a href="#more-about-random-walks"><span class="toc-section-number">4</span> More about Random Walks</a>
<ul>
<li><a href="#the-reflection-principle"><span class="toc-section-number">4.1</span> The reflection principle</a></li>
<li><a href="#stopping-times"><span class="toc-section-number">4.2</span> Stopping times</a></li>
<li><a href="#walds-identity-and-gamblers-ruin"><span class="toc-section-number">4.3</span> Wald’s identity and Gambler’s ruin</a></li>
<li><a href="#additional-problems-for-chapter-4"><span class="toc-section-number">4.4</span> Additional problems for Chapter 4</a></li>
</ul></li>
<li><a href="#markov-chains"><span class="toc-section-number">5</span> Markov Chains</a>
<ul>
<li><a href="#the-markov-property"><span class="toc-section-number">5.1</span> The Markov property</a></li>
<li><a href="#first-examples"><span class="toc-section-number">5.2</span> First Examples</a>
<ul>
<li><a href="#random-walks"><span class="toc-section-number">5.2.1</span> Random walks</a></li>
<li><a href="#gambler"><span class="toc-section-number">5.2.2</span> Gambler’s ruin</a></li>
<li><a href="#regime-switching"><span class="toc-section-number">5.2.3</span> Regime Switching</a></li>
<li><a href="#deterministically-monotone-markov-chain"><span class="toc-section-number">5.2.4</span> Deterministically monotone Markov chain</a></li>
<li><a href="#not-a-markov-chain"><span class="toc-section-number">5.2.5</span> Not a Markov chain</a></li>
<li><a href="#turning-a-non-markov-chain-into-a-markov-chain"><span class="toc-section-number">5.2.6</span> Turning a non-Markov chain into a Markov chain</a></li>
<li><a href="#deterministic-functions-of-markov-chains-do-not-need-to-be-markov-chains"><span class="toc-section-number">5.2.7</span> Deterministic functions of Markov chains do not need to be Markov chains</a></li>
<li><a href="#a-game-of-tennis"><span class="toc-section-number">5.2.8</span> A game of tennis</a></li>
</ul></li>
<li><a href="#chapman-kolmogorov-equations"><span class="toc-section-number">5.3</span> Chapman-Kolmogorov equations</a></li>
<li><a href="#mc-sim"><span class="toc-section-number">5.4</span> How to simulate Markov chains</a></li>
<li><a href="#additional-problems-for-chapter-5"><span class="toc-section-number">5.5</span> Additional problems for Chapter 5</a></li>
</ul></li>
<li><a href="#classification-of-states"><span class="toc-section-number">6</span> Classification of States</a>
<ul>
<li><a href="#the-communication-relation"><span class="toc-section-number">6.1</span> The Communication Relation</a></li>
<li><a href="#classes"><span class="toc-section-number">6.2</span> Classes</a></li>
<li><a href="#transience-and-recurrence"><span class="toc-section-number">6.3</span> Transience and recurrence</a>
<ul>
<li><a href="#the-return-theorem"><span class="toc-section-number">6.3.1</span> The Return Theorem</a></li>
<li><a href="#a-recurrence-criterion"><span class="toc-section-number">6.3.2</span> A recurrence criterion</a></li>
<li><a href="#polyas-theorem"><span class="toc-section-number">6.3.3</span> Polya’s theorem</a></li>
</ul></li>
<li><a href="#class-properties"><span class="toc-section-number">6.4</span> Class properties</a></li>
<li><a href="#a-few-examples"><span class="toc-section-number">6.5</span> A few Examples</a></li>
<li><a href="#additional-problems-for-chapter-6"><span class="toc-section-number">6.6</span> Additional problems for Chapter 6</a></li>
</ul></li>
<li><a href="#appendix-appendix">(APPENDIX) Appendix</a></li>
<li><a href="#dist"><span class="toc-section-number">7</span> Probability Distributions</a>
<ul>
<li><a href="#discrete-distributions"><span class="toc-section-number">7.1</span> Discrete distributions:</a></li>
<li><a href="#continuous-distributions"><span class="toc-section-number">7.2</span> Continuous distributions:</a></li>
</ul></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Lecture notes for "Introduction to Stochastic Processes"</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="preface" class="section level1 unnumbered">
<h1 class="unnumbered">Preface</h1>
<p><img src="pics/title.png" width = 400 style="position:absolute;top:0px;left:50%;" /></p>
<p>This is an always-evolving set of lecture notes for <strong>Introduction to Stochastic Processes (M362M)</strong>. It should start with me explaining what stochastic processes are. Instead, here is a list of several questions you will be able to give answers to when you complete this course.</p>
<p><strong>Question 1</strong> In a simplistic model, the price of a share of a stock goes either up or down by <span class="math inline">\(\$1\)</span> each day, with probability <span class="math inline">\(1/2\)</span>. You own a single share whose value today is <span class="math inline">\(\$100\)</span>, so that its tomorrow’s price will be <span class="math inline">\(\$101\)</span> or <span class="math inline">\(\$99\)</span> with probability <span class="math inline">\(1/2\)</span>, etc. Your strategy is to hold onto your share until one of the following two things happen: you go bankrupt (the stock price hits <span class="math inline">\(0\)</span>), or you make a <span class="math inline">\(\$50\)</span> dollar profit (the stock price hits <span class="math inline">\(\$150\)</span>.)</p>
<ol style="list-style-type: decimal">
<li>How likely is it that you will make a profit before you go bankrupt?</li>
<li>How long will it take?</li>
<li>Is it possible that it takes forever, i.e., that the stock price hovers between <span class="math inline">\(\$1\)</span> and <span class="math inline">\(\$149\)</span> forever?</li>
</ol>
<p><strong>Question 2.</strong> A person carrying a certain disease infects a random number of people in a week, and then stops being infectious. Each of the infected people transmits the disease in the same way, etc. Suppose that the number of people each (infectious) individual infects is either <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span> or <span class="math inline">\(2\)</span> or <span class="math inline">\(3\)</span>, each with probability <span class="math inline">\(1/4\)</span> and that different infectious individuals may infect different number of people and behave independently of each other.</p>
<ol style="list-style-type: decimal">
<li>What is the probability that the disease will ever be eradicated?</li>
<li>What is the probability that every single individual in the population of <span class="math inline">\(328,000,000\)</span> will eventually be infected?</li>
</ol>
<p><strong>Question 3.</strong> In a game of tennis, Player <span class="math inline">\(1\)</span> wins against Player <span class="math inline">\(2\)</span> in each rally (the smallest chunk of the match that leads to point, i.e., to a score change from <span class="math inline">\(15-30\)</span> to <span class="math inline">\(30-30\)</span>, for example) with probability <span class="math inline">\(p\)</span>.
What is the probability that Player <span class="math inline">\(1\)</span> wins</p>
<ol style="list-style-type: decimal">
<li>a game (the chunk of the match that leads to a score change such as <span class="math inline">\(5-3\)</span> to <span class="math inline">\(6-3\)</span> within a set)?</li>
<li>a set? the entire match?</li>
<li>Is the game of tennis set up in such a way that is <em>amplifies</em> or <em>reduces</em> the difference in skill between players?</li>
</ol>
<p><img src="pics/knight1.png" width="35%" style="float:right; padding:10px" style="display: block; margin: auto;" /></p>
<p><strong>Question 4.</strong>
A knight starts in the lower left corner of the chess board and starts moving ``randomly’’. That means that from any position, it chooses one of the possible (legal) moves and takes it, with all legal moves having the same probability. It keeps doing the same thing until it comes back to the square it started from.</p>
<ol style="list-style-type: decimal">
<li>What is the expected number of moves the knight will make before it returns to “square one”?</li>
<li>How about the same problem, but using a different chess piece? Which one do you think will come back is the smallest (expected) number of steps?</li>
<li>(*) How about the same problem, but until <em>all</em> squares have been visited at least once?</li>
</ol>
<p><strong>Question 5.</strong> How does Google search work?</p>
<!--chapter:end:index.Rmd-->
</div>
<div id="intro" class="section level1" number="1">
<h1 number="1"><span class="header-section-number">1</span> An intro to R and RStudio</h1>
<div id="setting-up-an-r-environment-on-your-computer" class="section level2" number="1.1">
<h2 number="1.1"><span class="header-section-number">1.1</span> Setting up an R environment on your computer</h2>
<div id="installing-r" class="section level3" number="1.1.1">
<h3 number="1.1.1"><span class="header-section-number">1.1.1</span> Installing R</h3>
<p>Learning basic R is an important part of this course, and the first order of business is to download and install an R distribution on your personal computer. We will be using RStudio as an IDE (integrated development environment). Like R itself, it is free and readily available for all major platforms. To download R to your computer, go to
<a href="https://cloud.r-project.org">https://cloud.r-project.org</a> and
download the version of R for your operating system (Windows, Mac or Linux). If you are on a Mac, you want the “Latest release” which, at the time of writing, is 4.0.2. On Windows, follow the link “install R for the first time”. We are not going to do any cutting edge stuff in this class, so an older release should be fine, too, if you happen to have it already installed on your system. Once you download the installation file (.pkg on a Mac or .exe on Windows), run it and follow instructions. If you are running Linux, you don’t need me to tell you what to do. Once it is successfully installed, <strong>don’t run the installed app</strong>. We will use RStudio for that.</p>
</div>
<div id="installing-rstudio" class="section level3" number="1.1.2">
<h3 number="1.1.2"><span class="header-section-number">1.1.2</span> Installing RStudio</h3>
<p>To install RStudio, go to <a href="https://rstudio.com/products/rstudio/download/">https://rstudio.com/products/rstudio/download/</a>. There are several versions to choose from - the one you are looking for is “RStudio desktop - Free”. After you download and install it, you are ready to run it. When it opens, you will see something like this</p>
<p><img src="pics/RStudio_IDE.png" width="100%" style="padding:10px" style="display: block; margin: auto;" /></p>
<p>The part on the left is called the <em>console</em> and that is (one of the places) where you enter commands. Before you do, it is important to adjust a few settings. Open the options window by navigating to to Tools-&gt;Global Options. In there, uncheck “Restore .RData into workspace on startup” and set “Save workspace to .RData on exit” to “Never”, as shown below:
<img src="pics/Restore_RData.png" width="75%" style="margin-left: 12.5%;margin-right:12.5%; padding:10px" style="display: block; margin: auto;" /></p>
<p>This way, R will not pollute your environment with values you defined two weeks ago and completely forgot about. These settings are really an atavism and serve no purpose (for users like us) other than to introduce hard-to-track bugs.</p>
<p>There are many other settings you can play with in RStudio, but the two I mentioned above are the only ones that I really recommend setting as soon as you install it.</p>
</div>
<div id="installing-basic-packages" class="section level3" number="1.1.3">
<h3 number="1.1.3"><span class="header-section-number">1.1.3</span> Installing basic packages</h3>
<p>Finally, we need to install several R packages we will be using (mostly implicitly) during the class. First, run the following command in your console</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a>install.packages( &quot;tidyverse&quot;)</span></code></pre></div>
<p>This will install a number of useful packages and should only take about a minute or two. The next part is a bit longer, and can take up to 15 minutes if you have a slow computer/internet connection.
You only have to do it once, though. Skip both steps involving <code>tinytex</code> below if you have LaTeX already installed on your system<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Start with</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>install.packages(&quot;tinytex&quot;)</span></code></pre></div>
<p>followed by</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode markdown"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>tinytex::install_tinytex()</span></code></pre></div>
<p>Note that if you go to the top right corner of each of the code blocks (gray boxes) containing instructions above, an icon will appear. If you click on it, it will copy the content of the box into your clipboard, and you can simply paste it into RStudio. You can do that with any code block in these notes.</p>
</div>
</div>
<div id="learning-the-basics-of-r" class="section level2" number="1.2">
<h2 number="1.2"><span class="header-section-number">1.2</span> Learning the basics of R</h2>
<p>Once R and RStudio are on your computer, it is time to get acquainted with the basics of R. This class is not about the finer points of R itself, and I will try to make your R experience as smooth as possible. After all, R is a tool that will help us explore and understand stochastic processes. Having said that, it is important to realize that R is a powerful programming language specifically created for statistical and probabilistic applications. Some knowledge of R is a valuable skill to have in today’s job market, and you should take this opportunity to learn it. The best way, of course, is by using it, but before you start, you need to know the very basics. Don’t worry, R is very user friendly and easy to get started in. In addition, it has been around for a long time (its predecessor S appeared in 1976) and is extremely well documented - google <em>introduction to R</em> or a similar phrase, and you will get lots of useful hits.</p>
<p>My plan is to give you a bare minimum in the next few paragraphs, and then to explain additional R concepts as we need them. This way, you will not be overwhelmed right from the start, and you will get a bit of a mathematical context as you learn more. Conversely, learning R commands will help with the math, too.</p>
<div id="the-console-scripts-and-r-notebooks" class="section level3" number="1.2.1">
<h3 number="1.2.1"><span class="header-section-number">1.2.1</span> The console, Scripts and R Notebooks</h3>
<p>There at least three different ways of inputting commands into R - through console, scripts and R-notebooks.</p>
<p><strong>The console</strong>, as I already mentioned, is a window in RStudio where you can enter your R commands one by one. As a command is entered (and enter pressed) R will run it and display the result below. A typical console session looks like this
<img src="pics/console_session.png" width="100%" style="padding:10px" style="display: block; margin: auto;" />
If you define a variable in a command, it will be available in all the subsequent commands. This way of interacting with R is perfect for quick-and-dirty computations and, what is somewhat euphemistically called “prototyping”. In other words, this way you are using R as a calculator. There is another reason why you might be using the console. It is perfect for package installation and for help-related commands. If you type <code>help('log')</code>, the output will appear in the <code>Help</code> pane on the right. You can also see all the available variables in the <code>Environment</code> pane on the (top) right.</p>
<p>As your needs increase, you will need more complex (and longer) code to meet them. This is where <strong>scripts</strong> come in. They are text files (but have the extension <code>.R</code>) that hold R code. Scripts can run as a whole, and be saved for later. To create a new script, go to File-&gt;New File-&gt;R Script. That will split your RStudio window in two:
<img src="pics/script_console.png" width="100%" style="padding:10px" style="display: block; margin: auto;" />
The top part will become a script editor, and your console will shrink to occupy the bottom part. You can write you code in there, edit and update it, and then run the whole script by clicking on Source, or pressing the associated shortcut key.</p>
<p>Inspired by Python Jupyter notebooks, <strong>R notebooks</strong> are a creature somewhere between scripts and the console, but also have some features of their own.
An R notebook is nothing other than a specially formatted text file which contains <em>chunks</em> of R code mixed with regular text. You can think of these chunks as mini scripts. What differentiates them from scripts is that chunks can be executed (evaluated) and the output becomes a part of the notebook:
<img src="pics/notebooks.png" width="100%" style="padding:10px" style="display: block; margin: auto;" />
R notebooks are R’s implementation of <em>literate programming</em>. The idea is that documentation should be written at the same time as the program itself. As far as this course if concerned, R notebooks are just the right medium for homework and exam submission. You can run code and provide the interpretation of its output in a single document. See <a href="other/Homework-instructions.html">here</a> for more information.</p>
<p>Each chapter in these lecture notes is an R notebook!</p>
</div>
<div id="asking-for-help" class="section level3" number="1.2.2">
<h3 number="1.2.2"><span class="header-section-number">1.2.2</span> Asking for help</h3>
<p>The most important thing about <strong>learning R</strong> (and many other things, for that matter) is knowing whom (and how) to ask for help. Luckily, R is a well established language, and you can get a lot of information by simply googling your problem. For example, if you google <code>logarithm in R</code> the top hit (at the time of writing) gives a nice overview and some examples.</p>
<p>Another way to get information about a command or a concept in R is to use the command <code>help</code>. For example, if you input <code>help("log")</code> or <code>?log</code> in your console, the right hand of your screen will display information on the function <code>log</code> and some of its cousins. Almost every help entry has examples at the bottom, and that is where I always go first.</p>
</div>
<div id="vectors" class="section level3" number="1.2.3">
<h3 number="1.2.3"><span class="header-section-number">1.2.3</span> Vectors</h3>
<p>Objects we will be manipulating in this class are almost exclusively vectors and matrices. The simplest vectors are those that have a single component, in other words, numbers. In R, you can assign a number to a variable using two different notations. Both</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>a &lt;-<span class="st"> </span><span class="dv">1</span></span></code></pre></div>
<p>and</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>a =<span class="st"> </span><span class="dv">1</span></span></code></pre></div>
<p>will assign the value <span class="math inline">\(1\)</span> to the variable <code>a</code>. If you want to create a longer vector, you can use the <strong>concatenation operator</strong> <code>c</code> as follows:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)</span></code></pre></div>
<p>Once you evaluate the above in your console, the value of <code>x</code> is stored and you can access it by using the command <code>print</code></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="kw">print</span>(x)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="co">## [1] 1 2 3 4</span></span></code></pre></div>
<p>or simply evaluating <code>x</code> itself:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a>x</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a><span class="co">## [1] 1 2 3 4</span></span></code></pre></div>
<p>Unlike all code blocks above them, the last two contain both input and output. It is standard not to mark the output by any symbol (like the usual <code>&gt;</code>), and to mark the output by <code>##</code> which otherwise marks comments. This way, you can copy any code block from these notes and paste it into the console (or your script) without having to modify it in any way. Try it!</p>
<p>We built the vector <code>x</code> above by concatenating four numbers (vectors of length 1). You can concatenate vectors of different sizes, too:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a>a =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a>b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a>(<span class="dt">x =</span> <span class="kw">c</span>(a, b, <span class="dv">7</span>))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a><span class="co">## [1] 1 2 3 4 5 6 7</span></span></code></pre></div>
<p>You may be wondering why I put <code>x = c(a,b,7)</code> in parentheses. Without them, <code>x</code> would still become (1,2,3,4,5,6,7), but its value would not be printed out. A statement in parentheses is not only evaluated, but its result is also printed out. This way, <code>(x = 2+3)</code> is equivalent to <code>x = 2+3</code> followed by <code>x</code> or <code>print(x)</code>.</p>
<p>Vectors can contain things other than numbers. Strings, for example:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="st">&quot;Picard&quot;</span>, <span class="st">&quot;Data&quot;</span>, <span class="st">&quot;Geordi&quot;</span>))</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a><span class="co">## [1] &quot;Picard&quot; &quot;Data&quot;   &quot;Geordi&quot;</span></span></code></pre></div>
<p>If you need a vector consisting of consecutive numbers, use the colon <code>:</code> notation:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a><span class="dv">1</span><span class="op">:</span><span class="dv">10</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a><span class="co">##  [1]  1  2  3  4  5  6  7  8  9 10</span></span></code></pre></div>
<p>For sequences of equally spaced numbers, use the command <code>seq</code> (check its help for details)</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">5</span>, <span class="dt">to =</span> <span class="dv">20</span>, <span class="dt">by =</span> <span class="dv">3</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a><span class="co">## [1]  5  8 11 14 17 20</span></span></code></pre></div>
<p>An important feature or R is that many of its functions are <strong>vectorized</strong>. That means that if you give such a function a vector as an argument, the returned value will be a vector of results of that operation performed element by element. For example</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true"></a>x <span class="op">+</span><span class="st"> </span>y</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true"></a><span class="co">## [1] 12 24 35</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true"></a>x <span class="op">*</span><span class="st"> </span>y</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true"></a><span class="co">## [1]  20  80 150</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true"></a>x<span class="op">^</span><span class="dv">2</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true"></a><span class="co">## [1] 100 400 900</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true"></a><span class="kw">cos</span>(x)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true"></a><span class="co">## [1] -0.8390715  0.4080821  0.1542514</span></span></code></pre></div>
<p>The vectors do not need to be of the same size. R uses the <strong>recycling rule</strong> -
it recycles the values of the shorter one, starting from the beginning, until
its size matches the longer one:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>, <span class="dv">60</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true"></a>x <span class="op">+</span><span class="st"> </span>y</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true"></a><span class="co">## [1] 11 23 31 43 51 63</span></span></code></pre></div>
<p>The case where the shorter vector is of length 1 is particularly useful:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true"></a>x <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true"></a><span class="co">## [1] 11 21 31 41</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true"></a>x <span class="op">*</span><span class="st"> </span>(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true"></a><span class="co">## [1] -20 -40 -60 -80</span></span></code></pre></div>
<p>Extracting parts of the vector is accomplished by using the <strong>indexing</strong> operator <code>[]</code>. Here are some examples (what do negative numbers do?)</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true"></a>x[<span class="dv">1</span>]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true"></a><span class="co">## [1] 10</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true"></a>x[<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)]</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true"></a><span class="co">## [1] 10 20</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true"></a>x[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true"></a><span class="co">## [1] 20 30 40 50</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true"></a>x[<span class="op">-</span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">4</span>)]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true"></a><span class="co">## [1] 10 20 50</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true"></a>x[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true"></a><span class="co">## [1] 10 20 30 40</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true"></a>x[<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">4</span>)]</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true"></a><span class="co">## [1] 10 10 20 20 50 40</span></span></code></pre></div>
<p>People familiar with Python should be aware of the following two differences: 1. indexing starts at 1 and not 0, and 2. negative indexing removes components; it does not start counting from the end!</p>
<p>It is important to note that the thing you put inside <code>[]</code> needs to be a vector itself. The above examples all dealt with numerical indices, but you can use logical indices, too. A variable is said to be <strong>logical</strong> or <strong>Boolean</strong> if it can take only one of the two values <code>TRUE</code> or <code>FALSE</code>. A vector whose components are all logical, are called, of course, logical vectors. You can think of logical indexing as the operation where you go through your original vector, and choose which components you want to keep (<code>TRUE</code>) and which you want the throw away (<code>FALSE</code>). For example</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">c</span>(<span class="ot">TRUE</span>, <span class="ot">FALSE</span>, <span class="ot">FALSE</span>, <span class="ot">TRUE</span>, <span class="ot">TRUE</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true"></a>x[y]</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true"></a><span class="co">## [1] 10 40 50</span></span></code></pre></div>
<p>This is especially useful when used together with the <strong>comparison operators</strong>. The expressions like <code>x &lt; y</code> or <code>x == y</code> are operators<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> in R, just like <code>x + y</code> or <code>x / y</code>. The difference is that <code>&lt;</code> and <code>==</code> return logical values. For example</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true"></a><span class="dv">1</span> <span class="op">==</span><span class="st"> </span><span class="dv">2</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true"></a><span class="co">## [1] FALSE</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true"></a><span class="dv">3</span> <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true"></a><span class="co">## [1] FALSE</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true"></a><span class="dv">3</span> <span class="op">&gt;=</span><span class="st"> </span><span class="dv">2</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true"></a><span class="co">## [1] TRUE</span></span></code></pre></div>
<p>These operators are vectorized, so you can do things like this</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true"></a>x <span class="op">==</span><span class="st"> </span>y</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true"></a><span class="co">## [1]  TRUE FALSE  TRUE FALSE  TRUE</span></span></code></pre></div>
<p>or, using recycling,</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true"></a>x <span class="op">&gt;</span><span class="st"> </span><span class="dv">3</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true"></a><span class="co">## [1] FALSE FALSE FALSE  TRUE  TRUE</span></span></code></pre></div>
<p>Let’s combine that with indexing. Suppose that we want to keep only the values greater than 4 in the vector <code>x</code>. The vector <code>y = ( x &gt; 4 )</code> is going to be of the same length as <code>x</code> and contain logical values.
When we index <code>x</code> using it, only the values of <code>x</code> on positions where <code>x &gt; 4</code> will survive, and these are exactly the values we needed:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">4</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true"></a>y =<span class="st"> </span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true"></a>x[y]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true"></a><span class="co">## [1] 5 5 6</span></span></code></pre></div>
<p>or, simply,</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true"></a>x[x <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span>]</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true"></a><span class="co">## [1] 5 5 6</span></span></code></pre></div>
<p>Indexing can be used to set the values of a vector just as easily</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true"></a>x[<span class="dv">2</span><span class="op">:</span><span class="dv">4</span>] =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true"></a>x</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true"></a><span class="co">## [1] 10  0  1  2 50</span></span></code></pre></div>
<p>Recycling rules apply in the same way as above</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>)</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true"></a>x[<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">5</span>)] =<span class="st"> </span><span class="dv">7</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true"></a>x</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true"></a><span class="co">## [1]  7  7 30 40  7</span></span></code></pre></div>
</div>
<div id="matrices" class="section level3" number="1.2.4">
<h3 number="1.2.4"><span class="header-section-number">1.2.4</span> Matrices</h3>
<p>A matrix in R can be created using the command <code>matrix</code>. The unusual part is that the input is a vector and R populates the components of the matrix by filling it in column by column or row by row. As always, an example will make this clear</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true"></a>(<span class="dt">A =</span> <span class="kw">matrix</span>(x, <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>))</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true"></a><span class="co">##      [,1] [,2] [,3]</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true"></a><span class="co">## [1,]    1    2    3</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true"></a><span class="co">## [2,]    4    5    6</span></span></code></pre></div>
<p>The first argument of the function <code>matrix</code> is the vector which contains all the values. If you want a matrix with m rows and n columns, this vector should be of size <span class="math inline">\(m n\)</span>. The arguments <code>ncol</code> and <code>nrow</code> are self-explanatory, and <code>byrow</code> is a logical argument which signals whether to fill by columns or by rows. Here is what happens when we set <code>byrow = FALSE</code></p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true"></a>(<span class="dt">A =</span> <span class="kw">matrix</span>(x, <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">byrow =</span> <span class="ot">FALSE</span>))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true"></a><span class="co">##      [,1] [,2] [,3]</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true"></a><span class="co">## [1,]    1    3    5</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true"></a><span class="co">## [2,]    2    4    6</span></span></code></pre></div>
<p>Accessing components of a matrix is as intuitive as it gets</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true"></a>(<span class="dt">A =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">7</span>, <span class="dv">2</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">2</span>))</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true"></a><span class="co">## [1,]    1    7</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true"></a><span class="co">## [2,]   -1    2</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true"></a></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true"></a>A[<span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true"></a><span class="co">## [1] 7</span></span></code></pre></div>
<p>Note that I did not use the argument <code>byrow</code> at all. In such cases, R always uses the default value (documented in the function’s help). For <code>matrix</code> the default value of <code>byrow</code> is <code>FALSE</code>, i.e., it fills the matrix column by column. This is not what we usually want because we tend to think of matrices as composed of rows. Moral: do not forget <code>byrow = TRUE</code> if that is what you, indeed, want.</p>
<p>Usual matrix operations can be performed in R in the obvious way</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true"></a>(<span class="dt">A =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-1</span>, <span class="dv">7</span>, <span class="dv">2</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">2</span>))</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true"></a><span class="co">## [1,]    1    7</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true"></a><span class="co">## [2,]   -1    2</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true"></a>(<span class="dt">B =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">-3</span>, <span class="dv">-4</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">2</span>))</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true"></a><span class="co">## [1,]    2   -3</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true"></a><span class="co">## [2,]    2   -4</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true"></a>A <span class="op">+</span><span class="st"> </span>B</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true"></a><span class="co">## [1,]    3    4</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true"></a><span class="co">## [2,]    1   -2</span></span></code></pre></div>
<p>You should be careful with matrix multiplication. The naive operator <code>*</code> yields a matrix, but probably not the one you want (what does <code>*</code> do?)</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true"></a>(<span class="dt">A =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">2</span>))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true"></a><span class="co">## [1,]    1    0</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true"></a><span class="co">## [2,]    2    1</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true"></a>(<span class="dt">B =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>), <span class="dt">nrow =</span> <span class="dv">2</span>, <span class="dt">ncol =</span> <span class="dv">2</span>))</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true"></a><span class="co">## [1,]    3    1</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true"></a><span class="co">## [2,]    5    0</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true"></a>A <span class="op">*</span><span class="st"> </span>B</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true"></a><span class="co">## [1,]    3    0</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true"></a><span class="co">## [2,]   10    0</span></span></code></pre></div>
<p>If you want the matrix product, you have to use <code>%*%</code></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true"></a>A <span class="op">%*%</span><span class="st"> </span>B</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true"></a><span class="co">## [1,]    3    1</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true"></a><span class="co">## [2,]   11    2</span></span></code></pre></div>
</div>
<div id="functions" class="section level3" number="1.2.5">
<h3 number="1.2.5"><span class="header-section-number">1.2.5</span> Functions</h3>
<p>The following syntax is used to define functions in R:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true"></a>my_function =<span class="st"> </span><span class="cf">function</span>(x, y, z) {</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true"></a>    <span class="kw">return</span>(x <span class="op">+</span><span class="st"> </span>y <span class="op">+</span><span class="st"> </span>z)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true"></a>}</span></code></pre></div>
<p>The function <code>my_function</code> returns the sum of its arguments. Having defined it, as above, we can use it like this</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true"></a><span class="kw">my_function</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">9</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true"></a><span class="co">## [1] 13</span></span></code></pre></div>
<p>Neither the output nor the arguments of a function in R are restricted to numbers. Our next example function, named <code>winners</code>, takes two vectors as arguments and returns a vector. Its components are those components of the first input vector (<code>x</code>) that are larger than the corresponding components of the second input vector (<code>y</code>)</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true"></a>winners =<span class="st"> </span><span class="cf">function</span>(x, y) {</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true"></a>    z =<span class="st"> </span>x <span class="op">&gt;</span><span class="st"> </span>y</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true"></a>    <span class="kw">return</span>(x[z])</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true"></a>}</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true"></a><span class="kw">winners</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">2</span>), <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">9</span>, <span class="dv">2</span>))</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true"></a><span class="co">## [1] 4 5</span></span></code></pre></div>
<p>Note how we used several things we learned above in this function. First, we defined the logical vector which indicates where <code>x</code> is larger than <code>y</code>. Then, we used logical indexing to return only certain components of <code>x</code>.</p>
</div>
<div id="if-else-statements" class="section level3" number="1.2.6">
<h3 number="1.2.6"><span class="header-section-number">1.2.6</span> If-else statements</h3>
<p>Our final element of R is its <code>if-else</code> statement. The syntax of the <code>if</code> statement is</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true"></a><span class="cf">if</span> (condition) {</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true"></a>    statement</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true"></a>}</span></code></pre></div>
<p>where <code>condition</code> is anything that has a logical value, and statement is any R statement. First R evaluates <code>condition</code>. If it is true, it runs <code>statement</code>. If it is false, nothing happens. If you want something to happen if (and only if) your condition is false, you need an <code>if-else</code> statement:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true"></a><span class="cf">if</span> (condition) {</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true"></a>    statement1</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true"></a>} <span class="cf">else</span> {</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true"></a>    statement2</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true"></a>}</span></code></pre></div>
<p>This way, <code>statement1</code> is evaluated when <code>condition</code> is true and <code>statement1</code> when it is false. Since conditions inside the <code>if</code> statement return logical values, we can combine them using <em>ands</em>, <em>ors</em> or <em>nots</em>. The R notation for these operations is &amp;, | and ! respectively, and to remind you what they do, here is a simple table</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
x
</th>
<th style="text-align:left;">
y
</th>
<th style="text-align:left;">
x &amp; y (and)
</th>
<th style="text-align:left;">
x | y (or)
</th>
<th style="text-align:left;">
!x (not)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
TRUE
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
<tr>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
FALSE
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
</tbody>
</table>
<p>Let’s put what we learned about functions and if-else statements together to write a function <code>distance_or_zero</code> whose arguments are coordinates <code>x</code> and <code>y</code> of a point in the plane, and whose output is the distance from the point (x,y) to the origin if this distance happens to be between 1 and 2, and and 0 otherwise. We will use similar functions later when we discuss Monte Carlo methods:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true"></a>distance_or_zero =<span class="st"> </span><span class="cf">function</span>(x, y) {</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true"></a>    distance =<span class="st"> </span><span class="kw">sqrt</span>(x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true"></a>    <span class="cf">if</span> (distance <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2</span> <span class="op">&amp;</span><span class="st"> </span>distance <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span>) {</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true"></a>        <span class="kw">return</span>(distance)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true"></a>    } <span class="cf">else</span> {</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true"></a>        <span class="kw">return</span>(<span class="dv">0</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true"></a>    }</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true"></a>}</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true"></a><span class="kw">distance_or_zero</span>(<span class="fl">1.2</span>, <span class="fl">1.6</span>)</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true"></a><span class="co">## [1] 2</span></span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true"></a><span class="kw">distance_or_zero</span>(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true"></a><span class="co">## [1] 0</span></span></code></pre></div>
</div>
</div>
<div id="problems" class="section level2" number="1.3">
<h2 number="1.3"><span class="header-section-number">1.3</span> Problems</h2>
<p>Here are several simple problems. Their goal is to give you an idea of exactly how much R is required to get started in this course.</p>
<div class="problem">
<p>Compute the following (your answer should be a decimal number):</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(1/238746238746\)</span></li>
<li><span class="math inline">\(2^{45}\)</span></li>
<li><span class="math inline">\(3^{28}\)</span></li>
<li><span class="math inline">\(\sqrt{15}\)</span></li>
<li><span class="math inline">\(\cos(\pi/8)\)</span>w</li>
<li><span class="math inline">\(e^2\)</span></li>
<li><span class="math inline">\(\log(2)\)</span> (the base is <span class="math inline">\(e\)</span>)</li>
<li><span class="math inline">\(\log_{10}(2)\)</span> (the base is <span class="math inline">\(10\)</span>)</li>
<li><span class="math inline">\(\sqrt[3]{ \frac{1342.16-2.18}{(3 \pi +4.12)^2}}\)</span></li>
</ol>
<p>Note: some of the answers will look like this <code>3.14e+13</code>. If you do not know what that means, google <em>E notation</em>.</p>
</div>
<div class="solution">
<p>Remember, parentheses around the whole expression tell R to print out the result</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true"></a>(<span class="dv">1</span><span class="op">/</span><span class="dv">238746238746</span>)</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true"></a><span class="co">## [1] 0.000000000004188548</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true"></a>(<span class="dv">2</span><span class="op">^</span>(<span class="dv">45</span>))  <span class="co"># this is the same as 2**45</span></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true"></a><span class="co">## [1] 35184372088832</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true"></a>(<span class="dv">3</span><span class="op">^</span>(<span class="dv">28</span>))</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true"></a><span class="co">## [1] 22876792454961</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true"></a>(<span class="kw">sqrt</span>(<span class="dv">15</span>))</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true"></a><span class="co">## [1] 3.872983</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true"></a>(<span class="kw">cos</span>(pi<span class="op">/</span><span class="dv">8</span>))</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true"></a><span class="co">## [1] 0.9238795</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true"></a>(<span class="kw">exp</span>(<span class="dv">2</span>))</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true"></a><span class="co">## [1] 7.389056</span></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true"></a>(<span class="kw">log</span>(<span class="dv">2</span>))</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true"></a><span class="co">## [1] 0.6931472</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true"></a>(<span class="kw">log10</span>(<span class="dv">2</span>))</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true"></a><span class="co">## [1] 0.30103</span></span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true"></a>(((<span class="fl">1342.16</span> <span class="op">-</span><span class="st"> </span><span class="fl">2.18</span>)<span class="op">/</span>(<span class="dv">3</span> <span class="op">*</span><span class="st"> </span>pi <span class="op">+</span><span class="st"> </span><span class="fl">4.12</span>)<span class="op">^</span><span class="dv">2</span>)<span class="op">^</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>))</span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true"></a><span class="co">## [1] 1.940222</span></span></code></pre></div>
</div>
<div class="problem">
<ol style="list-style-type: decimal">
<li><p>Define two variables <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> with values <span class="math inline">\(3\)</span> and <span class="math inline">\(4\)</span> and “put” their product into a variable called <span class="math inline">\(c\)</span>. Output the value of <span class="math inline">\(c\)</span>.</p></li>
<li><p>Define two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> of length <span class="math inline">\(3\)</span>, such that the components of <span class="math inline">\(x\)</span> are <span class="math inline">\(1,2,3\)</span> and the components of <span class="math inline">\(y\)</span> are <span class="math inline">\(8,9,0\)</span>. Ouput their (componentwise) sum.</p></li>
<li><p>Define a <span class="math inline">\(2\times 2\)</span> matrix <span class="math inline">\(A=\begin{pmatrix} 1 &amp; 2 \\ -1 &amp; 3 \end{pmatrix}\)</span>.</p></li>
<li><p>Compute the matrix square <span class="math inline">\(A^2\)</span>.</p></li>
</ol>
</div>
<div class="solution">
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true"></a>a =<span class="st"> </span><span class="dv">3</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true"></a>b =<span class="st"> </span><span class="dv">4</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true"></a>(<span class="dt">c =</span> a <span class="op">*</span><span class="st"> </span>b)</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true"></a><span class="co">## [1] 12</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true"></a></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">8</span>, <span class="dv">9</span>, <span class="dv">0</span>)</span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true"></a>(x <span class="op">+</span><span class="st"> </span>y)</span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true"></a><span class="co">## [1]  9 11  3</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true"></a>(<span class="dt">A =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">-1</span>, <span class="dv">3</span>), <span class="dt">byrow =</span> <span class="ot">TRUE</span>, <span class="dt">nrow =</span> <span class="dv">2</span>))</span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true"></a><span class="co">## [1,]    1    2</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true"></a><span class="co">## [2,]   -1    3</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true"></a></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true"></a>(A <span class="op">%*%</span><span class="st"> </span>A)</span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true"></a><span class="co">##      [,1] [,2]</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true"></a><span class="co">## [1,]   -1    8</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true"></a><span class="co">## [2,]   -4    7</span></span></code></pre></div>
</div>
<div class="problem">
<ol style="list-style-type: decimal">
<li><p>Construct a vector <span class="math inline">\(x\)</span> which contains all numbers from <span class="math inline">\(1\)</span> to <span class="math inline">\(100\)</span>.</p></li>
<li><p>Construct a vector <span class="math inline">\(y\)</span> which contains <em>squares</em> of all numbers between <span class="math inline">\(20\)</span> and <span class="math inline">\(2000\)</span>.</p></li>
<li><p>Construct a vector <span class="math inline">\(z\)</span> which contains only those components of <span class="math inline">\(y\)</span> whose values are between <span class="math inline">\(400,000\)</span> and <span class="math inline">\(500,000\)</span>.</p></li>
<li><p>Compute the average (arithmetic mean) of all the components of <span class="math inline">\(z\)</span>. There is an R function that does that for you - find it!</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">100</span></span></code></pre></div>
<p><part> 2. </part></p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true"></a>y =<span class="st"> </span>(<span class="dv">20</span><span class="op">:</span><span class="dv">2000</span>)<span class="op">^</span><span class="dv">2</span></span></code></pre></div>
<p><part> 3. </part></p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true"></a>(<span class="dt">z =</span> y[y <span class="op">&gt;</span><span class="st"> </span><span class="dv">400000</span> <span class="op">&amp;</span><span class="st"> </span>y <span class="op">&lt;</span><span class="st"> </span><span class="dv">500000</span>])</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true"></a><span class="co">##  [1] 400689 401956 403225 404496 405769 407044 408321 409600 410881 412164</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true"></a><span class="co">## [11] 413449 414736 416025 417316 418609 419904 421201 422500 423801 425104</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true"></a><span class="co">## [21] 426409 427716 429025 430336 431649 432964 434281 435600 436921 438244</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true"></a><span class="co">## [31] 439569 440896 442225 443556 444889 446224 447561 448900 450241 451584</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true"></a><span class="co">## [41] 452929 454276 455625 456976 458329 459684 461041 462400 463761 465124</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true"></a><span class="co">## [51] 466489 467856 469225 470596 471969 473344 474721 476100 477481 478864</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true"></a><span class="co">## [61] 480249 481636 483025 484416 485809 487204 488601 490000 491401 492804</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true"></a><span class="co">## [71] 494209 495616 497025 498436 499849</span></span></code></pre></div>
<p><part> 4. </part></p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true"></a>(<span class="kw">mean</span>(z))</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true"></a><span class="co">## [1] 449368.7</span></span></code></pre></div>
</div>
<div class="problem">
<ol style="list-style-type: decimal">
<li><p>Write a function that takes a numerical argument <span class="math inline">\(x\)</span> and returns <span class="math inline">\(5\)</span> if <span class="math inline">\(x\geq 5\)</span> and <span class="math inline">\(x\)</span> itself otherwise.</p></li>
<li><p>Write a function that returns <code>TRUE</code> (a logical value) if its argument is between <span class="math inline">\(2\)</span> and <span class="math inline">\(3\)</span> and <code>FALSE</code> otherwise.</p></li>
<li><p>(Extra credit) Write a function that takes two equal-sized vectors as arguments and returns the angle between them in degrees. For definiteness, the angle between two vectors is defined to be <span class="math inline">\(0\)</span> when either one of them is <span class="math inline">\((0,0,\dots,0)\)</span>.</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true"></a>f =<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true"></a>    <span class="cf">if</span> (x <span class="op">&gt;</span><span class="st"> </span><span class="dv">5</span>) {</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true"></a>        <span class="kw">return</span>(<span class="dv">5</span>)</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true"></a>    } <span class="cf">else</span> {</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true"></a>        <span class="kw">return</span>(x)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true"></a>    }</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true"></a>}</span></code></pre></div>
<p>This could be made much shorter using the function <code>ifelse</code>:</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true"></a>f =<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">ifelse</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">5</span>, <span class="dv">5</span>, x)</span></code></pre></div>
<p><part> 2. </part></p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true"></a>g &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true"></a>    <span class="cf">if</span> (x <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span> <span class="op">&amp;</span><span class="st"> </span>x <span class="op">&lt;</span><span class="st"> </span><span class="dv">3</span>) {</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true"></a>        <span class="kw">return</span>(<span class="ot">TRUE</span>)</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true"></a>    } <span class="cf">else</span> {</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true"></a>        <span class="kw">return</span>(<span class="ot">FALSE</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true"></a>    }</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true"></a>}</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true"></a><span class="co"># more R-ic (what&#39;s &#39;Pythonic&#39;, but for R?)</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true"></a>g &lt;-<span class="st"> </span><span class="cf">function</span>(x) x <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span> <span class="op">&amp;</span><span class="st"> </span>x <span class="op">&lt;</span><span class="st"> </span><span class="dv">3</span></span></code></pre></div>
<p><part> 3. </part></p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true"></a>angle &lt;-<span class="st"> </span><span class="cf">function</span>(x, y) {</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true"></a>    length_x =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true"></a>    length_y =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(y<span class="op">^</span><span class="dv">2</span>))</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true"></a>    <span class="cf">if</span> (length_y <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">|</span><span class="st"> </span>length_x <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true"></a>        <span class="kw">return</span>(<span class="dv">0</span>)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true"></a>    } <span class="cf">else</span> {</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true"></a>        inn_prod =<span class="st"> </span><span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span>y)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true"></a>        cos_alpha =<span class="st"> </span>inn_prod<span class="op">/</span>(length_x <span class="op">*</span><span class="st"> </span>length_y)</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true"></a>        <span class="kw">return</span>(<span class="kw">acos</span>(cos_alpha)<span class="op">/</span>pi <span class="op">*</span><span class="st"> </span><span class="dv">180</span>)</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true"></a>    }</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true"></a>}</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true"></a></span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true"></a><span class="co"># some testing</span></span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true"></a></span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true"></a><span class="co"># should be 0</span></span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true"></a>(<span class="kw">angle</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true"></a><span class="co">## [1] 0.000001207418</span></span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true"></a></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true"></a><span class="co"># should be 0</span></span>
<span id="cb46-20"><a href="#cb46-20" aria-hidden="true"></a>(<span class="kw">angle</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>), <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)))</span>
<span id="cb46-21"><a href="#cb46-21" aria-hidden="true"></a><span class="co">## [1] 0</span></span>
<span id="cb46-22"><a href="#cb46-22" aria-hidden="true"></a></span>
<span id="cb46-23"><a href="#cb46-23" aria-hidden="true"></a><span class="co"># should be 45</span></span>
<span id="cb46-24"><a href="#cb46-24" aria-hidden="true"></a>(<span class="kw">angle</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb46-25"><a href="#cb46-25" aria-hidden="true"></a><span class="co">## [1] 45</span></span>
<span id="cb46-26"><a href="#cb46-26" aria-hidden="true"></a></span>
<span id="cb46-27"><a href="#cb46-27" aria-hidden="true"></a><span class="co"># should be 90</span></span>
<span id="cb46-28"><a href="#cb46-28" aria-hidden="true"></a>(<span class="kw">angle</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)))</span>
<span id="cb46-29"><a href="#cb46-29" aria-hidden="true"></a><span class="co">## [1] 90</span></span></code></pre></div>
</div>
<p>⬇︎ In case you were wondering, the text below belongs to footnotes from somewhere high above.⬇︎</p>
<!--chapter:end:01-R.Rmd-->
</div>
</div>
<div id="simulation-of-random-variables-and-monte-carlo" class="section level1" number="2">
<h1 number="2"><span class="header-section-number">2</span> Simulation of Random Variables and Monte Carlo</h1>
<div style="counter-reset: thechapter 2;">

</div>
<p>In the spirit of “learn by doing”, these lecture notes contain many “Problems”.
The green ones come with solutions and usually introduce new concepts and feature a <em>Comments</em> section right after the solution. These comments are subdivided into <em>R</em> and
<em>Math</em> comments focusing on the computational or conceptual features,
respectively. Note that you are not expected to be able to do the green
problems before reading their solutions and comments, so don’t worry if you
cannot. It is a good practice to try, though. Problems in the Additional Problems section, which are left unsolved,
however, do not require any new ideas and are there to help you practice the
skills presented before.</p>
<div id="simulation-of-some-common-probability-distributions" class="section level2" number="2.1">
<h2 number="2.1"><span class="header-section-number">2.1</span> Simulation of some common probability distributions</h2>
<p>… where we also review some probability along the way.</p>
<div class="example">
<p>‘’Draw’’ 50 simulations from the geometric distribution with parameter <span class="math inline">\(p=0.4\)</span>.</p>
</div>
<p class="solution">
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true"></a><span class="kw">rgeom</span>(<span class="dv">50</span>, <span class="dt">prob =</span> <span class="fl">0.4</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true"></a><span class="co">##  [1] 1 0 3 4 1 2 0 0 2 2 0 1 5 0 1 0 2 1 1 0 2 2 2 1 0 0 1 3 2 2 1 1 1 3 5 0 1 1</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true"></a><span class="co">## [39] 0 0 0 1 2 0 1 1 1 0 1 0</span></span></code></pre></div>
</p>
<div class="comments">
<p><em>R:</em> R makes it very easy to simulate draws from a large class of <em>named
distributions</em><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>,
such as geometric, binomial, uniform, normal, etc. For a list of all available
distributions, run <code>help("distributions")</code> Each available distribution has an <em>R
name</em>; the uniform is <code>unif</code> the normal is <code>norm</code> and the binomial is <code>binom</code>,
etc. If you want to simulate <span class="math inline">\(n\)</span> draws (aka a <em>sample</em> of size <span class="math inline">\(n\)</span>) from a
distribution, you form a full command by appending the letter <code>r</code> to its R name
and use <span class="math inline">\(n\)</span> as an argument. That is how we arrived to <code>rgeom(50)</code> in the
solution above. The additional arguments of the function <code>rgeom</code> have to do with
the parameters of that distribution. Which parameters go with which
distributions, and how to input them as arguments to <code>rgeom</code> or <code>rnorm</code> is best
looked up in R’s extensive documentation. Try <code>help("rnorm")</code>, for example.</p>
<p><em>Math:</em>
You could spend your whole life trying to understand what it really means to
“simulate” or “generate” a random number. The numbers you obtain from so-called
<em>random number generators</em> (RNG) are never random. In fact, they are completely
deterministically generated. Still, sequences of numbers obtained from (good)
random number generators share so many properties with sequences of mythical
<em>truly</em> random numbers, that we can use them as if they were truly random. For
the purposes of this class, you can assume that the numbers R gives you as
<em>random</em> are random enough. Random number generation is a fascinating topic at
the intersection of number theory, probability, statistics, computer science and
even philosophy, but we do not have the time to cover any of it in this class.
If you want to read a story about a particularly bad random number generator, go
<a href="https://en.wikipedia.org/wiki/RANDU">here</a>.</p>
<p>You might have encountered a geometric distribution before. A random variable with that
distribution can take any positive integer value or <span class="math inline">\(0\)</span>, i.e., its support is
<span class="math inline">\({\mathbb N}_0=\{0,1,2,3,\dots\}\)</span>.
As you can see from the output above, the value <span class="math inline">\(0\)</span> appears more often than the value <span class="math inline">\(3\)</span>,
and the value <span class="math inline">\(23\)</span> does not appear at all in this particular simulation run. The
probability of seeing the value <span class="math inline">\(k\in \{0,1,2,3,\dots\}\)</span> as a result of a single
draw is given by <span class="math inline">\((1-p)^k p\)</span>, where <span class="math inline">\(p\)</span> is called the <em>parameter</em> of the distribution.</p>
That corresponds to the following interpretation of the geometric distribution:
keep tossing a biased coin (with probability p of obtaining H) until you see the first H; the number Ts before that is that value your geometric random variable<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>
If we put these probabilities in a single table (and choose <span class="math inline">\(p=0.4\)</span>, for example) it is
going to look like this:
<table>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
0
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
5
</th>
<th style="text-align:right;">
6
</th>
<th style="text-align:right;">
7
</th>
<th style="text-align:left;">
…
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Prob.
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
0.24
</td>
<td style="text-align:right;">
0.144
</td>
<td style="text-align:right;">
0.086
</td>
<td style="text-align:right;">
0.052
</td>
<td style="text-align:right;">
0.031
</td>
<td style="text-align:right;">
0.019
</td>
<td style="text-align:right;">
0.011
</td>
<td style="text-align:left;">
…
</td>
</tr>
</tbody>
</table>
<p>Of course, the possible values our random variable can take do not stop at <span class="math inline">\(7\)</span>.
In fact, there are infinitely many possible values, but we do not have infinite
space. Note that even though the value <span class="math inline">\(23\)</span> does not appear in the output of the
command <code>rgeom</code> above, it probably would if we simulated many more than <span class="math inline">\(50\)</span>
values. Let’s try it with <span class="math inline">\(500\)</span> draws - the table below counts how many <span class="math inline">\(0s\)</span>,
<span class="math inline">\(1s\)</span>, <span class="math inline">\(2s\)</span>, etc. we got:</p>
<table>
<thead>
<tr>
<th style="text-align:right;">
0
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
5
</th>
<th style="text-align:right;">
6
</th>
<th style="text-align:right;">
7
</th>
<th style="text-align:right;">
8
</th>
<th style="text-align:right;">
9
</th>
<th style="text-align:right;">
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
208
</td>
<td style="text-align:right;">
132
</td>
<td style="text-align:right;">
62
</td>
<td style="text-align:right;">
43
</td>
<td style="text-align:right;">
23
</td>
<td style="text-align:right;">
16
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2
</td>
</tr>
</tbody>
</table>
<p>Still no luck, but we do observe values above 5 more often. By trial and error,
we arrive at about <span class="math inline">\(1,000,000\)</span> as the required number of simulations:</p>
<table>
<thead>
<tr>
<th style="text-align:right;">
0
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:left;">
…
</th>
<th style="text-align:right;">
23
</th>
<th style="text-align:right;">
24
</th>
<th style="text-align:right;">
25
</th>
<th style="text-align:right;">
26
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
400616
</td>
<td style="text-align:right;">
238946
</td>
<td style="text-align:right;">
144274
</td>
<td style="text-align:right;">
86489
</td>
<td style="text-align:left;">
…
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:right;">
3
</td>
</tr>
</tbody>
</table>
</div>
<div class="example">
<p>Compute the probability that among <span class="math inline">\(1,000,000\)</span> draws of a geometric random
variable with parameter <span class="math inline">\(p=0.4\)</span>, we never see a number greater than <span class="math inline">\(22\)</span>.</p>
</div>
<p class="solution">
<p>First, we compute the probability that the value seen in a <em>single</em> draw does not exceed <span class="math inline">\(22\)</span>:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true"></a><span class="kw">pgeom</span>(<span class="dv">22</span>, <span class="dt">prob =</span> <span class="fl">0.4</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true"></a><span class="co">## [1] 0.9999921</span></span></code></pre></div>
<p>Different draws are <em>independent</em> of each other, so we need to raise this to the power <span class="math inline">\(1,000,000\)</span>.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true"></a>(<span class="kw">pgeom</span>(<span class="dv">22</span>, <span class="dt">prob =</span> <span class="fl">0.4</span>))<span class="op">^</span>(<span class="fl">1e+06</span>)</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true"></a><span class="co">## [1] 0.0003717335</span></span></code></pre></div>
</p>
<div class="comments">
<p><em>R.</em> The command we used here is <code>pgeom</code> which is a cousin of <code>rgeom</code>. In
general, R commands that involve named probability distributions consist of two
parts. The prefix, i.e., the initial letter (<code>p</code> in this case) stands for the
operation you want to perform, and the rest is the R name of the distribution.
There are 4 prefixes, and the commands they produce are</p>
<table>
<colgroup>
<col width="18%" />
<col width="81%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Prefix</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><code>r</code></td>
<td align="left">Simulate <strong>r</strong>andom draws from the distribution.</td>
</tr>
<tr class="even">
<td align="left"><code>p</code></td>
<td align="left">Compute the cumulative <strong>p</strong>robability distribution function (cdf) (<strong>NOT pdf</strong>)</td>
</tr>
<tr class="odd">
<td align="left"><code>d</code></td>
<td align="left">Compute the probability <strong>d</strong>ensity (pdf) or the probability mass function (pmf)</td>
</tr>
<tr class="even">
<td align="left"><code>q</code></td>
<td align="left">Compute the <strong>q</strong>uantile function</td>
</tr>
</tbody>
</table>
<p>(see the Math section below for the reminder of what these things are). In this
problem, we are dealing with a geometric random variable <span class="math inline">\(X\)</span>, which has a
discrete distribution with support <span class="math inline">\(0,1,2,3,\dots\)</span>. Therefore, the R name is
<code>geom</code>. We are interested in the probability <span class="math inline">\({\mathbb{P}}[ X\leq 22]\)</span>, which
corresponds to the cdf of <span class="math inline">\(X\)</span> at <span class="math inline">\(x=22\)</span>, so we use the
the prefix <code>p</code>. Finally, we used the named parameter <code>p</code> and gave it the value <code>p = 0.4</code>, because the geometric distribution has a single parameter <span class="math inline">\(p\)</span>.</p>
<p>This problem also gives us a chance to discuss precision. As you can see, the
probability of a single draw not exceeding <span class="math inline">\(22\)</span> is very close to <span class="math inline">\(1\)</span>. In fact,
it is equal to it to 5 decimal places. By default, R displays 7 significant
digits of a number. That is enough for most applications (and barely enough for
this one), but sometimes we need more. For example, let’s try to compute the
probability of seeing no T (tails) in 10 tosses of a biased coin, where the
probability of H (heads) is 0.9.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.1</span><span class="op">^</span><span class="dv">10</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true"></a><span class="co">## [1] 1</span></span></code></pre></div>
<p>While very close to it, this probability is clearly not equal to <span class="math inline">\(1\)</span>, as suggested by the output above.
The culprit is the default precision. We can increase the precision (up to <span class="math inline">\(22\)</span> digits) using the <code>options</code> command</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true"></a><span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">17</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true"></a><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.1</span><span class="op">^</span><span class="dv">10</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true"></a><span class="co">## [1] 0.99999999989999999</span></span></code></pre></div>
<p>Precision issues like this one should not appear in this course, but they will
out there “in the wild”, so it might be a good idea to be aware of them.</p>
<p><em>Math.</em> If you forgot all about pdfs, cdfs and such things here is a little reminder:</p>
<table>
<colgroup>
<col width="13%" />
<col width="86%" />
</colgroup>
<tbody>
<tr class="odd">
<td>cdf</td>
<td><span class="math inline">\(F(x) = {\mathbb{P}}[X\leq x]\)</span></td>
</tr>
<tr class="even">
<td>pdf</td>
<td><span class="math inline">\(f(x)\)</span> such that <span class="math inline">\({\mathbb{P}}[X \in [a,b]] = \int_a^b f(x) \, dx\)</span> for all <span class="math inline">\(a&lt;b\)</span></td>
</tr>
<tr class="odd">
<td>pmf</td>
<td><span class="math inline">\(p(x)\)</span> such that <span class="math inline">\({\mathbb{P}}[X=a_n] = p(a_n)\)</span> for some sequence <span class="math inline">\(a_n\)</span></td>
</tr>
<tr class="even">
<td>qf</td>
<td><span class="math inline">\(q(p)\)</span> is a number such that <span class="math inline">\({\mathbb{P}}[ X \leq q(p)] = p\)</span></td>
</tr>
</tbody>
</table>
<p>Those random variables that admit a pdf are called <strong>continuous</strong>. The prime
examples are the normal, or the exponential distribution. The ones where a pmf
exists are called <strong>discrete</strong>. The sequence <span class="math inline">\(a_n\)</span> covers all values that such
a, discrete, random variable can take. Most often, <span class="math inline">\(a_n\)</span> either covers the set
of all natural numbers <span class="math inline">\(0,1,2,\dots\)</span> or a finite subset such as <span class="math inline">\(1,2,3,4,5,6\)</span>.</p>
<p>Coming back to our original problem, we note that the probability we obtained is
quite small. Since <span class="math inline">\(1/0.000372\)</span> is about <span class="math inline">\(2690\)</span>, we would have to run about
<span class="math inline">\(2690\)</span> rounds of <span class="math inline">\(1,000,000\)</span> simulations before the largest number falls below
<span class="math inline">\(23\)</span>.</p>
</div>
<div class="example">
<p>Compute the <span class="math inline">\(0.05\)</span>, <span class="math inline">\(0.1\)</span>, <span class="math inline">\(0.4\)</span>, <span class="math inline">\(0.6\)</span> and <span class="math inline">\(0.95\)</span> quantiles of the normal
distribution with mean <span class="math inline">\(1\)</span> and standard deviation <span class="math inline">\(2\)</span>.</p>
</div>
<p class="solution">
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true"></a><span class="kw">qnorm</span>(<span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.95</span>), <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true"></a><span class="co">## [1] -2.2897073 -1.5631031  0.4933058  1.5066942  4.2897073</span></span></code></pre></div>
</p>
<div class="comments">
<p><em>R.</em> The function we used is <code>qnorm</code>, with the prefix <code>q</code> which computes the
quantile function and the R name <code>norm</code> because we are looking for the quantiles
of the normal distribution. The additional (named) parameters are where the
parameters of the distribution come in (the mean and the standard variation) in
this case. Note how we plugged in the entire vector
<code>c(0.05, 0.1, 0.4, 0.6, 0.98)</code> instead of a single value into <code>qnorm</code>. You can
do that because this function is <strong>vectorized</strong>. That means that if you give it
a vector as an argument, it will “apply itself” to each component of the vector
separately, and return the vector of results. Many (but not all) functions in R
are vectorized<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>.</p>
<p>As a sanity check, let’s apply <code>pnrom</code> (which computes the cdf of the normal) to these quantile values:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true"></a>p =<span class="st"> </span><span class="kw">qnorm</span>(<span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.95</span>), <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true"></a><span class="kw">pnorm</span>(p, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true"></a><span class="co">## [1] 0.05 0.10 0.40 0.60 0.95</span></span></code></pre></div>
<p>As expected, we got the original values back - the normal quantile function and its cdf are inverses of each other.</p>
<p><em>Math.</em> Computing the cdf of a standard normal is the same thing reading a
<em>normal table</em>. Computing a quantile is the opposite; you go into the middle of
the table and find your value, and then figure out which “Z” would give you that
value.</p>
</div>
<div class="example">
<p>Simulate <span class="math inline">\(60\)</span> throws of a fair <span class="math inline">\(10\)</span>-sided die.</p>
</div>
<p class="solution">
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true"></a><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">60</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true"></a><span class="co">##  [1]  2  8  9  8  4  7  7  7  2  3  3 10  6  1  9  7  4  7  6  2  2  3 10  1  9</span></span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true"></a><span class="co">## [26]  7  3  2  8  4  1  2  8  1  4  9  1  9 10 10  6  1  8  6  1 10  5  1  6  9</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true"></a><span class="co">## [51]  8  3  8  9  4  6  1  6  7  8</span></span></code></pre></div>
</p>
<div class="comments">
<p><em>Math.</em> Let <span class="math inline">\(X\)</span> denote the outcome of a single throw of a fair <span class="math inline">\(10\)</span>-sided die.
The distribution of <span class="math inline">\(X\)</span> is discrete (it can only take the values
<span class="math inline">\(1,2,\dots, 10\)</span>) but it is not one of the more famous named distributions. I
guess you could call it a <em>discrete uniform on <span class="math inline">\({1,2,\dots, 10}\)</span></em>, but a better
way to describe such distribution is by a <strong>distribution table</strong>, which is
really just a list of possible values a random variable can take, together with
their, respective, probabilities. In this case,</p>
<table>
<thead>
<tr>
<th style="text-align:left;">
1
</th>
<th style="text-align:left;">
2
</th>
<th style="text-align:left;">
3
</th>
<th style="text-align:left;">
4
</th>
<th style="text-align:left;">
5
</th>
<th style="text-align:left;">
6
</th>
<th style="text-align:left;">
7
</th>
<th style="text-align:left;">
8
</th>
<th style="text-align:left;">
9
</th>
<th style="text-align:left;">
10
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
<td style="text-align:left;">
0.1
</td>
</tr>
</tbody>
</table>
<p><em>R.</em> The command used to draw a sample from a (finite) collection is, of, course
<code>sample</code>. The first argument is a vector, and it plays the role of the “bag”
from which you are drawing. If we are interested in repeated, random samples, we
also need to specify <code>replace = FALSE</code> otherwise, you could draw any single
number at most once:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true"></a><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">8</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true"></a><span class="co">## [1]  1  5  6  7  8 10  3  4</span></span></code></pre></div>
<p>With more than 10 draws, we would run out of numbers to draw:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true"></a><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">12</span>, <span class="dt">replace =</span> <span class="ot">FALSE</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true"></a><span class="co">## Error in sample.int(length(x), size, replace, prob): cannot take a sample larger than the population when &#39;replace = FALSE&#39;</span></span></code></pre></div>
<p>The bag you draw from can contain objects other than numbers:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true"></a><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Picard&quot;</span>, <span class="st">&quot;Data&quot;</span>, <span class="st">&quot;Geordi&quot;</span>), <span class="dv">9</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true"></a><span class="co">## [1] &quot;Picard&quot; &quot;Data&quot;   &quot;Geordi&quot; &quot;Geordi&quot; &quot;Data&quot;   &quot;Data&quot;   &quot;Picard&quot; &quot;Data&quot;  </span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true"></a><span class="co">## [9] &quot;Geordi&quot;</span></span></code></pre></div>
So far, each object in the bag had the same probability of being drawn. You can
use the <code>sample</code> command to produce a <em>weighted</em> sample, too. For example, if we
wanted to simulate <span class="math inline">\(10\)</span> draws from the following distribution
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
0.1
</td>
</tr>
</tbody>
</table>
<p>we would use the additional argument <code>prob</code>:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true"></a><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="dv">10</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.7</span>, <span class="fl">0.1</span>))</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true"></a><span class="co">##  [1] 1 2 2 1 1 2 2 3 2 2</span></span></code></pre></div>
<p>Note how it is mostly <span class="math inline">\(2\)</span>s, as expected.</p>
</div>
<div class="example">
<p>Draw a sample of size <span class="math inline">\(n=10\)</span> from <span class="math inline">\(N(1,2)\)</span>, i.e., from the normal distribution
with parameters <span class="math inline">\(\mu=1\)</span>, <span class="math inline">\(\sigma = 2\)</span>. Plot a histogram of the obtained
values. Repeat for <span class="math inline">\(n=100\)</span> and <span class="math inline">\(n=100000\)</span>.</p>
</div>
<p class="solution">
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true"></a><span class="kw">hist</span>(x)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-69-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true"></a><span class="kw">hist</span>(x)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-70-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true"></a><span class="kw">hist</span>(x)</span></code></pre></div>
<img src="_main_files/figure-html/unnamed-chunk-71-1.png" width="672" style="display: block; margin: auto;" />
</p>
<div class="comments">
<p><em>R.</em> It cannot be simpler! You use the command <code>hist</code>, feed it a vector of
values, and it produces a histogram. It will even label the axes for you. If you
want to learn how to tweak various features of your histogram, type <code>?hist</code>.</p>
<!-- Esthetically, the built-in histograms leave something to be desired. We can do better, using the package `ggplot2`. You don't have to use it in this class, but if you want to, you install it first by running `install.packages("ggplot2")` (you have to do this only once). Then, every time you want to use it, you run `library(ggplot2)` to notify R that you are about to use a function from that package. It would take a whole semester to learn everything there is to know about `ggplot2`; I will only show what a histogram looks like in it: -->
<!-- ```{r} -->
<!-- library(ggplot2) -->
<!-- x = rnorm(100000, mean = 1, sd = 2) -->
<!-- qplot(x, bins=40) -->
<!-- ``` -->
<p><em>Math</em>. Mathematically, histogram can be produced for any (finite) sequence of
numbers: we divide the range into several bins, count how many of the points in
the sequence falls into each bin, and then draw a bar above that bin whose
height is equal (or proportional to) that count. The picture tells use about how
the sequence we started from is “distributed”. The order of the points does not
matter - you would get exactly the same picture if you sorted the points first.
If the sequence of points you draw the histogram of comes from, say, normal
distribution, the histogram will resemble the shape of the pdf of a normal
distribution. I say resemble, because its shape is ultimately random. If the
number of points is small (like in the second part of this problem) the
histogram may look nothing like the normal pdf. However, when the number of
points gets larger and larger, the shape of the histogram gets closer and closer
to the underlying pdf (if it exists). I keep writing “shape” because the three
histograms above have very different scales on the <span class="math inline">\(y\)</span> axis. That is because we
used counts to set the vertical sizes of bins.
A more natural choice is to use the proportions, i.e. relative frequencies (i.e.
counts divided by the total number of points) for bar heights. More precisely, the bar height <span class="math inline">\(h\)</span> over the bin <span class="math inline">\([a,b]\)</span> is chosen so that the area of the bar, i.e., <span class="math inline">\((b-a)\times h\)</span> equals to the proportion of all points that fall inside <span class="math inline">\([a,b]\)</span>. This way, the total area under the histogram is always <span class="math inline">\(1\)</span>. To draw such a <strong>density histogram</strong> in R we would
need to add the additional option <code>freq = FALSE</code> to <code>hist</code>:</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100000</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true"></a><span class="kw">hist</span>(x, <span class="dt">freq =</span> <span class="ot">FALSE</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-72-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Note how the <span class="math inline">\(y\)</span>-axes label changed from “Frequency” to “Density”.
With such a normalization, the histogram of <span class="math inline">\(x\)</span> can be directly compared to the
probability density of a normal distribution. Here is a histogram of <span class="math inline">\(100,000\)</span>
simulations from our normal distribution with its density function (pdf)
superimposed; I am leaving the code in case you are interested:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true"></a>sims =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10000</span>, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">6</span>, <span class="dv">8</span>, <span class="dt">by =</span> <span class="fl">0.02</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">dnorm</span>(x, <span class="dt">mean =</span> <span class="dv">1</span>, <span class="dt">sd =</span> <span class="dv">2</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true"></a><span class="kw">hist</span>(sims, <span class="dt">freq =</span> <span class="ot">FALSE</span>, <span class="dt">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true"></a><span class="kw">points</span>(x, y, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">lwd =</span> <span class="dv">3</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-73-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="multivariate-distributions" class="section level2" number="2.2">
<h2 number="2.2"><span class="header-section-number">2.2</span> Multivariate Distributions</h2>
<div class="example">
<p>Let <code>x</code> contain <span class="math inline">\(2,000\)</span> draws from <span class="math inline">\(N(0,1)\)</span>, <code>z</code> another <span class="math inline">\(2,000\)</span> draws from <span class="math inline">\(N(0,1)\)</span> and let <code>y=x^2+z</code>.</p>
<ol style="list-style-type: decimal">
<li><p>Draw a scatterplot of <code>x</code> and <code>y</code> to visualize the joint distribution of <code>x</code> and <code>y</code></p></li>
<li><p>Plot two histograms, one of <code>x</code> and one of <code>y</code>. Do they tell the whole story about the joint distribution of <code>x</code> and <code>y</code>?</p></li>
<li><p>Are <code>x</code> and <code>y</code> correlated? Do <code>x</code> and <code>y</code> in your plot “look independent”? Use the permutation test to check of independence between <code>x</code> and <code>y</code>.</p></li>
</ol>
</div>
<p class="solution">
<p><part> 1. </part></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">2000</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true"></a>z =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">2000</span>)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true"></a>y =<span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>z</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true"></a><span class="kw">plot</span>(x, y)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-74-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><part> 2. </part></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true"></a><span class="kw">hist</span>(x)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true"></a><span class="kw">hist</span>(y)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-75-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>No, the two histograms would not be enough to describe the joint distribution.
There are many ways in which two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be jointly
distributed, but whose separate (marginal) distributions match the histograms
above. To give a very simple example, let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be discrete random
variables, each of which can only take values <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. Consider the following
two possible <em>joint</em> distribution tables for the random pair <span class="math inline">\((X,Y)\)</span>:</p>
<table class="kable_wrapper">
<tbody>
<tr>
<td>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;text-align: center;">
</th>
<th style="text-align:right;text-align: center;">
0
</th>
<th style="text-align:right;text-align: center;">
1
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
0
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
0.25
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
1
</td>
<td style="text-align:right;">
0.25
</td>
<td style="text-align:right;">
0.25
</td>
</tr>
</tbody>
</table>
</td>
<td>
<table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;text-align: center;">
</th>
<th style="text-align:right;text-align: center;">
0
</th>
<th style="text-align:right;text-align: center;">
1
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
0
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.0
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
1
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.5
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p>In both cases, the marginals are the same, i.e., both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are equally
likely to take the value <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, i.e., they both have the Bernoulli
distribution with parameter <span class="math inline">\(p=1/2\)</span>. That would correspond to the separate
histograms to be the same. On the other hand, their joint distributions (aka
dependence structures) are completely different. In the first (left) case, <span class="math inline">\(X\)</span>
and <span class="math inline">\(Y\)</span> are independent, but in the second they are completely dependent.</p>
<p><part> 3. </part></p>
<p>They are probably not correlated since the sample correlation between <code>x</code>
and <code>y</code> is close to <span class="math inline">\(0\)</span>:</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true"></a>(<span class="kw">cor</span>(x, y))</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true"></a><span class="co">## [1] -0.02880239</span></span></code></pre></div>
<p>but they do not look independent.</p>
<p>To apply the permutation test, we first plot the scatterplot of <code>x</code> vs. <code>y</code> as
above. Then, we replace <code>y</code> by a vector with the same components, but randomly
permute their positions, and then plot a scatterplot again. We repeat this three
times:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true"></a>y_perm_<span class="dv">1</span> =<span class="st"> </span><span class="kw">sample</span>(y)</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true"></a>y_perm_<span class="dv">2</span> =<span class="st"> </span><span class="kw">sample</span>(y)</span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true"></a>y_perm_<span class="dv">3</span> =<span class="st"> </span><span class="kw">sample</span>(y)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true"></a><span class="kw">plot</span>(x, y)</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true"></a><span class="kw">plot</span>(x, y_perm_<span class="dv">1</span>)</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true"></a><span class="kw">plot</span>(x, y_perm_<span class="dv">2</span>)</span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true"></a><span class="kw">plot</span>(x, y_perm_<span class="dv">3</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-78-1.png" width="672" style="display: block; margin: auto;" /></p>
The conclusion is clear, the first (upper-left) plot is very different than the
other three. Therefore, <code>x</code> and <code>y</code> are probably not independent.
</p>
<div class="comments">
<p><em>Math.</em> The point of this problem is to review the notion of the <strong>joint
distribution</strong> between two random variables. The most important point here is
that there is more to the joint distribution of two random vectors, than just
the two distributions taken separately. In a sense, the whole is (much) more
than the sum of its parts. This is something that does not happen in the
deterministic world. If you give me the <span class="math inline">\(x\)</span>-coordinate of a point, and,
separately, its <span class="math inline">\(y\)</span>-coordinate, I will be able to pinpoint the exact location of
that point.</p>
<p>On the other hand, suppose that the <span class="math inline">\(x\)</span>-coordinate of a point is unknown, so we
treat it as a random variable, and suppose that this variable admits the
standard normal distribution. Do the same for <span class="math inline">\(y\)</span>. Even with this information,
you cannot say anything about the position of the point <span class="math inline">\((x,y)\)</span>. It could be
that the reason we are uncertain about <span class="math inline">\(x\)</span> and the reason we are uncertain about
<span class="math inline">\(y\)</span> have nothing to do with each other; in that case we would be right to assume
that <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent. If, on the other hand, we got the values of
both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> by measuring them using the same, inaccurate, tape measure, we
cannot assume that the errors are independent. It is more likely that both <span class="math inline">\(x\)</span>
and <span class="math inline">\(y\)</span> are too big, or both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are too small.</p>
<p>Mathematically, we say that random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if
<span class="math display">\[{\mathbb{P}}[X \in [a,b]] \times {\mathbb{P}}[ Y \in [c,d] ] = {\mathbb{P}}[ X\in [a,b] \text{ and } Y\in [c,d]]\text{ for all } a,b,c,d.\]</span>
While up to the point,
this definition is not very eye-opening, or directly applicable in most cases.
Intuitively, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if the distribution of <span class="math inline">\(Y\)</span> would not
change if we received additional information about <span class="math inline">\(X\)</span>. In our problem, random
variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> correspond to vectors <code>x</code> and <code>y</code>. Their scatterplot above
clearly conveys the following message: when <code>x</code> is around <span class="math inline">\(-2\)</span>, we expect <code>y</code> to
be around <code>4</code>, while when <code>x</code> is around <span class="math inline">\(0\)</span>, <code>y</code> would be expected to be around
<span class="math inline">\(0\)</span>, too.</p>
<p>Sometimes, it is not so easy to decide whether two variables are independent by staring at a scatterplot. What would you say about the scatterplot below?
<img src="_main_files/figure-html/unnamed-chunk-79-1.png" width="50%" style="display: block; margin: auto;" />
The <strong>permutation test</strong> is designed to help you decide when two (simulated)
random variables are likely to be independent. The idea is simple. Suppose that
<code>x</code> and <code>y</code> are simulations from two independent (not necessarily identical)
distributions; say <code>x=runif(1000)</code> and <code>y=rnorm(1000)</code>. The vector
<code>y_perm=sample(y)</code> is a randomly permuted version of <code>y</code> (see R section below)
and it contains exactly the same information about the distribution of <code>y</code> as
<code>y</code> itself does. Both <code>y</code> and <code>y_perm</code> will produce exactly the same histogram.
Permuting <code>y</code>, however, “uncouples” it from <code>x</code>. If there was any dependence
between the values of <code>x</code> and <code>y</code> before, there certainly isn’t any now. In
other the joint distribution of <code>x</code> and <code>y_perm</code> has the same marginals as the
joint distribution of <code>x</code> and <code>y</code>, but all the (possible) dependence has been
removed. What remains is to compare the scatterplot between <code>x</code> and <code>y</code> and the
scatterplot between <code>x</code> and <code>y_perm</code>. If they look about the same, we conclude
that <code>x</code> and <code>y</code> are independent. Otherwise, there is some dependence between
them.</p>
<p>One question remains: why did we have to draw three scatterplots of permuted
versions of <code>y</code>? That is because we have only finitely many data points, and it
can happen, by pure chance, that the permutation we applied to <code>y</code> does not
completely scramble its dependence on <code>x</code>. With a “sample” of three such plots,
we
get a better feeling for the inherent randomness in this permutation procedure,
and it is much easier to tell whether “one of these things is not like the
others”. Btw, the random variables in the scatterplot above are, indeed,
independent; here are the <span class="math inline">\(4\)</span> permutation-test plots to “prove” it:
<img src="_main_files/figure-html/unnamed-chunk-80-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Unlike univariate (one-variable) distributions which are visualized using
histograms or similar plots, multivariate (several-variable) distributions are
harder to depict. The most direct relative of the histogram is a <strong>3d
histogram</strong>. Just like the <span class="math inline">\(x\)</span>-axis is divided into bins in the univariate case,
in the multivariate case we divide the <span class="math inline">\(xy\)</span>-plane into regions (squares, e.g.)
and count the number of points falling into each of these regions. After that
a 3d bar (a skyscraper) is drawn above each square with the height of each
skyscraper equal (or proportional) to the number of points which fall into its
base. Here is a 3d histogram of our original pair (<code>x</code>,<code>y</code>) from the problem.
You should be able to rotate and zoom it right here in the notes, provided your
browser has JavaScript enabled:</p>
<div align="center">
<html>
<head><meta charset="utf-8" /></head>
<body>
    <div>
        
                <script type="text/javascript">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>
        <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>    
            <div id="98c4b07f-5cfa-4b01-bebc-368a7a67fb26" class="plotly-graph-div" style="height:700px; width:650px;"></div>
            <script type="text/javascript">
                
                    window.PLOTLYENV=window.PLOTLYENV || {};
                    
                if (document.getElementById("98c4b07f-5cfa-4b01-bebc-368a7a67fb26")) {
                    Plotly.newPlot(
                        '98c4b07f-5cfa-4b01-bebc-368a7a67fb26',
                        [{"color": "#cc5500", "flatshading": true, "i": [256, 274, 256, 273, 256, 259, 273, 274, 258, 275, 259, 273, 288, 306, 288, 305, 288, 291, 305, 306, 290, 307, 291, 305, 320, 334, 320, 333, 320, 323, 333, 334, 322, 335, 323, 333, 344, 354, 344, 353, 344, 347, 353, 354, 346, 355, 347, 353, 168, 198, 168, 197, 168, 171, 197, 198, 170, 199, 171, 197, 224, 242, 224, 241, 224, 227, 241, 242, 226, 243, 227, 241, 260, 278, 260, 277, 260, 263, 277, 278, 262, 279, 263, 277, 292, 310, 292, 309, 292, 295, 309, 310, 294, 311, 295, 309, 32, 54, 32, 53, 32, 35, 53, 54, 34, 55, 35, 53, 72, 98, 72, 97, 72, 75, 97, 98, 74, 99, 75, 97, 120, 146, 120, 145, 120, 123, 145, 146, 122, 147, 123, 145, 172, 202, 172, 201, 172, 175, 201, 202, 174, 203, 175, 201, 228, 246, 228, 245, 228, 231, 245, 246, 230, 247, 231, 245, 0, 18, 0, 17, 0, 3, 17, 18, 2, 19, 3, 17, 36, 58, 36, 57, 36, 39, 57, 58, 38, 59, 39, 57, 76, 102, 76, 101, 76, 79, 101, 102, 78, 103, 79, 101, 124, 150, 124, 149, 124, 127, 149, 150, 126, 151, 127, 149, 176, 206, 176, 205, 176, 179, 205, 206, 178, 207, 179, 205, 4, 22, 4, 21, 4, 7, 21, 22, 6, 23, 7, 21, 40, 62, 40, 61, 40, 43, 61, 62, 42, 63, 43, 61, 80, 106, 80, 105, 80, 83, 105, 106, 82, 107, 83, 105, 128, 154, 128, 153, 128, 131, 153, 154, 130, 155, 131, 153, 8, 26, 8, 25, 8, 11, 25, 26, 10, 27, 11, 25, 44, 66, 44, 65, 44, 47, 65, 66, 46, 67, 47, 65, 84, 110, 84, 109, 84, 87, 109, 110, 86, 111, 87, 109, 132, 158, 132, 157, 132, 135, 157, 158, 134, 159, 135, 157, 180, 210, 180, 209, 180, 183, 209, 210, 182, 211, 183, 209, 12, 30, 12, 29, 12, 15, 29, 30, 14, 31, 15, 29, 48, 70, 48, 69, 48, 51, 69, 70, 50, 71, 51, 69, 88, 114, 88, 113, 88, 91, 113, 114, 90, 115, 91, 113, 136, 162, 136, 161, 136, 139, 161, 162, 138, 163, 139, 161, 184, 214, 184, 213, 184, 187, 213, 214, 186, 215, 187, 213, 92, 118, 92, 117, 92, 95, 117, 118, 94, 119, 95, 117, 140, 166, 140, 165, 140, 143, 165, 166, 142, 167, 143, 165, 188, 218, 188, 217, 188, 191, 217, 218, 190, 219, 191, 217, 232, 250, 232, 249, 232, 235, 249, 250, 234, 251, 235, 249, 264, 282, 264, 281, 264, 267, 281, 282, 266, 283, 267, 281, 192, 222, 192, 221, 192, 195, 221, 222, 194, 223, 195, 221, 236, 254, 236, 253, 236, 239, 253, 254, 238, 255, 239, 253, 268, 286, 268, 285, 268, 271, 285, 286, 270, 287, 271, 285, 296, 314, 296, 313, 296, 299, 313, 314, 298, 315, 299, 313, 324, 338, 324, 337, 324, 327, 337, 338, 326, 339, 327, 337, 300, 318, 300, 317, 300, 303, 317, 318, 302, 319, 303, 317, 328, 342, 328, 341, 328, 331, 341, 342, 330, 343, 331, 341, 348, 358, 348, 357, 348, 351, 357, 358, 350, 359, 351, 357], "j": [272, 258, 257, 272, 258, 257, 272, 275, 259, 274, 257, 275, 304, 290, 289, 304, 290, 289, 304, 307, 291, 306, 289, 307, 332, 322, 321, 332, 322, 321, 332, 335, 323, 334, 321, 335, 352, 346, 345, 352, 346, 345, 352, 355, 347, 354, 345, 355, 196, 170, 169, 196, 170, 169, 196, 199, 171, 198, 169, 199, 240, 226, 225, 240, 226, 225, 240, 243, 227, 242, 225, 243, 276, 262, 261, 276, 262, 261, 276, 279, 263, 278, 261, 279, 308, 294, 293, 308, 294, 293, 308, 311, 295, 310, 293, 311, 52, 34, 33, 52, 34, 33, 52, 55, 35, 54, 33, 55, 96, 74, 73, 96, 74, 73, 96, 99, 75, 98, 73, 99, 144, 122, 121, 144, 122, 121, 144, 147, 123, 146, 121, 147, 200, 174, 173, 200, 174, 173, 200, 203, 175, 202, 173, 203, 244, 230, 229, 244, 230, 229, 244, 247, 231, 246, 229, 247, 16, 2, 1, 16, 2, 1, 16, 19, 3, 18, 1, 19, 56, 38, 37, 56, 38, 37, 56, 59, 39, 58, 37, 59, 100, 78, 77, 100, 78, 77, 100, 103, 79, 102, 77, 103, 148, 126, 125, 148, 126, 125, 148, 151, 127, 150, 125, 151, 204, 178, 177, 204, 178, 177, 204, 207, 179, 206, 177, 207, 20, 6, 5, 20, 6, 5, 20, 23, 7, 22, 5, 23, 60, 42, 41, 60, 42, 41, 60, 63, 43, 62, 41, 63, 104, 82, 81, 104, 82, 81, 104, 107, 83, 106, 81, 107, 152, 130, 129, 152, 130, 129, 152, 155, 131, 154, 129, 155, 24, 10, 9, 24, 10, 9, 24, 27, 11, 26, 9, 27, 64, 46, 45, 64, 46, 45, 64, 67, 47, 66, 45, 67, 108, 86, 85, 108, 86, 85, 108, 111, 87, 110, 85, 111, 156, 134, 133, 156, 134, 133, 156, 159, 135, 158, 133, 159, 208, 182, 181, 208, 182, 181, 208, 211, 183, 210, 181, 211, 28, 14, 13, 28, 14, 13, 28, 31, 15, 30, 13, 31, 68, 50, 49, 68, 50, 49, 68, 71, 51, 70, 49, 71, 112, 90, 89, 112, 90, 89, 112, 115, 91, 114, 89, 115, 160, 138, 137, 160, 138, 137, 160, 163, 139, 162, 137, 163, 212, 186, 185, 212, 186, 185, 212, 215, 187, 214, 185, 215, 116, 94, 93, 116, 94, 93, 116, 119, 95, 118, 93, 119, 164, 142, 141, 164, 142, 141, 164, 167, 143, 166, 141, 167, 216, 190, 189, 216, 190, 189, 216, 219, 191, 218, 189, 219, 248, 234, 233, 248, 234, 233, 248, 251, 235, 250, 233, 251, 280, 266, 265, 280, 266, 265, 280, 283, 267, 282, 265, 283, 220, 194, 193, 220, 194, 193, 220, 223, 195, 222, 193, 223, 252, 238, 237, 252, 238, 237, 252, 255, 239, 254, 237, 255, 284, 270, 269, 284, 270, 269, 284, 287, 271, 286, 269, 287, 312, 298, 297, 312, 298, 297, 312, 315, 299, 314, 297, 315, 336, 326, 325, 336, 326, 325, 336, 339, 327, 338, 325, 339, 316, 302, 301, 316, 302, 301, 316, 319, 303, 318, 301, 319, 340, 330, 329, 340, 330, 329, 340, 343, 331, 342, 329, 343, 356, 350, 349, 356, 350, 349, 356, 359, 351, 358, 349, 359], "k": [274, 256, 273, 256, 259, 256, 274, 273, 275, 258, 273, 259, 306, 288, 305, 288, 291, 288, 306, 305, 307, 290, 305, 291, 334, 320, 333, 320, 323, 320, 334, 333, 335, 322, 333, 323, 354, 344, 353, 344, 347, 344, 354, 353, 355, 346, 353, 347, 198, 168, 197, 168, 171, 168, 198, 197, 199, 170, 197, 171, 242, 224, 241, 224, 227, 224, 242, 241, 243, 226, 241, 227, 278, 260, 277, 260, 263, 260, 278, 277, 279, 262, 277, 263, 310, 292, 309, 292, 295, 292, 310, 309, 311, 294, 309, 295, 54, 32, 53, 32, 35, 32, 54, 53, 55, 34, 53, 35, 98, 72, 97, 72, 75, 72, 98, 97, 99, 74, 97, 75, 146, 120, 145, 120, 123, 120, 146, 145, 147, 122, 145, 123, 202, 172, 201, 172, 175, 172, 202, 201, 203, 174, 201, 175, 246, 228, 245, 228, 231, 228, 246, 245, 247, 230, 245, 231, 18, 0, 17, 0, 3, 0, 18, 17, 19, 2, 17, 3, 58, 36, 57, 36, 39, 36, 58, 57, 59, 38, 57, 39, 102, 76, 101, 76, 79, 76, 102, 101, 103, 78, 101, 79, 150, 124, 149, 124, 127, 124, 150, 149, 151, 126, 149, 127, 206, 176, 205, 176, 179, 176, 206, 205, 207, 178, 205, 179, 22, 4, 21, 4, 7, 4, 22, 21, 23, 6, 21, 7, 62, 40, 61, 40, 43, 40, 62, 61, 63, 42, 61, 43, 106, 80, 105, 80, 83, 80, 106, 105, 107, 82, 105, 83, 154, 128, 153, 128, 131, 128, 154, 153, 155, 130, 153, 131, 26, 8, 25, 8, 11, 8, 26, 25, 27, 10, 25, 11, 66, 44, 65, 44, 47, 44, 66, 65, 67, 46, 65, 47, 110, 84, 109, 84, 87, 84, 110, 109, 111, 86, 109, 87, 158, 132, 157, 132, 135, 132, 158, 157, 159, 134, 157, 135, 210, 180, 209, 180, 183, 180, 210, 209, 211, 182, 209, 183, 30, 12, 29, 12, 15, 12, 30, 29, 31, 14, 29, 15, 70, 48, 69, 48, 51, 48, 70, 69, 71, 50, 69, 51, 114, 88, 113, 88, 91, 88, 114, 113, 115, 90, 113, 91, 162, 136, 161, 136, 139, 136, 162, 161, 163, 138, 161, 139, 214, 184, 213, 184, 187, 184, 214, 213, 215, 186, 213, 187, 118, 92, 117, 92, 95, 92, 118, 117, 119, 94, 117, 95, 166, 140, 165, 140, 143, 140, 166, 165, 167, 142, 165, 143, 218, 188, 217, 188, 191, 188, 218, 217, 219, 190, 217, 191, 250, 232, 249, 232, 235, 232, 250, 249, 251, 234, 249, 235, 282, 264, 281, 264, 267, 264, 282, 281, 283, 266, 281, 267, 222, 192, 221, 192, 195, 192, 222, 221, 223, 194, 221, 195, 254, 236, 253, 236, 239, 236, 254, 253, 255, 238, 253, 239, 286, 268, 285, 268, 271, 268, 286, 285, 287, 270, 285, 271, 314, 296, 313, 296, 299, 296, 314, 313, 315, 298, 313, 299, 338, 324, 337, 324, 327, 324, 338, 337, 339, 326, 337, 327, 318, 300, 317, 300, 303, 300, 318, 317, 319, 302, 317, 303, 342, 328, 341, 328, 331, 328, 342, 341, 343, 330, 341, 331, 358, 348, 357, 348, 351, 348, 358, 357, 359, 350, 357, 351], "opacity": 1, "type": "mesh3d", "x": [-4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -4.016320668186614, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.253477083952596, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -3.2034770839525963, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.4406334997185786, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -2.390633499718579, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.6277899154845616, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -1.577789915484562, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.8149463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.7649463312505445, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, -0.002102747016526907, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.04789725298347314, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8107408372174907, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 0.8607408372174898, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6235844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 1.6735844214515074, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.436428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 2.486428005685525, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.249271589919543, 3.2992715899195426, 3.2992715899195426, 3.2992715899195426, 3.2992715899195426, 3.2992715899195426, 3.2992715899195426, 3.2992715899195426, 3.2992715899195426, 4.06211517415356, 4.06211517415356, 4.06211517415356, 4.06211517415356, 4.06211517415356, 4.06211517415356, 4.06211517415356, 4.06211517415356], "y": [1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 3.8549731900120006, 3.8549731900120006, 5.674403403703017, 5.674403403703017, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 3.8549731900120006, 3.8549731900120006, 5.674403403703017, 5.674403403703017, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 3.8549731900120006, 3.8549731900120006, 5.674403403703017, 5.674403403703017, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 3.8549731900120006, 3.8549731900120006, 5.674403403703017, 5.674403403703017, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 3.8549731900120006, 3.8549731900120006, 5.674403403703017, 5.674403403703017, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 3.8549731900120006, 3.8549731900120006, 5.674403403703017, 5.674403403703017, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 3.8549731900120006, 3.8549731900120006, 5.674403403703017, 5.674403403703017, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 3.8549731900120006, 3.8549731900120006, 5.674403403703017, 5.674403403703017, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, -1.7533174510610485, -1.7533174510610485, 0.06611276262996779, 0.06611276262996779, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, -1.7533174510610485, -1.7533174510610485, 0.06611276262996779, 0.06611276262996779, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 1.9855429763209842, 1.9855429763209842, 3.8049731900120003, 3.8049731900120003, 5.7244034037030165, 5.7244034037030165, 7.543833617394033, 7.543833617394033, 7.593833617394033, 7.593833617394033, 9.413263831085048, 9.413263831085048, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, -1.7533174510610485, -1.7533174510610485, 0.06611276262996779, 0.06611276262996779, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, -1.7533174510610485, -1.7533174510610485, 0.06611276262996779, 0.06611276262996779, 0.11611276262996784, 0.11611276262996784, 1.9355429763209842, 1.9355429763209842, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, -3.622747664752065, -3.622747664752065, -1.8033174510610486, -1.8033174510610486, -1.7533174510610485, -1.7533174510610485, 0.06611276262996779, 0.06611276262996779, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, -3.622747664752065, -3.622747664752065, -1.8033174510610486, -1.8033174510610486, -1.7533174510610485, -1.7533174510610485, 0.06611276262996779, 0.06611276262996779, 9.46326383108505, 9.46326383108505, 11.282694044776065, 11.282694044776065, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, -3.622747664752065, -3.622747664752065, -1.8033174510610486, -1.8033174510610486, -1.7533174510610485, -1.7533174510610485, 0.06611276262996779, 0.06611276262996779, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, 13.202124258467084, 13.202124258467084, 15.0215544721581, 15.0215544721581, -3.622747664752065, -3.622747664752065, -1.8033174510610486, -1.8033174510610486, -1.7533174510610485, -1.7533174510610485, 0.06611276262996779, 0.06611276262996779, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, 13.202124258467084, 13.202124258467084, 15.0215544721581, 15.0215544721581, -3.622747664752065, -3.622747664752065, -1.8033174510610486, -1.8033174510610486, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, 13.202124258467084, 13.202124258467084, 15.0215544721581, 15.0215544721581, -3.622747664752065, -3.622747664752065, -1.8033174510610486, -1.8033174510610486, 11.332694044776066, 11.332694044776066, 13.152124258467081, 13.152124258467081, 13.202124258467084, 13.202124258467084, 15.0215544721581, 15.0215544721581, -3.622747664752065, -3.622747664752065, -1.8033174510610486, -1.8033174510610486, 13.202124258467084, 13.202124258467084, 15.0215544721581, 15.0215544721581, -3.622747664752065, -3.622747664752065, -1.8033174510610486, -1.8033174510610486, 13.202124258467084, 13.202124258467084, 15.0215544721581, 15.0215544721581], "z": [0.0, 4.0, 0.0, 4.0, 0.0, 84.0, 0.0, 84.0, 0.0, 81.0, 0.0, 81.0, 0.0, 2.0, 0.0, 2.0, 0.0, 4.0, 0.0, 4.0, 0.0, 84.0, 0.0, 84.0, 0.0, 81.0, 0.0, 81.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 266.0, 0.0, 266.0, 0.0, 1201.0, 0.0, 1201.0, 0.0, 1204.0, 0.0, 1204.0, 0.0, 170.0, 0.0, 170.0, 0.0, 1.0, 0.0, 1.0, 0.0, 266.0, 0.0, 266.0, 0.0, 1201.0, 0.0, 1201.0, 0.0, 1204.0, 0.0, 1204.0, 0.0, 170.0, 0.0, 170.0, 0.0, 50.0, 0.0, 50.0, 0.0, 998.0, 0.0, 998.0, 0.0, 1429.0, 0.0, 1429.0, 0.0, 1481.0, 0.0, 1481.0, 0.0, 817.0, 0.0, 817.0, 0.0, 28.0, 0.0, 28.0, 0.0, 50.0, 0.0, 50.0, 0.0, 998.0, 0.0, 998.0, 0.0, 1429.0, 0.0, 1429.0, 0.0, 1481.0, 0.0, 1481.0, 0.0, 817.0, 0.0, 817.0, 0.0, 28.0, 0.0, 28.0, 0.0, 234.0, 0.0, 234.0, 0.0, 449.0, 0.0, 449.0, 0.0, 107.0, 0.0, 107.0, 0.0, 128.0, 0.0, 128.0, 0.0, 459.0, 0.0, 459.0, 0.0, 188.0, 0.0, 188.0, 0.0, 234.0, 0.0, 234.0, 0.0, 449.0, 0.0, 449.0, 0.0, 107.0, 0.0, 107.0, 0.0, 128.0, 0.0, 128.0, 0.0, 459.0, 0.0, 459.0, 0.0, 188.0, 0.0, 188.0, 0.0, 22.0, 0.0, 22.0, 0.0, 189.0, 0.0, 189.0, 0.0, 15.0, 0.0, 15.0, 0.0, 1.0, 0.0, 1.0, 0.0, 26.0, 0.0, 26.0, 0.0, 175.0, 0.0, 175.0, 0.0, 4.0, 0.0, 4.0, 0.0, 22.0, 0.0, 22.0, 0.0, 189.0, 0.0, 189.0, 0.0, 15.0, 0.0, 15.0, 0.0, 1.0, 0.0, 1.0, 0.0, 26.0, 0.0, 26.0, 0.0, 175.0, 0.0, 175.0, 0.0, 4.0, 0.0, 4.0, 0.0, 33.0, 0.0, 33.0, 0.0, 31.0, 0.0, 31.0, 0.0, 34.0, 0.0, 34.0, 0.0, 22.0, 0.0, 22.0, 0.0, 33.0, 0.0, 33.0, 0.0, 31.0, 0.0, 31.0, 0.0, 34.0, 0.0, 34.0, 0.0, 22.0, 0.0, 22.0, 0.0, 1.0, 0.0, 1.0, 0.0, 18.0, 0.0, 18.0, 0.0, 1.0, 0.0, 1.0, 0.0, 21.0, 0.0, 21.0, 0.0, 1.0, 0.0, 1.0, 0.0, 18.0, 0.0, 18.0, 0.0, 1.0, 0.0, 1.0, 0.0, 21.0, 0.0, 21.0, 0.0, 1.0, 0.0, 1.0, 0.0, 5.0, 0.0, 5.0, 0.0, 6.0, 0.0, 6.0, 0.0, 3.0, 0.0, 3.0, 0.0, 1.0, 0.0, 1.0, 0.0, 5.0, 0.0, 5.0, 0.0, 6.0, 0.0, 6.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0]}],
                        {"height": 700, "hovermode": false, "scene": {"camera": {"eye": {"x": -1.25, "y": 1.25, "z": 1.25}}}, "showlegend": false, "template": {"data": {"bar": [{"error_x": {"color": "#2a3f5f"}, "error_y": {"color": "#2a3f5f"}, "marker": {"line": {"color": "#E5ECF6", "width": 0.5}}, "type": "bar"}], "barpolar": [{"marker": {"line": {"color": "#E5ECF6", "width": 0.5}}, "type": "barpolar"}], "carpet": [{"aaxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "baxis": {"endlinecolor": "#2a3f5f", "gridcolor": "white", "linecolor": "white", "minorgridcolor": "white", "startlinecolor": "#2a3f5f"}, "type": "carpet"}], "choropleth": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "choropleth"}], "contour": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "contour"}], "contourcarpet": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "contourcarpet"}], "heatmap": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "heatmap"}], "heatmapgl": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "heatmapgl"}], "histogram": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "histogram"}], "histogram2d": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "histogram2d"}], "histogram2dcontour": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "histogram2dcontour"}], "mesh3d": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "type": "mesh3d"}], "parcoords": [{"line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "parcoords"}], "pie": [{"automargin": true, "type": "pie"}], "scatter": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatter"}], "scatter3d": [{"line": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatter3d"}], "scattercarpet": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattercarpet"}], "scattergeo": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattergeo"}], "scattergl": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattergl"}], "scattermapbox": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scattermapbox"}], "scatterpolar": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterpolar"}], "scatterpolargl": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterpolargl"}], "scatterternary": [{"marker": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "type": "scatterternary"}], "surface": [{"colorbar": {"outlinewidth": 0, "ticks": ""}, "colorscale": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "type": "surface"}], "table": [{"cells": {"fill": {"color": "#EBF0F8"}, "line": {"color": "white"}}, "header": {"fill": {"color": "#C8D4E3"}, "line": {"color": "white"}}, "type": "table"}]}, "layout": {"annotationdefaults": {"arrowcolor": "#2a3f5f", "arrowhead": 0, "arrowwidth": 1}, "coloraxis": {"colorbar": {"outlinewidth": 0, "ticks": ""}}, "colorscale": {"diverging": [[0, "#8e0152"], [0.1, "#c51b7d"], [0.2, "#de77ae"], [0.3, "#f1b6da"], [0.4, "#fde0ef"], [0.5, "#f7f7f7"], [0.6, "#e6f5d0"], [0.7, "#b8e186"], [0.8, "#7fbc41"], [0.9, "#4d9221"], [1, "#276419"]], "sequential": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]], "sequentialminus": [[0.0, "#0d0887"], [0.1111111111111111, "#46039f"], [0.2222222222222222, "#7201a8"], [0.3333333333333333, "#9c179e"], [0.4444444444444444, "#bd3786"], [0.5555555555555556, "#d8576b"], [0.6666666666666666, "#ed7953"], [0.7777777777777778, "#fb9f3a"], [0.8888888888888888, "#fdca26"], [1.0, "#f0f921"]]}, "colorway": ["#636efa", "#EF553B", "#00cc96", "#ab63fa", "#FFA15A", "#19d3f3", "#FF6692", "#B6E880", "#FF97FF", "#FECB52"], "font": {"color": "#2a3f5f"}, "geo": {"bgcolor": "white", "lakecolor": "white", "landcolor": "#E5ECF6", "showlakes": true, "showland": true, "subunitcolor": "white"}, "hoverlabel": {"align": "left"}, "hovermode": "closest", "mapbox": {"style": "light"}, "paper_bgcolor": "white", "plot_bgcolor": "#E5ECF6", "polar": {"angularaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "bgcolor": "#E5ECF6", "radialaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "scene": {"xaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}, "yaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}, "zaxis": {"backgroundcolor": "#E5ECF6", "gridcolor": "white", "gridwidth": 2, "linecolor": "white", "showbackground": true, "ticks": "", "zerolinecolor": "white"}}, "shapedefaults": {"line": {"color": "#2a3f5f"}}, "ternary": {"aaxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "baxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}, "bgcolor": "#E5ECF6", "caxis": {"gridcolor": "white", "linecolor": "white", "ticks": ""}}, "title": {"x": 0.05}, "xaxis": {"automargin": true, "gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "zerolinewidth": 2}, "yaxis": {"automargin": true, "gridcolor": "white", "linecolor": "white", "ticks": "", "title": {"standoff": 15}, "zerolinecolor": "white", "zerolinewidth": 2}}}, "title": {"x": 0.5}, "width": 650},
                        {"displayModeBar": false, "scrollZoom": false, "responsive": true}
                    )
                };
                
            </script>
        </div>
</body>
</html>
</div>
<p>A visualization solution that requires less technology would start the same way,
i.e., by dividing the <span class="math inline">\(xy\)</span> plane into regions, but instead of the third
dimension, it would use different colors to represent the counts. Here is an
example where the regions are hexagons, as opposed to squares; it just looks
better, for some reason:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-82-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Just to showcase the range of possibilities, here is another visualization
technique which which requires deeper statistical tools, namely the
<strong>density contour plot</strong>:
<img src="_main_files/figure-html/unnamed-chunk-83-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p><em>R.</em> There is very little new R here. You should remember that if <code>x</code> and <code>y</code> are
vectors of the same length, <code>plot(x,y)</code> gives you a scatterplot of <code>x</code> and <code>y</code>.</p>
<p>To compute the sample correlation between two vectors, use the <code>cor</code>.</p>
<p>We used the command <code>sample(y)</code> to obtain a randomly permuted version of <code>y</code>.
The simplicity of this is due to default parameters of the command <code>sample</code>
which we already learned about. In particular, the default number of samples is
exactly the size of the input vector <code>y</code> and, by default, sampling is performed
<em>without replacement</em>. If you think about it for a second, you will realize that
a sample of size <span class="math inline">\(n\)</span> from the vector of size <span class="math inline">\(n\)</span> <em>without</em> replacement is
nothing by a random permutation of <code>y</code>.</p>
<p>You are not required to do this in your submissions, but if you want to display
several plots side-by-side, use the command is <code>par(mfrow=c(m,n))</code> before the
<code>plot</code> commands. It tells R to plot the next <span class="math inline">\(mn\)</span> plots in a <span class="math inline">\(m\times n\)</span> grid.</p>
</div>
<div class="example">
<p>Let the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have the joint distribution given by
the following table:</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.3
</td>
</tr>
<tr>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.0
</td>
</tr>
</tbody>
</table>
<p>Simulate <span class="math inline">\(10,000\)</span> draws from the distribution of <span class="math inline">\((X,Y)\)</span> and display a
contingency table of your results.</p>
</div>
<p class="solution">
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true"></a>joint_distribution_long =<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true"></a>   <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>),</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true"></a>   <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true"></a>)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true"></a>probabilities_long =<span class="st"> </span></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true"></a><span class="st">       </span><span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.2</span>, <span class="fl">0.0</span>)</span>
<span id="cb68-7"><a href="#cb68-7" aria-hidden="true"></a></span>
<span id="cb68-8"><a href="#cb68-8" aria-hidden="true"></a>sampled_rows =<span class="st"> </span><span class="kw">sample</span>(</span>
<span id="cb68-9"><a href="#cb68-9" aria-hidden="true"></a>   <span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(joint_distribution_long),</span>
<span id="cb68-10"><a href="#cb68-10" aria-hidden="true"></a>   <span class="dt">size =</span> <span class="dv">10000</span>,</span>
<span id="cb68-11"><a href="#cb68-11" aria-hidden="true"></a>   <span class="dt">replace =</span> <span class="ot">TRUE</span>,</span>
<span id="cb68-12"><a href="#cb68-12" aria-hidden="true"></a>   <span class="dt">prob =</span> probabilities_long</span>
<span id="cb68-13"><a href="#cb68-13" aria-hidden="true"></a>)</span>
<span id="cb68-14"><a href="#cb68-14" aria-hidden="true"></a></span>
<span id="cb68-15"><a href="#cb68-15" aria-hidden="true"></a>draws =<span class="st"> </span>joint_distribution_long[sampled_rows, ]</span>
<span id="cb68-16"><a href="#cb68-16" aria-hidden="true"></a></span>
<span id="cb68-17"><a href="#cb68-17" aria-hidden="true"></a><span class="kw">table</span>(draws)</span>
<span id="cb68-18"><a href="#cb68-18" aria-hidden="true"></a><span class="co">##    y</span></span>
<span id="cb68-19"><a href="#cb68-19" aria-hidden="true"></a><span class="co">## x      1    2    3</span></span>
<span id="cb68-20"><a href="#cb68-20" aria-hidden="true"></a><span class="co">##   1  962 2027 3047</span></span>
<span id="cb68-21"><a href="#cb68-21" aria-hidden="true"></a><span class="co">##   2 1945 2019    0</span></span></code></pre></div>
</p>
<div class="comments">
<p><em>Math.</em> The main mathematical idea is to think of <em>each pair</em> of possible values
of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as a separate “object”, put all these objects in a “bag”, then
then draw from the bag. In other words, we convert the bivariate distribution
from the problem to the following univariate distribution</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
(1,1)
</th>
<th style="text-align:right;">
(1,2)
</th>
<th style="text-align:right;">
(1,3)
</th>
<th style="text-align:right;">
(2,1)
</th>
<th style="text-align:right;">
(2,2)
</th>
<th style="text-align:right;">
(2,3)
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
0
</td>
</tr>
</tbody>
</table>
<p>and sample from it. When you do, you will get a vector whose elements are pairs
of numbers. The last step is to extract the components of those pairs into
separate vectors.</p>
<p><em>R.</em> The most important new R concept here is <code>data.frame</code>. You should think of
it as a spreadsheet. It is, mathematically, a matrix, but we do not perform any
mathematical operations on it. Moreover, not all columns in the data frame have
to be numeric. Some of them can be strings, and other can be something even more
exotic. You should think of a data frame as a bunch of column vectors of the
same length stacked side by side. It is important to note that each column of a
data frame will have a name, so that we don’t have to access it by its position
only (as we would have to in the case of a matrix).</p>
<p>In this class, the column vectors of data frames are going to contain simulated
values. In statistics, it is data that comes in data frames, with rows
corresponding to different observations, and columns to various observed
variables.</p>
<p>The easiest way to construct a data frame using already existing vectors is as follows:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>, <span class="st">&quot;c&quot;</span>)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true"></a>(<span class="dt">df =</span> <span class="kw">data.frame</span>(x, y))</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true"></a><span class="co">##   x y</span></span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true"></a><span class="co">## 1 1 a</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true"></a><span class="co">## 2 2 b</span></span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true"></a><span class="co">## 3 3 c</span></span></code></pre></div>
<p>Note that the two columns inherited their names from the vectors <code>x</code> and <code>y</code>
that fed into them. Note, also, that all rows got consecutive numerical values
as names by default. Row names are sometimes useful to have, but are in general
a nuisance and should be ignored (especially in this class). Column names are
more important, and there is a special notation (the dollar-sign notation) that
allows you to access a column by its name:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true"></a>df<span class="op">$</span>y</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true"></a><span class="co">## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot;</span></span></code></pre></div>
<p>If you want to give your columns custom names (or if you are building them out
of explicitly given vectors as in the solution above) use the following syntax</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true"></a>z =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>, <span class="st">&quot;c&quot;</span>, <span class="st">&quot;d&quot;</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true"></a>(<span class="dt">df =</span> <span class="kw">data.frame</span>(<span class="dt">letters =</span> z, <span class="dt">numbers =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>)))</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true"></a><span class="co">##   letters numbers</span></span>
<span id="cb71-4"><a href="#cb71-4" aria-hidden="true"></a><span class="co">## 1       a       1</span></span>
<span id="cb71-5"><a href="#cb71-5" aria-hidden="true"></a><span class="co">## 2       b       2</span></span>
<span id="cb71-6"><a href="#cb71-6" aria-hidden="true"></a><span class="co">## 3       c       3</span></span>
<span id="cb71-7"><a href="#cb71-7" aria-hidden="true"></a><span class="co">## 4       d       4</span></span></code></pre></div>
<p>A feature that data frames share with vectors and matrices is that you can use vector indexing as in the following example (where <code>df</code> is as above)</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true"></a>df[<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1</span>), ]</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true"></a><span class="co">##     letters numbers</span></span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true"></a><span class="co">## 2         b       2</span></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true"></a><span class="co">## 4         d       4</span></span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true"></a><span class="co">## 4.1       d       4</span></span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true"></a><span class="co">## 1         a       1</span></span></code></pre></div>
<p>Make sure you understand why the expression inside the brackets is <code>c(2,4,4,1),</code>
and not <code>c(2,4,4,1)</code>. R’s desire to keep row names unique leads to some
cumbersome constructs such as <code>4.1</code> above. As I mentioned before, just disregard
them.</p>
<p>A nice thing about data frames is that they can easily be pretty-printed in
RStudio. Go to the Environment tab in one of your RStudio panes, and double
click on the name of the data frame you just built. It will appear as a nicely
formatted spreadsheet.</p>
<p>Once we have the data frame containing all <span class="math inline">\(6\)</span> pairs of possible values <span class="math inline">\(X\)</span> and
<span class="math inline">\(Y\)</span> can take (called <code>joint_distribution_long</code> in the solution above), we can
proceed by sampling from its rows, by sampling from the set <code>1,2,3,4,5,6</code> with
probabilities <code>0.1, 0.2, 0.3, 0.2, 0.2, 0.0</code>. The result of the corresponding
<code>sample</code> command will be a sequence - called <code>sampled_rows</code> in the solution - of
length <span class="math inline">\(10,000\)</span> composed of numbers <span class="math inline">\(1,2,3,4,5\)</span> or <span class="math inline">\(6\)</span>. The reason we chose the
name <code>sampled_rows</code> is because each number corresponds to a row from the data
frame <code>joint_distribution_long</code>, and by indexing <code>joint_distribution_long</code> by
<code>sampled_rows</code> we are effectively sampling from its rows. In other words, the
command <code>joint_distribution_long[sampled_rows, ]</code> turns a bunch of numbers into
a bunch of rows (many of them repeated) of the data frame
<code>joint_distribution_long</code>.</p>
<p>The final step is to use the function <code>table</code>. This time, we are applying it to
a data frame and not to a vector, but the effect is the same. It tabulates all
possible combinations of values of the columns, and counts how many times each
of them happened. The same result would have been obtained by calling
<code>table(draws$x, draws$y)</code>.</p>
</div>
</div>
<div id="monte-carlo" class="section level2" number="2.3">
<h2 number="2.3"><span class="header-section-number">2.3</span> Monte Carlo</h2>
<div class="example">
<p>Use Monte Carlo to estimate the expected value of the exponential random
variable with parameter <span class="math inline">\(\lambda= 4\)</span> using <span class="math inline">\(n=10\)</span>, <span class="math inline">\(n=1,000\)</span> and <span class="math inline">\(n=1,000,000\)</span>
simulations. Compare to the exact value.</p>
</div>
<p class="solution">
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">10</span>, <span class="dt">rate =</span> <span class="dv">4</span>)</span>
<span id="cb73-2"><a href="#cb73-2" aria-hidden="true"></a><span class="kw">mean</span>(x)</span>
<span id="cb73-3"><a href="#cb73-3" aria-hidden="true"></a><span class="co">## [1] 0.1779768</span></span></code></pre></div>
<p>For an exponential random variable with parameter <span class="math inline">\(\lambda\)</span>, the expected value is
<span class="math inline">\(1/\lambda\)</span> (such information can be found in <a href="./dist.html">Appendix A</a>) which,
in this case, is <span class="math inline">\(0.25\)</span>. The error made was 0.072023 for <span class="math inline">\(n=10\)</span> simulations.</p>
<p>We increase the number of simulations to <span class="math inline">\(n=1000\)</span> and get a better result</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">1000</span>, <span class="dt">rate =</span> <span class="dv">4</span>)</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true"></a><span class="kw">mean</span>(x)</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true"></a><span class="co">## [1] 0.2564643</span></span></code></pre></div>
<p>with (smaller) error -0.0064643. Finally, let’s try <span class="math inline">\(n=1,000,000\)</span>:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rexp</span>(<span class="fl">1e+06</span>, <span class="dt">rate =</span> <span class="dv">4</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true"></a><span class="kw">mean</span>(x)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true"></a><span class="co">## [1] 0.250381</span></span></code></pre></div>
The error is even smaller -0.00038101.
</p>
<!-- This can be obtained quite easily by integration (by parts): -->
<!-- $$ \EE[X] = \int_{-\infty}^{\infty} x f(x)\, dx = \int_0^{\infty} x \ld e^{-\ld x}\, dx = \tfrac{1}{\ld}$$ -->
<div class="comments">
<p><em>R.</em> The only new thing here is the command <code>mean</code> which computes the mean of a vector.</p>
<p><em>Math.</em> There is a lot going on here conceptually. This is the first time we
used the Monte Carlo method. It is an incredibly useful tool, as you will keep
being reminded throughout this class. The idea behind it is simple, and it is
based on the <em>Law of large numbers</em>:</p>
<p><strong>Theorem</strong> Let <span class="math inline">\(X_1,X_2, \dots\)</span> be an independent sequence of random
variables with the same distribution, for which the expected value can be
computed. Then
<span class="math display">\[ \tfrac{1}{n} \Big( X_1+X_2+\dots+X_n\Big) \to {\mathbb{E}}[X_1] \text{ as } n\to\infty\]</span>
The idea behind Monte Carlo is to turn this theorem “upside down”. The goal is
to compute <span class="math inline">\({\mathbb{E}}[X\_1]\)</span> and use a supply of random numbers, each of which
comes from the same distribution, to accomplish that. The random number
generator inside <code>rexp</code> gives us a supply of numbers (stored in the vector <code>x</code>)
and all we have to do is compute their average. This gives us the left-hand side
of the formula above, and, if <span class="math inline">\(n\)</span> is large enough, we hope that this average
does not differ too much from its theoretical limit. As <span class="math inline">\(n\)</span> gets larger, we
expect better and better results. That is why your error above gets smaller as
<span class="math inline">\(n\)</span> increases.</p>
<p>It looks like Monte Carlo can only be used to compute the expected value of a
random variable, which does not seem like such a bit deal. But it is! You will see
in the sequel that almost anything can be written as the expected value of
<em>some</em> random variable.</p>
</div>
<div class="example">
<p>Use Monte Carlo to estimate <span class="math inline">\({\mathbb{E}}[X^2]\)</span>, where <span class="math inline">\(X\)</span> is a standard normal
random variable.</p>
</div>
<p class="solution">
<p>You may or may now know that when <span class="math inline">\(X\)</span> is standard normal <span class="math inline">\(Y=X^2\)</span> has a <span class="math inline">\(\chi^2\)</span>
distribution with one degree of freedom. If you do, you can solve the problem
like this:</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">rchisq</span>(<span class="dv">5000</span>, <span class="dt">df =</span> <span class="dv">1</span>)</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true"></a><span class="kw">mean</span>(y)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true"></a><span class="co">## [1] 0.9771929</span></span></code></pre></div>
<p>If you don’t, you can do the following:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">5000</span>)</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true"></a>y =<span class="st"> </span>x<span class="op">^</span><span class="dv">2</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true"></a><span class="kw">mean</span>(y)</span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true"></a><span class="co">## [1] 1.019852</span></span></code></pre></div>
</p>
<div class="comments">
<p><em>Math+R.</em> We are asked to compute <span class="math inline">\({\mathbb{E}}[ X^2]\)</span>, which can be interpreted in
two ways. First, we can think of <span class="math inline">\(Y=X^2\)</span> as a random variable in its own right and you
can try to take draws from the distribution of <span class="math inline">\(Y\)</span>. In the case of the normal
distribution, the distribution of <span class="math inline">\(Y\)</span> is known - it happens to be a
<span class="math inline">\(\chi^2\)</span>-distribution with a single degree of freedom (don’t worry if you never
heard of it). We can simulate it in R by using its R name <code>chisq</code> and
get a number close to the exact value of <span class="math inline">\(1\)</span>.</p>
<p>If you did not know about the <span class="math inline">\(\chi^2\)</span> distribution, you would not know what R
name to put the prefix <code>r</code> in front of. What makes the simulation possible is
the fact that <span class="math inline">\(Y\)</span> is a <em>transformation</em> of
a random variable we know how to simulate. In that case, we simply simulate the
required number of draws <code>x</code> from the geometric distribution (using <code>rnorm</code>) and
then apply the transformation <span class="math inline">\(x \mapsto x^2\)</span> to the result. The transformed
vector <code>y</code> is then nothing but the sequence of draws from the distribution of
<span class="math inline">\(X^2\)</span>.</p>
<p>The idea described above is one of main advantages of the Monte Carlo technique:
if you know how to simulate a random variable, you also know how to simulate
any (deterministic) function of it. That fact will come into its own a bit later
when we start working with several random variables and stochastic processes,
but it can be very helpful even in the case of a single random variable, as you
will see in the next problem.</p>
</div>
<div class="example">
<p>Let <span class="math inline">\(X\)</span> be a standard normal random variable. Use Monte Carlo to estimate the
probability <span class="math inline">\({\mathbb{P}}[ X &gt; 1 ]\)</span>. Compare to the exact value.</p>
</div>
<p class="solution">
<p>The estimated probability:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10000</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true"></a>y =<span class="st"> </span>x <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true"></a>(<span class="dt">p_est =</span> <span class="kw">mean</span>(y))</span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true"></a><span class="co">## [1] 0.1608</span></span></code></pre></div>
<p>The exact probability and the error</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true"></a>p_true =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dv">1</span>)</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true"></a>(<span class="dt">err =</span> p_est <span class="op">-</span><span class="st"> </span>p_true)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true"></a><span class="co">## [1] 0.002144746</span></span></code></pre></div>
</p>
<div class="comments">
<p><em>R.</em> As we learned before, the symbol <code>&gt;</code> is an operation, which returns a Boolean (<code>TRUE</code> or <code>FALSE</code>) value. For example:</p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true"></a><span class="dv">1</span> <span class="op">&gt;</span><span class="st"> </span><span class="dv">2</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true"></a><span class="co">## [1] FALSE</span></span></code></pre></div>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true"></a><span class="dv">5</span><span class="op">^</span><span class="dv">2</span> <span class="op">&gt;</span><span class="st"> </span><span class="dv">20</span></span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true"></a><span class="co">## [1] TRUE</span></span></code></pre></div>
<p>It is vectorized:</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">-4</span>, <span class="dv">3</span>)</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true"></a>x <span class="op">&gt;</span><span class="st"> </span>y</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true"></a><span class="co">## [1] FALSE  TRUE  TRUE</span></span></code></pre></div>
<p>and recycling rules apply to it (so that you can compare a vector and a scalar, for example)</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">10</span></span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true"></a>x <span class="op">&gt;</span><span class="st"> </span><span class="dv">5</span></span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true"></a><span class="co">##  [1] FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE</span></span></code></pre></div>
<p>Therefore, the vector <code>y</code> in the solution is a vector of length <span class="math inline">\(10000\)</span> whose
elements are either <code>TRUE</code> or <code>FALSE</code>; here are the first 5 rows of data frame
with columns <code>x</code> and <code>y</code> from our solution:</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
x
</th>
<th style="text-align:left;">
y
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1.9493
</td>
<td style="text-align:left;">
FALSE
</td>
</tr>
<tr>
<td style="text-align:right;">
-1.1015
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
<tr>
<td style="text-align:right;">
1.0448
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.1384
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.2573
</td>
<td style="text-align:left;">
TRUE
</td>
</tr>
</tbody>
</table>
<p>Finally, <code>z</code> contains the <code>mean</code> of <code>y</code>. How do you compute a mean of Boolean values? In R (and many other languages) <code>TRUE</code> and <code>FALSE</code> have default numerical values, usually <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span>. This way, when <span class="math inline">\(R\)</span> is asked to compute the <code>sum</code> of a Boolean vector it will effectively count the number of values which are <code>TRUE</code>. Similarly, the <code>mean</code> is the relative proportion of <code>TRUE</code> values.</p>
<p><em>Math.</em> We computed the <strong>proportion</strong> of the “times” <span class="math inline">\(X&gt;1\)</span> (among many simulations of <span class="math inline">\(X\)</span>) and used it to approximate the <strong>probability</strong> <span class="math inline">\({\mathbb{P}}[ X&gt;1]\)</span>. More formally,
we started from a random variable <span class="math inline">\(X\)</span> with a normal distribution and then transformed it into another random variable, <span class="math inline">\(Y\)</span>, by setting <span class="math inline">\(Y=1\)</span> whenever <span class="math inline">\(X&gt;1\)</span> and <span class="math inline">\(0\)</span> otherwise. This is often written as follows
<span class="math display">\[ Y = \begin{cases} 1, &amp; X&gt;1 \\ 0, &amp; X\leq 1.\end{cases}\]</span>
The random variable <span class="math inline">\(Y\)</span> is very special - it can only take values <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> (i.e., its support is <span class="math inline">\(\{0,1\}\)</span>). Such random variables are called <strong>indicator random variables</strong>, and their distribution, called the <strong>Bernoulli distribution</strong>, always looks like this:</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
0
</th>
<th style="text-align:left;">
1
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
1-p
</td>
<td style="text-align:left;">
p
</td>
</tr>
</tbody>
</table>
<p>for some <span class="math inline">\(p \in [0,1]\)</span>. The parameter <span class="math inline">\(p\)</span> is nothing but the probability <span class="math inline">\({\mathbb{P}}[Y=1]\)</span>.</p>
<p>So why did we decide to transform <span class="math inline">\(X\)</span> into <span class="math inline">\(Y\)</span>? Because of the following simple fact:
<span class="math display">\[ {\mathbb{E}}[ Y] = 1 \times p + 0 \times (1-p) = p.\]</span>
The expected value of an indicator is the probability <span class="math inline">\(p\)</span>, and we know that we can use Monte Carlo whenever we can express the quantity we are computing as an expected value of a random variable we know how to simulate.</p>
</div>
<p>Many times, simulating a random variable is easier than analyzing it analytically. Here is a fun example:</p>
<div class="example">
<p>Use Monte Carlo to estimate the value of <span class="math inline">\(\pi\)</span> and compute the error.</p>
</div>
<p class="solution">
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">1000000</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true"></a>x =<span class="st">  </span><span class="kw">runif</span>(nsim,<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true"></a>y =<span class="st">  </span><span class="kw">runif</span>(nsim,<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true"></a>z =<span class="st"> </span>(x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>y <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true"></a>(<span class="dt">pi_est =</span> <span class="dv">4</span> <span class="op">*</span><span class="st"> </span><span class="kw">mean</span>(z))</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true"></a><span class="co">## [1] 3.141728</span></span>
<span id="cb84-7"><a href="#cb84-7" aria-hidden="true"></a>(<span class="dt">err =</span> pi_est <span class="op">-</span><span class="st"> </span>pi)</span>
<span id="cb84-8"><a href="#cb84-8" aria-hidden="true"></a><span class="co">## [1] 0.0001353464</span></span></code></pre></div>
</p>
<div class="comments">
<p><em>Math.</em>
<img src="pics/mc_pi.gif" width="50%" style="float:right; padding:10px" style="display: block; margin: auto;" />
As we learned in the previous problem, probabilities of events can be computed using Monte Carlo, as long as we know how to simulate the underlying indicator random variable. In this case, we want to compute <span class="math inline">\(\pi\)</span>, so we would need to find a “situation” in which the probability of something is <span class="math inline">\(\pi\)</span>. Of course, <span class="math inline">\(\pi&gt;1\)</span>, so it cannot be a probability of anything, but <span class="math inline">\(\pi/4\)</span> can, and computing <span class="math inline">\(\pi/4\)</span> is as useful as computing <span class="math inline">\(\pi\)</span>. To create the required probabilistic “situation” we think of the geometric meaning of <span class="math inline">\(\pi\)</span>, and come up with the following scheme. Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two independent uniform random variables each with values between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>. We can think of the pair <span class="math inline">\((X,Y)\)</span> as a random point in the square <span class="math inline">\([-1,1]\times [-1,1]\)</span>. This point will sometimes fall inside the unit circle, and sometimes it will not. What is the probability of hitting the circle? Well, since <span class="math inline">\((X,Y)\)</span> is uniformly distributed everywhere inside the square, this probability should be equal to the portion of the area of our square which belongs to the unit circle. The area of the square is <span class="math inline">\(4\)</span> and the area of the circle is <span class="math inline">\(\pi\)</span>, so the required probability is <span class="math inline">\(\pi/4\)</span>. Using the idea from the previous problem, we define the indicator random variable <span class="math inline">\(Z\)</span> as follows
<span class="math display">\[ Z = \begin{cases} 1 &amp; (X,Y) \text{ is inside the unit circle, } \\ 0 &amp; \text{ otherwise.}
\end{cases}
= \begin{cases} 1&amp; X^2+Y^2 &lt; 1, \\ 0 &amp; \text{ otherwise.} \end{cases}\]</span></p>
</div>
<div class="example">
<ol style="list-style-type: decimal">
<li><p>Write an R function <code>cumavg</code> which computes the sequence of running averages of a vector, i.e., if the input is <span class="math inline">\(x=(x_1,x_2,x_3,\dots, x_n)\)</span>, the output should be
<span class="math display">\[ \Big(x_1, \frac{1}{2} (x_1+x_2), \frac{1}{3}(x_1+x_2+x_3), \dots, \frac{1}{n} (x_1+x_2+\dots+x_n)\Big).\]</span> Test it to check that it really works. (<em>Hint:</em> look up the function <code>cumsum</code>. )</p></li>
<li><p>Apply <code>cumavg</code> to the vector <span class="math inline">\(4 z\)</span> from the previous problem and plot your results (use a smaller value for <code>nsim</code>. Maybe <span class="math inline">\(1000\)</span>.) Plot the values against their index. Add a red horizontal line at the level <span class="math inline">\(\pi\)</span>. Rerun the same code (including the simulation part) several times.</p></li>
</ol>
</div>
<p class="solution">
<p><part> 1. </part></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true"></a>cumavg =<span class="st"> </span><span class="cf">function</span>(x) {</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true"></a>    c =<span class="st"> </span><span class="kw">cumsum</span>(x)</span>
<span id="cb85-3"><a href="#cb85-3" aria-hidden="true"></a>    n =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x)</span>
<span id="cb85-4"><a href="#cb85-4" aria-hidden="true"></a>    <span class="kw">return</span>(c<span class="op">/</span>n)</span>
<span id="cb85-5"><a href="#cb85-5" aria-hidden="true"></a>}</span>
<span id="cb85-6"><a href="#cb85-6" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">9</span>)</span>
<span id="cb85-7"><a href="#cb85-7" aria-hidden="true"></a><span class="kw">cumavg</span>(x)</span>
<span id="cb85-8"><a href="#cb85-8" aria-hidden="true"></a><span class="co">## [1] 1 2 3 3 3 4</span></span></code></pre></div>
<p><part> 2. </part></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb86-2"><a href="#cb86-2" aria-hidden="true"></a>x =<span class="st">  </span><span class="kw">runif</span>(nsim,<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb86-3"><a href="#cb86-3" aria-hidden="true"></a>y =<span class="st">  </span><span class="kw">runif</span>(nsim,<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb86-4"><a href="#cb86-4" aria-hidden="true"></a>z =<span class="st"> </span>(x <span class="op">^</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>y <span class="op">^</span><span class="st"> </span><span class="dv">2</span>) <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb86-5"><a href="#cb86-5" aria-hidden="true"></a>pi_est =<span class="st"> </span><span class="kw">cumavg</span>(<span class="dv">4</span> <span class="op">*</span><span class="st"> </span>z)</span>
<span id="cb86-6"><a href="#cb86-6" aria-hidden="true"></a><span class="kw">plot</span>(</span>
<span id="cb86-7"><a href="#cb86-7" aria-hidden="true"></a>   <span class="dv">1</span><span class="op">:</span>nsim,</span>
<span id="cb86-8"><a href="#cb86-8" aria-hidden="true"></a>   pi_est,</span>
<span id="cb86-9"><a href="#cb86-9" aria-hidden="true"></a>   <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>,</span>
<span id="cb86-10"><a href="#cb86-10" aria-hidden="true"></a>   <span class="dt">xlab =</span> <span class="st">&quot;number of simulations&quot;</span>,</span>
<span id="cb86-11"><a href="#cb86-11" aria-hidden="true"></a>   <span class="dt">ylab =</span> <span class="st">&quot;estimate of pi&quot;</span>,</span>
<span id="cb86-12"><a href="#cb86-12" aria-hidden="true"></a>   <span class="dt">main =</span> <span class="st">&quot;Computing pi by Monte Carlo&quot;</span></span>
<span id="cb86-13"><a href="#cb86-13" aria-hidden="true"></a>)</span>
<span id="cb86-14"><a href="#cb86-14" aria-hidden="true"></a><span class="kw">abline</span>(pi, <span class="dv">0</span>,</span>
<span id="cb86-15"><a href="#cb86-15" aria-hidden="true"></a>       <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<img src="_main_files/figure-html/unnamed-chunk-107-1.png" width="672" style="display: block; margin: auto;" />
</p>
<div class="comments">
<p>Part 1. Once you know about <code>cumsum</code>, the problem becomes much easier.</p>
<p>Part 2. This course is not about R graphics, but I think it is a good idea to teach you how to make basic plots in R. We already used the function <code>plot</code> to draw scatterplots.
By default, each point drawn by <code>plot</code> is marked by a small circle so it might not seem like a good idea to use it. Luckily this, and many other things, can be adjusted by numerous additional arguments. One of such arguments is <code>type</code> which determines the type of the plot. We used <code>type="l"</code> which tells R to join the points with straight lines:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">7</span>)</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true"></a><span class="kw">plot</span>(x, y, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-108-1.png" width="672" style="display: block; margin: auto;" />
The other arguments, <code>xlab</code>, <code>ylab</code> and <code>main</code> determine labels for axes and the entire plot. The function <code>abline(a,b)</code> adds a line <span class="math inline">\(y = a x + b\)</span> to an already existing plot. It is very useful in statistics if one wants to show the regression line superimposed on the scatterplot of data. Finally, the argument <code>col</code>, of course, determines the color of the line. To learn about various graphical parameters, type <code>?par</code>.</p>
<p><em>Math.</em> The conceptual reason for this exercise is to explore (numerically) the kinds of errors we make when we use Monte Carlo. Unlike the deterministic numerical procedures, Monte Carlo has a strange property that no bound on the error can be made with absolute certainty. Let me give you an example. Suppose that you have a biased coin, with the probability <span class="math inline">\(0.6\)</span> of heads and <span class="math inline">\(0.4\)</span> of tails. You don’t know this probability, and use a Monte Carlo technique to estimate it - you toss your coin <span class="math inline">\(1000\)</span> times and record the number of times you observe <span class="math inline">\(H\)</span>. The law of large numbers suggests that the relative frequency of heads is close to the true probability of <span class="math inline">\(H\)</span>. Indeed, you run a simulation</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;T&quot;</span>, <span class="st">&quot;H&quot;</span>), <span class="dv">1000</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.4</span>, <span class="fl">0.6</span>), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true"></a>y =<span class="st"> </span>x <span class="op">==</span><span class="st"> &quot;H&quot;</span></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true"></a><span class="kw">mean</span>(y)</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true"></a><span class="co">## [1] 0.594</span></span></code></pre></div>
<p>and get a pretty accurate estimate of <span class="math inline">\(0.594\)</span>. If you run the same code a few more times, you will get different estimates, but all of them will be close to <span class="math inline">\(0.6\)</span>. Theoretically, however, your simulation could have yielded <span class="math inline">\(1000\)</span> Hs, which would lead you to report <span class="math inline">\(p=1\)</span> as the Monte-Carlo estimate. The point is that even though such disasters are theoretically possible, they are exceedingly unlikely. The probability of getting all <span class="math inline">\(H\)</span> in <span class="math inline">\(1000\)</span> tosses of this coin is a number with more than <span class="math inline">\(500\)</span> zeros after the decimal point.</p>
<p>The take-home message is that even though there are no guarantees, Monte Carlo performs well the vast majority of the time. The crucial ingredient, however, is the number of simulations. The plot you were asked to make illustrates exactly that. The function <code>cumavg</code> gives you a whole sequence of Monte-Carlo estimates of the same thing (the number <span class="math inline">\(\pi\)</span>) with different numbers of simulations <code>nsim</code>. For small values of <code>nsim</code> the error is typically very large (and very random). As the number of simulations grows, the situations stabilizes and the error decreases. Without going into the theory behind it, let me only mention is that in the majority of practical applications we have the following relationship:
<span class="math display">\[ error \sim \frac{1}{\sqrt{n}}.\]</span>
In words, if you want to double the precision, you need to quadruple the number of simulations. If you want an extra digit in your estimate, you need to multiply the number of simulations by <span class="math inline">\(100\)</span>. Here is an image where I superimposed <span class="math inline">\(40\)</span> plots like the one you were asked to produce (the red lines are <span class="math inline">\(\pm \frac{4}{\sqrt{n}}\)</span>):</p>
<p><img src="_main_files/figure-html/unnamed-chunk-110-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="conditional-distributions" class="section level2" number="2.4">
<h2 number="2.4"><span class="header-section-number">2.4</span> Conditional distributions</h2>
<div class="example">
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two independent geometric random variables with parameters <span class="math inline">\(p=0.5,\)</span> and let <span class="math inline">\(Z=X+Y\)</span>. Compute <span class="math inline">\({\mathbb{P}}[ X = 3| Z = 5]\)</span> using simulation. Compare to the exact value.</p>
</div>
<p class="solution">
<p>By simulation:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">1000000</span></span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true"></a>X =<span class="st"> </span><span class="kw">rgeom</span>(nsim, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true"></a>Y =<span class="st"> </span><span class="kw">rgeom</span>(nsim, <span class="dt">prob =</span> <span class="fl">0.5</span>)</span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true"></a>Z =<span class="st"> </span>X <span class="op">+</span><span class="st"> </span>Y</span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true"></a>X_cond =<span class="st"> </span>X[Z <span class="op">==</span><span class="st"> </span><span class="dv">5</span>]</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true"></a><span class="kw">mean</span>(X_cond  <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) </span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true"></a><span class="co">## [1] 0.1684758</span></span></code></pre></div>
To get the exact value, we start from the definition:
<span class="math display">\[ {\mathbb{P}}[ X = 3 | Z= 5 ] = \frac{{\mathbb{P}}[ X=3 \text{ and }Z=5]}{{\mathbb{P}}[Z=5]} = \frac{{\mathbb{P}}[X=3 \text{ and }Y = 2]}{{\mathbb{P}}[Z=5]}, \]</span>
where the last equality follows from the fact that <span class="math inline">\(\{ X=3 \text{ and } Z=5 \}\)</span> is exactly the same event as <span class="math inline">\(\{ X = 3 \text{ and } Y=2\}\)</span>.
Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, we have
<span class="math display">\[{\mathbb{P}}[ X=3 \text{ and }Y=2 ] = {\mathbb{P}}[X=3] \times {\mathbb{P}}[ Y=2] = 2^{-4} 2^{-3} = 2^{-7}.\]</span>
To compute <span class="math inline">\({\mathbb{P}}[ Z = 5]\)</span> we need to split the event <span class="math inline">\(\{ Z = 5 \}\)</span> into events we know how to deal with. Since <span class="math inline">\(Z\)</span> is built from <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we write
<span class="math display">\[ \begin{align} {\mathbb{P}}[ Z = 5 ] = &amp;{\mathbb{P}}[X=0 \text{ and }Y=5]+ {\mathbb{P}}[ X=1 \text{ and }Y=4] + {\mathbb{P}}[ X=2 \text{ and }Y=3] + \\
&amp;  {\mathbb{P}}[ X=3 \text{ and }Y=2] + {\mathbb{P}}[ X=4 \text{ and }Y=1] + {\mathbb{P}}[ X = 5 \text{ and }Y=0]. \end{align}\]</span>
Each of the individual probabilities in the sum above is <span class="math inline">\(2^{-7}\)</span>, so <span class="math inline">\({\mathbb{P}}[ X = 3 | Z = 5] = \frac{1}{6}\)</span>.
This gives us an error of 0.0018091.
</p>
<div class="comments">
<p><em>Math.</em> Let us, first, recall what the conditional probability is. The definition we learn in the probability class is the following <span class="math display">\[ {\mathbb{P}}[A | B] = \frac{{\mathbb{P}}[A \text{ and }B]}{{\mathbb{P}}[B]},\]</span> as long as <span class="math inline">\({\mathbb{P}}[B]&gt;0\)</span>. The interpretation is that <span class="math inline">\({\mathbb{P}}[A|B]\)</span> is still the probability of <span class="math inline">\(A\)</span>, but now in the world where <span class="math inline">\(B\)</span> is guaranteed to happen. Conditioning usually happens when we receive new information. If someone tells us that <span class="math inline">\(B\)</span> happened, we can disregard everything in the complement of <span class="math inline">\(B\)</span> and adjust our probability to account for that fact. First we remove from <span class="math inline">\(A\)</span> anything that belongs to the complement of <span class="math inline">\(B\)</span>, and recompute the probability <span class="math inline">\({\mathbb{P}}[A \cap B]\)</span>. We also have to divide by <span class="math inline">\({\mathbb{P}}[B]\)</span> because we want the total probability to be equal to <span class="math inline">\(1\)</span>.</p>
Our code starts as usual, but simulating <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> from the required distribution, and constructing a new vector <span class="math inline">\(Z\)</span> as their sum. The variable <code>X_cond</code> is new; we build it from <span class="math inline">\(X\)</span> by removing all the elements whose corresponding <span class="math inline">\(Z\)</span> is <em>not</em> equal to <span class="math inline">\(5\)</span>. This is an example of what is sometimes called the <strong>rejection method</strong> in simulation. We simply “reject” all simulations which do not satisfy the condition we are conditioning on. We can think of <code>X_cond</code> as bunch of simulations of <span class="math inline">\(X\)</span>, but in the world where <span class="math inline">\(Z=5\)</span> is guaranteed to happen. Once we have <code>X_cond</code>, we proceed as usual by computing the relative frequency of the value <span class="math inline">\(3\)</span> among all possible values <span class="math inline">\(X\)</span> can take. Note that the same <code>X_cond</code> can also be used to compute the conditional probability <span class="math inline">\({\mathbb{P}}[ X=1| Z=5]\)</span>. In fact, <code>X_cond</code> contains the information about the entire <strong>conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(Z=5\)</span></strong>; if we draw a histogram of <code>X_cond</code>, we will get a good idea of what this distribution looks like:
<img src="_main_files/figure-html/unnamed-chunk-112-1.png" width="672" style="display: block; margin: auto;" />
Since <code>X_cond</code> contains only discrete values from <span class="math inline">\(0\)</span> to <span class="math inline">\(5\)</span>, a contingency table might be a better tool for understanding its distribution:
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
0
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
5
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
7745
</td>
<td style="text-align:right;">
7761
</td>
<td style="text-align:right;">
7691
</td>
<td style="text-align:right;">
7807
</td>
<td style="text-align:right;">
7731
</td>
<td style="text-align:right;">
7604
</td>
</tr>
</tbody>
</table>
<p>The histogram and the table above suggest that the distribution of <span class="math inline">\(X\)</span>, given <span class="math inline">\(Z=5\)</span>, is uniform on <span class="math inline">\(\{0,1,2,3,4,5\}\)</span>. It is - a calculation almost identical to the one we performed above gives that <span class="math inline">\({\mathbb{P}}[ X= i| Z=5] = \frac{1}{6}\)</span> for each <span class="math inline">\(i=0,1,2,3,4,5\)</span>.</p>
<p>One more observation at the end. Note that we drew <span class="math inline">\(n=1,000,000\)</span> simulations this time. While it is probably an overkill for this particular example, conditional probabilities in general require more simulations than unconditional ones. Of course, that is because we reject most of our original draws. Indeed, the size of the vector <code>X_cond</code> is 46339 - more than a <span class="math inline">\(20\)</span>-fold decrease. This fact becomes particularly apparent when we try to use Monte Carlo for conditional distributions associated with <em>continuous</em> random vectors as we will see in out next problem.</p>
</div>
<div class="example">
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables where <span class="math inline">\(X\)</span> has the <span class="math inline">\(N(0,1)\)</span> distribution and <span class="math inline">\(Y\)</span> the exponential distribution with parameter <span class="math inline">\(\lambda=1\)</span>. Find a graphical approximation to the conditional density of <span class="math inline">\(Y\)</span>, given <span class="math inline">\(X+Y\geq 1\)</span>. Repeat the same, but condition on <span class="math inline">\(X+Y=1\)</span>.</p>
</div>
<p class="solution">
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="fl">1e+05</span></span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(nsim)</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">rexp</span>(nsim)</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true"></a></span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true"></a>cond =<span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y <span class="op">&gt;=</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true"></a>x_cond =<span class="st"> </span>x[cond]</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true"></a><span class="kw">hist</span>(x_cond, <span class="dt">breaks =</span> <span class="dv">100</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-114-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="fl">1e+05</span></span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true"></a>eps =<span class="st"> </span><span class="fl">0.1</span></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(nsim)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">rexp</span>(nsim)</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true"></a>cond =<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>eps <span class="op">&lt;</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>y) <span class="op">&amp;</span><span class="st"> </span>(x <span class="op">+</span><span class="st"> </span>y <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">+</span><span class="st"> </span>eps)</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true"></a>x_cond =<span class="st"> </span>x[cond]</span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true"></a><span class="kw">hist</span>(x_cond, <span class="dt">breaks =</span> <span class="dv">100</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-115-1.png" width="672" style="display: block; margin: auto;" /></p>
</p>
<div class="comments">
<p><em>Math.</em> In the case of conditioning on <span class="math inline">\(X+Y\geq 1\)</span> we repeated the same procedure as in the discrete case. We simply rejected all draws that do not satisfy the condition.</p>
<p>When Conditioning on <span class="math inline">\(X+Y=1\)</span>, however, you immediately encounter a problem that you don’t get with discrete distributions. The event <span class="math inline">\(\{ X+Y=1\}\)</span> has probability <span class="math inline">\(0\)</span> and will never happen.
That means that our strategy form the previous problem will simply not work - you will reject <strong>all</strong> draws. The problem goes beyond a particular approach to the problem, as the conditional probabilities such as <span class="math inline">\({\mathbb{P}}[ Y \geq 0 | X+Y=1]\)</span> are not well defined. Indeed, the formula
<span class="math display">\[ {\mathbb{P}}[ Y \geq 0 | X+Y=1] &quot;=&quot; \frac{{\mathbb{P}}[ Y\geq 0 \text{ and } X+Y=1]}{ {\mathbb{P}}[X+Y=1]}\]</span>
requires that the probability in the denominator be strictly positive. Otherwise you are dividing by zero. The theoretical solution to this is by no means simple and requires mathematics beyond the scope of these notes. Practically, there is a very simple way of going around it. Instead of conditioning on the zero-probability event <span class="math inline">\(X+Y=1\)</span>, we use a slightly more relaxed condition
<span class="math display">\[ X+Y \in (1-\varepsilon, 1+\varepsilon) \]</span> for a small, but positive, <span class="math inline">\(\varepsilon\)</span>. In many cases of interest, this approximation works very well, as long as <span class="math inline">\(\varepsilon\)</span> is not too big. How big? Well, that will depend on the particular problem, as well as on the number of simulations you are drawing. The best way is to try several values and experiment. For example, if we chose <span class="math inline">\(\varepsilon=0.01\)</span> in our problem, the number of elements in <code>x_cond</code> (i.e., the number of non-rejected draws) would be on the order of <span class="math inline">\(100\)</span>, which may be considered to small to produce an accurate histogram. On the other hand, when <span class="math inline">\(\varepsilon=1\)</span>, your result will be inaccurate because. The rule of thumb is to take the smallest <span class="math inline">\(\varepsilon\)</span> you can, while keeping the number of non-rejected draws sufficiently large.</p>
</div>
</div>
<div id="additional-problems-for-chapter-2" class="section level2" number="2.5">
<h2 number="2.5"><span class="header-section-number">2.5</span> Additional Problems for Chapter 2</h2>
<div class="problem">
<ol style="list-style-type: decimal">
<li><p>Find the Weibull distribution in R’s help system.<!-- ' -->
Simulate <span class="math inline">\(n=10000\)</span> draws from the Weibull distribution with shape parameter <span class="math inline">\(2\)</span> and scale parameter <span class="math inline">\(3\)</span>. Draw a histogram of your simulations.</p></li>
<li><p>Suppose that the vector <code>x</code> contains <span class="math inline">\(n=10000\)</span> simulations from the standard
normal <span class="math inline">\(\mu=0, \sigma=1)\)</span>. Without simulating any new random numbers,
transform it into the vector <code>y</code> such that <code>y</code> is a vector of <span class="math inline">\(n=10000\)</span>
simulations from the normal with <span class="math inline">\(\mu=1\)</span> and <span class="math inline">\(\sigma=0.5\)</span>. Draw histograms
of both <code>x</code> and <code>y</code> on the same plot. (<em>Note:</em> the extra parameter <code>add</code> is
used to superimpose plots. You may want to use different colors, too. Use
the e parameter <code>col</code> for that. )</p></li>
<li><p>Starting with <code>x=seq(-3,3,by=0.1)</code>, define the appropriate vector <code>y</code> and
use <code>x</code> and <code>y</code> to plot the graph of the cdf of the standard normal. The
command you want to use is <code>plot</code> with the following extra arguments</p>
<ul>
<li><code>type="l"</code> (to get a smooth line instead of a bunch of points).</li>
<li><code>main="The CDF of the standard normal"</code> (to set the title), and</li>
<li>another argument (which you must look up yourself) that will set the <span class="math inline">\(y\)</span>-axis label to <span class="math inline">\(F(x)\)</span>.</li>
</ul></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rweibull</span>(<span class="dv">10000</span>, <span class="dt">shape =</span> <span class="dv">2</span>, <span class="dt">scale =</span> <span class="dv">3</span>)</span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true"></a><span class="kw">hist</span>(x)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-268-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><part> 2. </part></p>
<p>Let <span class="math inline">\(X\)</span> be a normally distributed random variable, with parameters <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(\sigma_X\)</span>. When we apply a linear transformation <span class="math inline">\(Y = \alpha X + \beta\)</span> to X, the result <span class="math inline">\(Y\)</span> has a normal distribution again, but with different parameters. These parameters, call them <span class="math inline">\(\mu_Y\)</span> and <span class="math inline">\(\sigma_Y\)</span>, are easily identified by taking the expected value and the variance:</p>
<p><span class="math display">\[\begin{align} \mu_Y &amp; = {\mathbb{E}}[Y] = \alpha {\mathbb{E}}[X] + \beta = \alpha \mu_X + \beta \\
\sigma_Y^2 &amp; = \operatorname{Var}[Y] = \operatorname{Var}[\alpha X + \beta] = \alpha^2 \operatorname{Var}[X] = \alpha^2 \sigma_X^2 
\end{align}\]</span></p>
<p>In the problem we are given <span class="math inline">\(\mu_X=0\)</span> and <span class="math inline">\(\sigma_X=1\)</span>, so we must take <span class="math inline">\(\alpha = 0.5\)</span> and <span class="math inline">\(\beta=1\)</span> to get <span class="math inline">\(\mu_Y=1\)</span> and <span class="math inline">\(\sigma_Y=0.5\)</span> (note that this is
exactly the opposite of taking <span class="math inline">\(z\)</span>-scores, where we transform a general normal into the standard normal). In R</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">10000</span>)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span></code></pre></div>
<p>Let’s check that the parameters of <code>y</code> are as as required:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true"></a>(<span class="kw">mean</span>(y))</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true"></a><span class="co">## [1] 1.002488</span></span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true"></a>(<span class="kw">sd</span>(y))</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true"></a><span class="co">## [1] 0.4989339</span></span></code></pre></div>
<p><part> 3. </part></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dt">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">pnorm</span>(x)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true"></a><span class="kw">plot</span>(x, y, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;F(x)&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;The CDF of the standard normal&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-271-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div class="problem">
<ol style="list-style-type: decimal">
<li><p>Simulate <span class="math inline">\(n=1000\)</span> draws from the distribution whose distribution table is given by</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
<p>2</p>
</th>
<th style="text-align:right;">
<p>4</p>
</th>
<th style="text-align:right;">
<p>8</p>
</th>
<th style="text-align:right;">
<p>16</p>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
<p>0.2</p>
</td>
<td style="text-align:right;">
<p>0.3</p>
</td>
<td style="text-align:right;">
<p>0.1</p>
</td>
<td style="text-align:right;">
<p>0.4</p>
</td>
</tr>
</tbody>
</table>
<p>Draw a histogram of your results.</p></li>
<li><p>You may have learned in probability how to compute the pdf <span class="math inline">\(f_Y(y)\)</span> of a
transformation <span class="math inline">\(Y=g(X)\)</span> of a random variable with pdf <span class="math inline">\(f_X(x)\)</span>. Suppose
that you forgot how to do that, but have access to <span class="math inline">\(10,000\)</span> simulations from
the distribution of <span class="math inline">\(X\)</span>. How would you get an approximate idea about the
shape of the function <span class="math inline">\(f_Y\)</span>?</p>
<p>More concretely, take <span class="math inline">\(X\)</span> to be exponentially distributed with parameter <span class="math inline">\(1\)</span> and
<span class="math inline">\(g(x) = \sin(x)\)</span> and produce a picture that approximates the pdf <span class="math inline">\(f_Y\)</span> of <span class="math inline">\(Y\)</span>.
(Note: even if you remember how to do this analytically, you will run into a
difficulty. The function <span class="math inline">\(\sin(x)\)</span> is not one-to-one and the method usually
taught in probability classes will not apply. If you learned how to do it in the
many-to-one case of <span class="math inline">\(g(x)= \sin(x)\)</span>, kudos to your instructor!)</p></li>
<li><p>Let <span class="math inline">\(X\)</span> be a random variable with the Cauchy distribution, and <span class="math inline">\(Y = \operatorname{arctan}(X)\)</span>. R allows you to simulate from the Cauchy
distribution, even if you do not know what it is. How would you use that to
make an educated guess as to what the distribution of <span class="math inline">\(Y\)</span> is? To make your
life easier, consider <span class="math inline">\(\tfrac{2}{\pi} Y\)</span> first.</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>, <span class="dv">16</span>), <span class="dt">size =</span> <span class="dv">10000</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.4</span>), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true"></a><span class="kw">hist</span>(x)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-273-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Note: given that we are dealing with a discrete distribution,
a contingency table might be a better choice:</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
4
</th>
<th style="text-align:right;">
8
</th>
<th style="text-align:right;">
16
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
2044
</td>
<td style="text-align:right;">
2945
</td>
<td style="text-align:right;">
1045
</td>
<td style="text-align:right;">
3966
</td>
</tr>
</tbody>
</table>
<p><part> 2. </part></p>
<p>We apply the function <span class="math inline">\(\sin\)</span> to the simulations. The histogram of the obtained values is going to be a good (graphical) approximation to the pdf of the transformed random variable:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">100000</span>)</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">sin</span>(x)</span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true"></a><span class="kw">hist</span>(y)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-275-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><part> 3. </part></p>
<p>Having learned that histograms look like the pdfs of the underlying distributions, we draw the histogram:</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rcauchy</span>(<span class="dv">10000</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">atan</span>(x) <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">/</span>pi</span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true"></a><span class="kw">hist</span>(y)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-276-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It looks uniform (if we replace <span class="math inline">\(10,000\)</span> by <span class="math inline">\(100,000\)</span> it will look even more uniform). We conclude that <span class="math inline">\(2/\pi \arctan(X)\)</span> is probably uniformly distributed on <span class="math inline">\((-1,1)\)</span>. Hence, <span class="math inline">\(Y = \arctan(X)\)</span> is probably uniformly distributed on <span class="math inline">\((-\pi/2, \pi/2)\)</span>.</p>
</div>
<div class="problem">
<p>A basic method for obtaining simulations draws from distributions other than the
uniform is the <strong>transformation method</strong>. The idea is to start with (pseudo)
random numbers, i.e., draws from the uniform <span class="math inline">\(U(0,1)\)</span> distribution, and then
apply a function <span class="math inline">\(g\)</span> to each simulation. The difficulty is, of course, how to
choose the right function <span class="math inline">\(g\)</span>.
<br>
Let <span class="math inline">\(X\)</span> be a random variable with a continuous and strictly increasing cdf <span class="math inline">\(F\)</span>.
What is the distribution of <span class="math inline">\(Y=F(X)\)</span>? What does that have to do with the transformation method?</p>
<p>(Hint: if you are having difficulty with this problem, feel free to run some experiments using R. )</p>
</div>
<div class="solution">
<p>Let us perform an experiment where <span class="math inline">\(X \sim N(0,1)\)</span>. Remembering that the cdf is given by the R function <code>pnorm</code>:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">100000</span>)</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">pnorm</span>(x)</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true"></a><span class="kw">hist</span>(y)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-277-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This looks like a histogram of a uniform distribution on <span class="math inline">\((0,1)\)</span>.
Let’s try with some other continuous distributions</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true"></a>x1 =<span class="st"> </span><span class="kw">rexp</span>(<span class="dv">100000</span>)</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true"></a>x2 =<span class="st"> </span><span class="kw">rcauchy</span>(<span class="dv">100000</span>)</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true"></a>x3 =<span class="st"> </span><span class="kw">runif</span>(<span class="dv">100000</span>)</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true"></a>x4 =<span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">100000</span>, <span class="dt">shape =</span> <span class="dv">3</span>)</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true"></a><span class="kw">hist</span>(<span class="kw">pexp</span>(x1))</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true"></a><span class="kw">hist</span>(<span class="kw">pcauchy</span>(x2))</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true"></a><span class="kw">hist</span>(<span class="kw">punif</span>(x3))</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true"></a><span class="kw">hist</span>(<span class="kw">pgamma</span>(x4, <span class="dt">shape =</span> <span class="dv">3</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-278-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>All of those point to the same conjecture, namely that <span class="math inline">\(F(X)\)</span> is
uniformly distributed on <span class="math inline">\((0,1)\)</span>.
To prove that, we
take <span class="math inline">\(Y=F(X)\)</span> and try to compute that cdf <span class="math inline">\(F_Y\)</span> of <span class="math inline">\(Y\)</span>:
<span class="math display">\[F_Y(y) = {\mathbb{P}}[ Y \leq y] = {\mathbb{P}}[ F(X) \leq y]\]</span>
Since <span class="math inline">\(F\)</span> is strictly increasing, it admits an inverse <span class="math inline">\(F^{-1}\)</span>.
Moreover, for any <span class="math inline">\(y \in (0,1)\)</span>,
the set of all values of <span class="math inline">\(x\)</span> such that <span class="math inline">\(F(x)\leq y\)</span> (the red range) is
exactly the interval <span class="math inline">\((-\infty, F^{-1}(y)]\)</span> (the blue range), as in
the picture below:</p>
<center>
<img src="pics/cdf_plot.png" width="60%" style="padding:10px" style="display: block; margin: auto;" />
</center>
<p>Hence, <span class="math display">\[F_Y(y)={\mathbb{P}}[Y\leq y] = {\mathbb{P}}[ F(X) \leq y] = {\mathbb{P}}[ X \leq F^{-1}(y) ] = F(F^{-1}(y)) = y, 
\text{ for } y\in (0,1).\]</span> The cdf <span class="math inline">\(F_Y\)</span> is, therefore, equal to the cdf
of a uniform on <span class="math inline">\((0,1)\)</span>.
Since the cdf uniquely determines the distribution, <span class="math inline">\(Y\)</span> must be uniformly
distributed on <span class="math inline">\((0,1)\)</span>.</p>
</div>
<div class="problemec">
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> be two pdfs. We take a constant <span class="math inline">\(\alpha \in (0,1)\)</span> and define the function <span class="math inline">\(f\)</span> by
<span class="math display">\[ f(x) = \alpha f_1(x) + (1-\alpha) f_2(x).\]</span>
The function <span class="math inline">\(f\)</span> is the pdf of a third distribution, which is called the
<strong>mixture of <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> with weights <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(1-\alpha\)</span></strong>.
Assuming that you know how to simulate from the distributions with pdfs <span class="math inline">\(f_1\)</span>
and <span class="math inline">\(f_2\)</span>, how would you draw <span class="math inline">\(10,000\)</span> simulations from the mixture <span class="math inline">\(f\)</span>? Show
your method on the example of a mixture of <span class="math inline">\(N(0,1)\)</span> and <span class="math inline">\(N(4,1)\)</span> with
<span class="math inline">\(\alpha=2/3\)</span>. Plot the histogram of the obtained sample (play with the
parameter <code>breaks</code> until you get a nice picture.)</p>
<p>(<em>Hint:</em> start with two vectors, the first containing <span class="math inline">\(10,000\)</span> simulations from
<span class="math inline">\(f_1\)</span> and the second from <span class="math inline">\(f_2\)</span>. Then “toss” <span class="math inline">\(10,000\)</span> biased coins with
<span class="math inline">\(\mathbb{P}[ H ] = \alpha\)</span> … )</p></li>
<li><p>The <strong>double exponential</strong> or
<strong>Laplace</strong> distribution is a continuous probability distribution whose pdf is
given by <span class="math display">\[ \tfrac{1}{2} \exp(-|x|), x\in {\mathbb R}.\]</span> This distribution
is not built into R. How would you produce simulations from the double
exponential using R?</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<p>The idea is that before each draw a biased coin (with <span class="math inline">\({\mathbb{P}}[H]=\alpha\)</span>) is tossed. If <span class="math inline">\(H\)</span> is obtained, we draw from the distribution with pdf <span class="math inline">\(f_1\)</span>. Otherwise, we draw from the distribution with pdf <span class="math inline">\(f_2\)</span>. We write a function which performs one such simulation, and then use the command <code>replicate</code> to call it several times and store the results in the vector:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true"></a>single_draw =<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true"></a>    coin =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">prob =</span> <span class="kw">c</span>(<span class="dv">2</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>), <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true"></a>    <span class="cf">if</span> (coin <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) </span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true"></a>        <span class="kw">return</span>(<span class="kw">rnorm</span>(<span class="dv">1</span>)) <span class="cf">else</span> <span class="kw">return</span>(<span class="kw">rnorm</span>(<span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">4</span>, <span class="dt">sd =</span> <span class="dv">1</span>))</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true"></a>}</span>
<span id="cb101-6"><a href="#cb101-6" aria-hidden="true"></a></span>
<span id="cb101-7"><a href="#cb101-7" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb101-8"><a href="#cb101-8" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">replicate</span>(nsim, <span class="kw">single_draw</span>())</span>
<span id="cb101-9"><a href="#cb101-9" aria-hidden="true"></a></span>
<span id="cb101-10"><a href="#cb101-10" aria-hidden="true"></a><span class="kw">hist</span>(y)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-280-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As you can see, the histogram has two “humps”, one centered around <span class="math inline">\(0\)</span> and the other
centered around <span class="math inline">\(4\)</span>. The first one is taller, which reflects the higher
weight (<span class="math inline">\(\alpha=2/3\)</span>) that <span class="math inline">\(N(0,1)\)</span> has in this mixture.</p>
<p>If you wanted to write a more succinct vectorized code (which is not necessarily faster in this case), you could also do something like this</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true"></a>alpha =<span class="st"> </span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span></span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true"></a>x1 =<span class="st"> </span><span class="kw">rnorm</span>(nsim)</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true"></a>x2 =<span class="st"> </span><span class="kw">rnorm</span>(nsim, <span class="dt">mean =</span> <span class="dv">4</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true"></a>coin =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="ot">TRUE</span>, <span class="ot">FALSE</span>), <span class="dt">size =</span> nsim, <span class="dt">prob =</span> <span class="kw">c</span>(alpha, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>alpha), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">ifelse</span>(coin, x1, x2)</span></code></pre></div>
<p>The function <code>ifelse</code> is a vectorized version of the <code>if-then</code> blok and takes three arguments of equal length. The first one is a vector of logical values <code>c</code>, and the other two, <code>x1, x2</code> only need to be of the same type. The result of is a vector whose value at the position <code>i</code> is <code>x1[i]</code> if <code>c[i]==TRUE</code> and <code>x2[i]</code> otherwise.</p>
<p><part> 2. </part></p>
<p>The Laplace distribution can be understood as a mixture, with <span class="math inline">\(\alpha=1/2\)</span> of two distributions. The first one is an exponential, and the second one is the exponential, but with the negative sign.
Using our strategy from part 1. above, we could get simulations of it as follows:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">100000</span></span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true"></a>alpha =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span></span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true"></a>x1 =<span class="st"> </span><span class="kw">rexp</span>(nsim)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true"></a>x2 =<span class="st"> </span><span class="op">-</span><span class="kw">rexp</span>(nsim)  <span class="co"># note the minus sign in front of rexp</span></span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true"></a>coin =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="ot">TRUE</span>, <span class="ot">FALSE</span>), <span class="dt">size =</span> nsim, <span class="dt">prob =</span> <span class="kw">c</span>(alpha, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>alpha), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">ifelse</span>(coin, x1, x2)</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true"></a><span class="kw">hist</span>(y)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-282-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>You can do this more efficiently if you realize that every time we toss a coin and choose between
<code>x1</code> and <code>x2</code>, we are really choosing the sign in front of an exponentially distributed random
variable. In other words, we can use <code>coin</code> as
a vector of random signs for a vector or draws from the exponential distribution:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb104-2"><a href="#cb104-2" aria-hidden="true"></a>alpha =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span></span>
<span id="cb104-3"><a href="#cb104-3" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rexp</span>(nsim)</span>
<span id="cb104-4"><a href="#cb104-4" aria-hidden="true"></a>coin =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="dt">size =</span> nsim, <span class="dt">prob =</span> <span class="kw">c</span>(alpha, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>alpha), <span class="dt">replace =</span> <span class="ot">TRUE</span>)</span>
<span id="cb104-5"><a href="#cb104-5" aria-hidden="true"></a>y =<span class="st"> </span>coin <span class="op">*</span><span class="st"> </span>x</span>
<span id="cb104-6"><a href="#cb104-6" aria-hidden="true"></a><span class="kw">hist</span>(y)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-283-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div class="problem">
<p>Let <code>x=rnorm(1000)</code> and <code>y=rnorm(1000)</code>. For each of the following pairs, use the permutation test to decide whether they are independent or not</p>
<ol style="list-style-type: lower-alpha">
<li><code>x^2+y^2</code> and <code>y^2</code></li>
<li><code>(x+y)/sqrt(2)</code> and <code>(x-y)/sqrt(2)</code></li>
<li><code>x</code> and <code>1</code></li>
<li><code>x^2+y^2</code> and <code>atan(y/x)</code>.</li>
</ol>
<p>What your conclusions in b. and d. suggest about the geometric properties of the distribution of <span class="math inline">\((X,Y)\)</span> where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent with standard normal distributions?</p>
<p>(Note: do not worry about dividing by <span class="math inline">\(0\)</span> in d. It will happen with probability <span class="math inline">\(0\)</span>.)</p>
</div>
<div class="solution">
<p>Let us start by writing a function to save some keystrokes</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true"></a>permutation_test =<span class="st"> </span><span class="cf">function</span>(z, w) {</span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true"></a>    <span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true"></a>    <span class="kw">plot</span>(z, w, <span class="dt">asp =</span> <span class="dv">1</span>)</span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true"></a>    <span class="kw">plot</span>(z, <span class="kw">sample</span>(w), <span class="dt">asp =</span> <span class="dv">1</span>)</span>
<span id="cb105-5"><a href="#cb105-5" aria-hidden="true"></a>    <span class="kw">plot</span>(z, <span class="kw">sample</span>(w), <span class="dt">asp =</span> <span class="dv">1</span>)</span>
<span id="cb105-6"><a href="#cb105-6" aria-hidden="true"></a>    <span class="kw">plot</span>(z, <span class="kw">sample</span>(w), <span class="dt">asp =</span> <span class="dv">1</span>)</span>
<span id="cb105-7"><a href="#cb105-7" aria-hidden="true"></a>}</span>
<span id="cb105-8"><a href="#cb105-8" aria-hidden="true"></a></span>
<span id="cb105-9"><a href="#cb105-9" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)</span>
<span id="cb105-10"><a href="#cb105-10" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)</span></code></pre></div>
<p><part> 1. </part></p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true"></a><span class="kw">permutation_test</span>(x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>, y<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-285-1.png" width="768" style="display: block; margin: auto;" />
The first plot is very different from the other three. Therefore,the vectors are probably <em>not</em> independent.</p>
<p><part> 2. </part></p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" aria-hidden="true"></a><span class="kw">permutation_test</span>((x <span class="op">+</span><span class="st"> </span>y)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span>), (x <span class="op">-</span><span class="st"> </span>y)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-286-1.png" width="768" style="display: block; margin: auto;" />
The first plot could easily be confused for one of the other three. Therefore the vectors are probably independent.</p>
<p><part> 3. </part></p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true"></a><span class="co"># we have to use rep(1,length(x)) to get a vector of 1s of the same length as x.</span></span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true"></a><span class="co"># R will not recycle it properly if you simply write 1</span></span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true"></a><span class="kw">permutation_test</span>(x, <span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(x)))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-287-1.png" width="768" style="display: block; margin: auto;" />
The plots look very similar. Therefore, the vectors are probably independent. We could have known this without drawing any graphs. Anything is independent of a constant random variable (vector).</p>
<p><part> 4. </part></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true"></a><span class="kw">permutation_test</span>(x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>, <span class="kw">atan</span>(y<span class="op">/</span>x))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-288-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Plots look very similar to each other. Therefore, <code>z</code> and <code>w</code> are probably independent.</p>
<p>These plots in b) and d) reveal that the distribution of the random vector <span class="math inline">\((X,Y)\)</span> consisting of two independent standard normals is probably rotation invariant. In b) we are asked to compare the coordinates of the vector obtained from <span class="math inline">\((X,Y)\)</span> by a rotation at <span class="math inline">\(45\)</span> degrees around the origin. The fact that independence persisted suggests that components remain independent even after a (specific) rotation. If you tried rotations by different angles you would get the same result. The experiment in d) told us that the (squared) distance <span class="math inline">\(X^2+Y^2\)</span> and angle between <span class="math inline">\((X,Y)\)</span> and the <span class="math inline">\(x\)</span>-are independent. This is also something that one would expect from a rotationally-invariant distribution. Indeed, the distribution of the distance to the origin should not depend on the direction.</p>
<p>It is important to note that none of this proves anything. It is simply numerical evidence for a given conclusion.</p>
</div>
<div class="problem">
<p>Simulate <span class="math inline">\(n=10000\)</span> draws from the joint distribution given by the following
table:</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
1
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.3
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
2
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.0
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
3
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.4
</td>
</tr>
</tbody>
</table>
<p>Display the contingency table of your results, as well as a table showing the
“errors”, i.e., differences between the theoretical frequencies (i.e.,
probabilities given above) and the obtained relative frequencies in the sample.</p>
</div>
<div class="solution">
<p>We are using the procedure from Section 2.2 in the notes.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb110-2"><a href="#cb110-2" aria-hidden="true"></a></span>
<span id="cb110-3"><a href="#cb110-3" aria-hidden="true"></a>joint_distribution_long =<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb110-4"><a href="#cb110-4" aria-hidden="true"></a>   <span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>),</span>
<span id="cb110-5"><a href="#cb110-5" aria-hidden="true"></a>   <span class="dt">y =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb110-6"><a href="#cb110-6" aria-hidden="true"></a>   )</span>
<span id="cb110-7"><a href="#cb110-7" aria-hidden="true"></a>probabilities_long =</span>
<span id="cb110-8"><a href="#cb110-8" aria-hidden="true"></a><span class="st">   </span><span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.0</span>, <span class="fl">0.3</span>,</span>
<span id="cb110-9"><a href="#cb110-9" aria-hidden="true"></a>     <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.0</span>,</span>
<span id="cb110-10"><a href="#cb110-10" aria-hidden="true"></a>     <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="fl">0.4</span>)</span>
<span id="cb110-11"><a href="#cb110-11" aria-hidden="true"></a></span>
<span id="cb110-12"><a href="#cb110-12" aria-hidden="true"></a>sampled_rows =<span class="st"> </span><span class="kw">sample</span>(</span>
<span id="cb110-13"><a href="#cb110-13" aria-hidden="true"></a>   <span class="dt">x =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(joint_distribution_long),</span>
<span id="cb110-14"><a href="#cb110-14" aria-hidden="true"></a>   <span class="dt">size =</span> nsim,</span>
<span id="cb110-15"><a href="#cb110-15" aria-hidden="true"></a>   <span class="dt">replace =</span> <span class="ot">TRUE</span>,</span>
<span id="cb110-16"><a href="#cb110-16" aria-hidden="true"></a>   <span class="dt">prob =</span> probabilities_long</span>
<span id="cb110-17"><a href="#cb110-17" aria-hidden="true"></a>)</span>
<span id="cb110-18"><a href="#cb110-18" aria-hidden="true"></a></span>
<span id="cb110-19"><a href="#cb110-19" aria-hidden="true"></a>draws =<span class="st"> </span>joint_distribution_long[sampled_rows,]</span>
<span id="cb110-20"><a href="#cb110-20" aria-hidden="true"></a></span>
<span id="cb110-21"><a href="#cb110-21" aria-hidden="true"></a>(<span class="dt">freq =</span> <span class="kw">table</span>(draws))</span>
<span id="cb110-22"><a href="#cb110-22" aria-hidden="true"></a><span class="co">##    y</span></span>
<span id="cb110-23"><a href="#cb110-23" aria-hidden="true"></a><span class="co">## x      1    2    3</span></span>
<span id="cb110-24"><a href="#cb110-24" aria-hidden="true"></a><span class="co">##   1  946    0 3044</span></span>
<span id="cb110-25"><a href="#cb110-25" aria-hidden="true"></a><span class="co">##   2 1015 1018    0</span></span>
<span id="cb110-26"><a href="#cb110-26" aria-hidden="true"></a><span class="co">##   3    0    0 3977</span></span>
<span id="cb110-27"><a href="#cb110-27" aria-hidden="true"></a></span>
<span id="cb110-28"><a href="#cb110-28" aria-hidden="true"></a>(<span class="dt">rel_freq =</span> <span class="kw">table</span>(draws) <span class="op">/</span><span class="st"> </span>nsim)</span>
<span id="cb110-29"><a href="#cb110-29" aria-hidden="true"></a><span class="co">##    y</span></span>
<span id="cb110-30"><a href="#cb110-30" aria-hidden="true"></a><span class="co">## x        1      2      3</span></span>
<span id="cb110-31"><a href="#cb110-31" aria-hidden="true"></a><span class="co">##   1 0.0946 0.0000 0.3044</span></span>
<span id="cb110-32"><a href="#cb110-32" aria-hidden="true"></a><span class="co">##   2 0.1015 0.1018 0.0000</span></span>
<span id="cb110-33"><a href="#cb110-33" aria-hidden="true"></a><span class="co">##   3 0.0000 0.0000 0.3977</span></span>
<span id="cb110-34"><a href="#cb110-34" aria-hidden="true"></a></span>
<span id="cb110-35"><a href="#cb110-35" aria-hidden="true"></a>(<span class="dt">th_freq =</span> <span class="kw">matrix</span>(probabilities_long, <span class="dt">byrow =</span> <span class="ot">TRUE</span>, <span class="dt">nrow =</span> <span class="dv">3</span>))</span>
<span id="cb110-36"><a href="#cb110-36" aria-hidden="true"></a><span class="co">##      [,1] [,2] [,3]</span></span>
<span id="cb110-37"><a href="#cb110-37" aria-hidden="true"></a><span class="co">## [1,]  0.1  0.0  0.3</span></span>
<span id="cb110-38"><a href="#cb110-38" aria-hidden="true"></a><span class="co">## [2,]  0.1  0.1  0.0</span></span>
<span id="cb110-39"><a href="#cb110-39" aria-hidden="true"></a><span class="co">## [3,]  0.0  0.0  0.4</span></span>
<span id="cb110-40"><a href="#cb110-40" aria-hidden="true"></a></span>
<span id="cb110-41"><a href="#cb110-41" aria-hidden="true"></a>(<span class="dt">err =</span> rel_freq <span class="op">-</span><span class="st"> </span>th_freq)</span>
<span id="cb110-42"><a href="#cb110-42" aria-hidden="true"></a><span class="co">##    y</span></span>
<span id="cb110-43"><a href="#cb110-43" aria-hidden="true"></a><span class="co">## x         1       2       3</span></span>
<span id="cb110-44"><a href="#cb110-44" aria-hidden="true"></a><span class="co">##   1 -0.0054  0.0000  0.0044</span></span>
<span id="cb110-45"><a href="#cb110-45" aria-hidden="true"></a><span class="co">##   2  0.0015  0.0018  0.0000</span></span>
<span id="cb110-46"><a href="#cb110-46" aria-hidden="true"></a><span class="co">##   3  0.0000  0.0000 -0.0023</span></span></code></pre></div>
</div>
<div class="problem">
<p>Estimate the following integrals using Monte Carlo</p>
<ol style="list-style-type: lower-alpha">
<li><p><span class="math inline">\(\int_0^1 \cos(x)\, dx\)</span></p></li>
<li><p><span class="math inline">\(\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}\frac{e^{-x^2/2}}{1+x^4}\,dx\)</span></p></li>
<li><p><span class="math inline">\(\int_0^{\infty} e^{-x^3-x}\, dx\)</span></p></li>
<li><p><span class="math inline">\(\int_{-\infty}^{\infty} \frac{\cos(x^2)}{1+x^2}\, dx\)</span> (extra credit)</p></li>
</ol>
</div>
<div class="solution">
<p>The idea here is to use the “fundamental theorem of statistics”
<span class="math display">\[ {\mathbb{E}}[ g(X) ] = \int g(x)\, f_X(x)\, dx \]</span>
where <span class="math inline">\(f_X\)</span> is the pdf of <span class="math inline">\(X\)</span> and <span class="math inline">\(g\)</span> is any reasonably well-behaved function.
Normally, one would use the integral on the right to compute the expectation
on the left. We are flipping the logic, and using the expectation
(which we can approximate via Monte Carlo) to estimate the integral on the
right.</p>
<p><part> a) </part></p>
<p>We pick <span class="math inline">\(g(x) = \cos(x)\)</span> and <span class="math inline">\(X\)</span> a r.v. with a uniform distribution on <span class="math inline">\((0,1)\)</span>, so that <span class="math inline">\(f_X(x) = 1\)</span> for <span class="math inline">\(x\in (0,1)\)</span> and <span class="math inline">\(0\)</span> otherwise:</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb111-2"><a href="#cb111-2" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">runif</span>(nsim)</span>
<span id="cb111-3"><a href="#cb111-3" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">cos</span>(x)</span>
<span id="cb111-4"><a href="#cb111-4" aria-hidden="true"></a><span class="kw">mean</span>(y)</span>
<span id="cb111-5"><a href="#cb111-5" aria-hidden="true"></a><span class="co">## [1] 0.841413</span></span></code></pre></div>
<p>For comparison, the exact value of the integral is <span class="math inline">\(\sin(1) \approx 0.841471\)</span>.</p>
<p><part> b) </part></p>
<p>We cannot use the uniform distribution anymore, because the limits of integration are <span class="math inline">\(\pm \infty\)</span>. Part of the expression inside the integral can be recognized as a (standard) normal density, so we take <span class="math inline">\(X \sim N(0,1)\)</span> and <span class="math inline">\(g(x) = 1/(1+x^4)\)</span></p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb112-2"><a href="#cb112-2" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rnorm</span>(nsim)</span>
<span id="cb112-3"><a href="#cb112-3" aria-hidden="true"></a>y =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x<span class="op">^</span><span class="dv">4</span>)</span>
<span id="cb112-4"><a href="#cb112-4" aria-hidden="true"></a><span class="kw">mean</span>(y)</span>
<span id="cb112-5"><a href="#cb112-5" aria-hidden="true"></a><span class="co">## [1] 0.6790206</span></span></code></pre></div>
<p>The “exact” value (i.e., very precise approximation to this integral obtained using another numerical method) is <span class="math inline">\(0.676763\)</span>.</p>
<p><part> c) </part></p>
<p>We integrate <span class="math inline">\(g(x) = \exp(-x^3)\)</span> against the exponential pdf <span class="math inline">\(f_X(x) = \exp(-x)\)</span>, for <span class="math inline">\(x&gt;0\)</span>:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rexp</span>(nsim)</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x<span class="op">^</span><span class="dv">3</span>)</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true"></a><span class="kw">mean</span>(y)</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true"></a><span class="co">## [1] 0.5678835</span></span></code></pre></div>
<p>A close approximation of the true value is <span class="math inline">\(0.56889\)</span>.</p>
<p><part> d) </part></p>
<p>In this case, a possible choice of the distribution for <span class="math inline">\(X\)</span> is the Cauchy distribution (no worries if you never heard about it), whose pdf is <span class="math inline">\(f_X(x) = \frac{1}{\pi(1+x^2)}\)</span>, so that <span class="math inline">\(g(x) = \pi \cos(x^2)\)</span>:</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">rcauchy</span>(nsim)</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true"></a>y =<span class="st"> </span>pi <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(x<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true"></a><span class="kw">mean</span>(y)</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true"></a><span class="co">## [1] 1.304995</span></span></code></pre></div>
<p>The “exact” value is <span class="math inline">\(1.30561\)</span>.</p>
</div>
<div class="problem">
The <strong>tricylinder</strong> is a solid body constructed as follows: create three
cylinders of radius 1 around each of the three coordinate axes and intersect
them:
<figure style="
    display: flex;
    flex-flow: column;
    padding: 5px;
    margin: auto;">
<img src="pics/tricylinder.png" style="border-bottom: -10px;"></img>
<figcaption style=" 
    background-color: #fff;
    color: #000;
    font: italic smaller sans-serif;
    padding: 3px;
    text-align: center;">
Image by Ag2gaeh - Own work, CC BY-SA 4.0, <a href="https://commons.wikimedia.org/w/index.php?curid=63604565" class="uri">https://commons.wikimedia.org/w/index.php?curid=63604565</a>
</figcaption>
</figure>
Use Monte Carlo to estimate the volume of the tricylinder and
check your estimate against the exact value <span class="math inline">\(8(2-\sqrt{2})\)</span>.
</p>
</div>
<div class="solution">
<p>By the very construction, it is clear that the entire tricylinder lies within the cube <span class="math inline">\([-1,1]\times [-1,1] \times [-1,1]\)</span>. Therefore, we can compute its volume by simulating random draws from the uniform distribution in that cube, and
computing the relative frequence of those values that fall inside the tricylinder. The whole point is that it is easy to check, given a point <span class="math inline">\((x,y,z)\)</span>, whether it lies inside the tricylinder or not. Indeed, the answer is “yes” if and only if all three of the following inequalities are satisfied:
<span class="math display">\[ x^2+y^2 \le 1,\  x^2+z^2\leq 1 \text{ and } y^2+z^2\leq 1.\]</span></p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true"></a>x =<span class="st"> </span><span class="kw">runif</span>(nsim, <span class="dt">min =</span> <span class="dv">-1</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb115-3"><a href="#cb115-3" aria-hidden="true"></a>y =<span class="st"> </span><span class="kw">runif</span>(nsim, <span class="dt">min =</span> <span class="dv">-1</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb115-4"><a href="#cb115-4" aria-hidden="true"></a>z =<span class="st"> </span><span class="kw">runif</span>(nsim, <span class="dt">min =</span> <span class="dv">-1</span>, <span class="dt">max =</span> <span class="dv">1</span>)</span>
<span id="cb115-5"><a href="#cb115-5" aria-hidden="true"></a>is_in =<span class="st"> </span>(x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span> <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>) <span class="op">&amp;</span><span class="st"> </span>(x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>z<span class="op">^</span><span class="dv">2</span> <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>) <span class="op">&amp;</span><span class="st"> </span>(y<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>z<span class="op">^</span><span class="dv">2</span> <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>)</span>
<span id="cb115-6"><a href="#cb115-6" aria-hidden="true"></a>(<span class="dv">2</span><span class="op">^</span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(is_in)<span class="op">/</span>nsim)</span>
<span id="cb115-7"><a href="#cb115-7" aria-hidden="true"></a><span class="co">## [1] 4.692</span></span></code></pre></div>
<p>We multiplied by <span class="math inline">\(2^3\)</span> because that is the volume of the cube <span class="math inline">\([-1,1]\times [-1,1] \times [-1,1]\)</span>. Without it, we would get the portion of the cube taken by the tricylinder, and not its volume.</p>
<p>The true value of <span class="math inline">\(8(2-\sqrt{2})\)</span> is, approximately, <span class="math inline">\(4.6862\)</span>.</p>
</div>
<div class="problemec">
<p>Read about the <strong>Monty Hall Problem</strong> online (the introduction to its
<a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">Wikipedia page</a> has a nice description),
Use Monte Carlo to compare the two possible strategies (switching and
not-switching) and decide which is better.</p>
</div>
<div class="solution">
<p>The host knows where the car is and what contestant’s guess is. If those two are the same (i.e., contestant guessed right), he will choose one of the two remaining doors at random. If not, he simply shows the contestant the other door with the goat behind it. This exactly what the function <code>show_door</code> implements:</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" aria-hidden="true"></a>show_door =<span class="st"> </span><span class="cf">function</span>(car, guess) {</span>
<span id="cb116-2"><a href="#cb116-2" aria-hidden="true"></a>    all_doors =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb116-3"><a href="#cb116-3" aria-hidden="true"></a>    goat_doors =<span class="st"> </span>all_doors[all_doors <span class="op">!=</span><span class="st"> </span>car]</span>
<span id="cb116-4"><a href="#cb116-4" aria-hidden="true"></a>    </span>
<span id="cb116-5"><a href="#cb116-5" aria-hidden="true"></a>    <span class="cf">if</span> (car <span class="op">==</span><span class="st"> </span>guess) {</span>
<span id="cb116-6"><a href="#cb116-6" aria-hidden="true"></a>        random_goat_door =<span class="st"> </span><span class="kw">sample</span>(goat_doors, <span class="dt">size =</span> <span class="dv">1</span>)</span>
<span id="cb116-7"><a href="#cb116-7" aria-hidden="true"></a>        <span class="kw">return</span>(random_goat_door)</span>
<span id="cb116-8"><a href="#cb116-8" aria-hidden="true"></a>    } <span class="cf">else</span> {</span>
<span id="cb116-9"><a href="#cb116-9" aria-hidden="true"></a>        the_other_goat_door =<span class="st"> </span>goat_doors[goat_doors <span class="op">!=</span><span class="st"> </span>guess]</span>
<span id="cb116-10"><a href="#cb116-10" aria-hidden="true"></a>        <span class="kw">return</span>(the_other_goat_door)</span>
<span id="cb116-11"><a href="#cb116-11" aria-hidden="true"></a>    }</span>
<span id="cb116-12"><a href="#cb116-12" aria-hidden="true"></a>}</span></code></pre></div>
<p>Next, we write a function which simulates the outcome of a single game. It will have one argument, <code>switch</code> which will determine whether the contestant switches the door or not.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true"></a>one_game =<span class="st"> </span><span class="cf">function</span>(<span class="cf">switch</span>) {</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true"></a>    all_doors =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb117-3"><a href="#cb117-3" aria-hidden="true"></a>    car =<span class="st"> </span><span class="kw">sample</span>(all_doors, <span class="dt">size =</span> <span class="dv">1</span>)</span>
<span id="cb117-4"><a href="#cb117-4" aria-hidden="true"></a>    guess =<span class="st"> </span><span class="kw">sample</span>(all_doors, <span class="dt">size =</span> <span class="dv">1</span>)</span>
<span id="cb117-5"><a href="#cb117-5" aria-hidden="true"></a>    </span>
<span id="cb117-6"><a href="#cb117-6" aria-hidden="true"></a>    <span class="cf">if</span> (<span class="cf">switch</span>) {</span>
<span id="cb117-7"><a href="#cb117-7" aria-hidden="true"></a>        unguessed_doors =<span class="st"> </span>all_doors[all_doors <span class="op">!=</span><span class="st"> </span>guess]</span>
<span id="cb117-8"><a href="#cb117-8" aria-hidden="true"></a>        shown_door =<span class="st"> </span><span class="kw">show_door</span>(car, guess)</span>
<span id="cb117-9"><a href="#cb117-9" aria-hidden="true"></a>        switched_guess =<span class="st"> </span>unguessed_doors[unguessed_doors <span class="op">!=</span><span class="st"> </span>shown_door]</span>
<span id="cb117-10"><a href="#cb117-10" aria-hidden="true"></a>        <span class="kw">return</span>(switched_guess <span class="op">==</span><span class="st"> </span>car)</span>
<span id="cb117-11"><a href="#cb117-11" aria-hidden="true"></a>    } <span class="cf">else</span> {</span>
<span id="cb117-12"><a href="#cb117-12" aria-hidden="true"></a>        <span class="kw">return</span>(guess <span class="op">==</span><span class="st"> </span>car)</span>
<span id="cb117-13"><a href="#cb117-13" aria-hidden="true"></a>    }</span>
<span id="cb117-14"><a href="#cb117-14" aria-hidden="true"></a>}</span></code></pre></div>
<p>Finally we run two batches of <span class="math inline">\(10,000\)</span> simulations, one with <code>switch=TRUE</code> and another with <code>switch=FALSE</code>:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb118-2"><a href="#cb118-2" aria-hidden="true"></a>switch_doors =<span class="st"> </span><span class="kw">replicate</span>(nsim, <span class="kw">one_game</span>(<span class="ot">TRUE</span>))</span>
<span id="cb118-3"><a href="#cb118-3" aria-hidden="true"></a>dont_switch_doors =<span class="st"> </span><span class="kw">replicate</span>(nsim, <span class="kw">one_game</span>(<span class="ot">FALSE</span>))</span>
<span id="cb118-4"><a href="#cb118-4" aria-hidden="true"></a>(<span class="dt">prob_with_switching =</span> <span class="kw">mean</span>(switch_doors))</span>
<span id="cb118-5"><a href="#cb118-5" aria-hidden="true"></a><span class="co">## [1] 0.6568</span></span>
<span id="cb118-6"><a href="#cb118-6" aria-hidden="true"></a>(<span class="dt">prob_without_switching =</span> <span class="kw">mean</span>(dont_switch_doors))</span>
<span id="cb118-7"><a href="#cb118-7" aria-hidden="true"></a><span class="co">## [1] 0.3282</span></span></code></pre></div>
<p>Therefore, the probability of winning after switching is about double the probability of winning without switching. Switching is good for you!</p>
<p>(A philosophical note: this was the most “agnostic” approach to this simulation. Simulations can often be simplified with a bit of insight. For example, we could have realized that the switching strategy simply flips the correctness of the guess (from “correct” to “wrong” and vice versa) and used it to write a much shorter answer. Ultimately, we could have realized that, because the probability of the initial guess being correct is <span class="math inline">\(1/3\)</span>, switching leads to a correct guess in <span class="math inline">\(2/3\)</span> of the cases (and not switching in only <span class="math inline">\(1/3\)</span> of the cases). In this case, the whole code would be <code>sample(c("correct", "incorrect"), size=10000, prob= c(2/3,1/3), replacement=TRUE)</code>, which is an extremely inefficient way to estimate the value of the number <span class="math inline">\(2/3\)</span>!)</p>
</div>
<div class="problem">
<p>We learned how to simulate from a joint distribution of two discrete vectors
<span class="math inline">\((X,Y\)</span>) by thinking of it as one-dimensional distribution but with values
represented by pairs of numbers. Here is another way this can be done:</p>
<ol style="list-style-type: decimal">
<li><p>Find the marginal distribution of one of them, say <span class="math inline">\(X\)</span>, and simulate from it</p></li>
<li><p>Given the value you just obtained, let’s <!--'--> call it <span class="math inline">\(x\)</span>, simulate from the conditional distribution of <span class="math inline">\(Y\)</span>, given <span class="math inline">\(X=x\)</span>.</p></li>
</ol>
Implement this procedure on the following joint distribution:
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:right;">
1
</th>
<th style="text-align:right;">
2
</th>
<th style="text-align:right;">
3
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;">
1
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.3
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
2
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.0
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
3
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.0
</td>
<td style="text-align:right;">
0.4
</td>
</tr>
</tbody>
</table>
<p>Display the contingency table of your simulations, first using counts, and then
using relative frequencies. Compare to the theoretical values (i.e., the
probabilities in the table above).</p>
</div>
<div class="solution">
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true"></a>margin_X =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.2</span>, <span class="fl">0.1</span>, <span class="fl">0.7</span> )</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true"></a>cond_Y_X =<span class="st"> </span><span class="kw">matrix</span>(</span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true"></a>   <span class="kw">c</span>( <span class="fl">0.5</span>, <span class="fl">0.0</span>, <span class="dv">3</span><span class="op">/</span><span class="dv">7</span>,</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true"></a>      <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.0</span>,</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true"></a>      <span class="fl">0.0</span>, <span class="fl">0.0</span>, <span class="dv">4</span><span class="op">/</span><span class="dv">7</span>),</span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true"></a>   <span class="dt">byrow=</span><span class="ot">TRUE</span>,</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true"></a>   <span class="dt">nrow=</span><span class="dv">3</span>)</span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true"></a></span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true"></a>single_draw =<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true"></a>   x =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>margin_X)</span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true"></a>   y =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span>cond_Y_X[,x])</span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true"></a>   <span class="kw">return</span>(<span class="kw">c</span>(x,y))</span>
<span id="cb119-13"><a href="#cb119-13" aria-hidden="true"></a>}</span>
<span id="cb119-14"><a href="#cb119-14" aria-hidden="true"></a></span>
<span id="cb119-15"><a href="#cb119-15" aria-hidden="true"></a>nsim=<span class="dv">10000</span></span>
<span id="cb119-16"><a href="#cb119-16" aria-hidden="true"></a>df =<span class="st"> </span><span class="kw">data.frame</span>(</span>
<span id="cb119-17"><a href="#cb119-17" aria-hidden="true"></a>   <span class="kw">t</span>(<span class="kw">replicate</span>(nsim, <span class="kw">single_draw</span>()))</span>
<span id="cb119-18"><a href="#cb119-18" aria-hidden="true"></a>)</span>
<span id="cb119-19"><a href="#cb119-19" aria-hidden="true"></a><span class="kw">colnames</span>(df) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;x&quot;</span>,<span class="st">&quot;y&quot;</span>)</span>
<span id="cb119-20"><a href="#cb119-20" aria-hidden="true"></a><span class="kw">t</span>(<span class="kw">table</span>(df))</span>
<span id="cb119-21"><a href="#cb119-21" aria-hidden="true"></a><span class="co">##    x</span></span>
<span id="cb119-22"><a href="#cb119-22" aria-hidden="true"></a><span class="co">## y      1    2    3</span></span>
<span id="cb119-23"><a href="#cb119-23" aria-hidden="true"></a><span class="co">##   1 1000    0 2986</span></span>
<span id="cb119-24"><a href="#cb119-24" aria-hidden="true"></a><span class="co">##   2  983 1067    0</span></span>
<span id="cb119-25"><a href="#cb119-25" aria-hidden="true"></a><span class="co">##   3    0    0 3964</span></span></code></pre></div>
<p>The variables <code>margin_X</code> and <code>cond_X_Y</code> are what you get when you compute the marginal and the conditional distribution from the given joint-distribution table as you did in your probability class.</p>
<p>The function <code>single_draw</code> performs a single draw form the distribution of <span class="math inline">\((X,Y)\)</span> by first drawing the value of <span class="math inline">\(X\)</span> from its marginal distribution. Then, it chooses the row of the conditional distribution table according to the obtained value of <span class="math inline">\(X\)</span> and simulates from it.</p>
<p>The function <code>replicate</code> is used to repeat <code>single_draw</code> many times and collect the results. By default, <code>replicate</code> attaches the output of each new “replication” as a new column and not a row, so we need to transpose the final product. That is what the function <code>t()</code> is for. We turn the result into a data frame because the function <code>table</code> knows how to handle data frames automatically. Another use of the transpose gives <code>x</code> the horizontal axis, and <code>y</code> the vertical one, like in the statement of the problem.</p>
</div>
<div class="problem">
<p>Exactly one percent of the people in a given population have a certain disease.
The accuracy of the diagnostic test for it is such that it
detects the sick as sick with probability <span class="math inline">\(0.95\)</span> and the healthy as healthy with
probability <span class="math inline">\(0.9\)</span>. A person chosen at random from the population tested
positive. What is the probability the he/she is, in fact, sick. Do the problem
both analytically and by Monte Carlo.</p>
</div>
<div class="solution">
This is a classical example of the unexpected conclusions we sometimes get from the Bayes formula. As usual, we depict the situation using a tree diagram:
<center>
<img src="pics/bayes_pic.png" width="50%" style="padding:10px" style="display: block; margin: auto;" />
</center>
<p>The person can test positive (denoted by <span class="math inline">\(tS\)</span> in the plot) in two ways. By actually being sick (<span class="math inline">\(S\)</span>) and then testing positive, or by being healthy and then testing positive. Bayes formula (or simply a look at the picture above) gives us
<span class="math display">\[ {\mathbb{P}}[ S | tS ] = \frac{ 0.01 \times 0.95}{ 0.01\times 0.95 + 0.1\times 0.99 } \approx 0.088.\]</span>
Thus, even when the test is quite accurate, the probability of getting a false positive is very high.</p>
<p>Let us do the same via Monte Carlo. We proceed like this. First we “pick a person” from the population by sampling from <code>c("H", "S")</code> and then “test” this person. After repeating this <code>nsim</code> times, we condition on the positive test, by removing all draws where the test was negative. This leaves us with a population of people who tested positive, and we simply need to see what proportion of those were are actually sick.</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true"></a>single_draw =<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb120-2"><a href="#cb120-2" aria-hidden="true"></a>    x =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;S&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.99</span>, <span class="fl">0.01</span>))</span>
<span id="cb120-3"><a href="#cb120-3" aria-hidden="true"></a>    <span class="cf">if</span> (x <span class="op">==</span><span class="st"> &quot;H&quot;</span>) {</span>
<span id="cb120-4"><a href="#cb120-4" aria-hidden="true"></a>        y =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;tH&quot;</span>, <span class="st">&quot;tS&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.9</span>, <span class="fl">0.1</span>))</span>
<span id="cb120-5"><a href="#cb120-5" aria-hidden="true"></a>    } <span class="cf">else</span> {</span>
<span id="cb120-6"><a href="#cb120-6" aria-hidden="true"></a>        y =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;tH&quot;</span>, <span class="st">&quot;tS&quot;</span>), <span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))</span>
<span id="cb120-7"><a href="#cb120-7" aria-hidden="true"></a>    }</span>
<span id="cb120-8"><a href="#cb120-8" aria-hidden="true"></a>    <span class="kw">return</span>(<span class="kw">c</span>(x, y))</span>
<span id="cb120-9"><a href="#cb120-9" aria-hidden="true"></a>}</span>
<span id="cb120-10"><a href="#cb120-10" aria-hidden="true"></a></span>
<span id="cb120-11"><a href="#cb120-11" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">100000</span></span>
<span id="cb120-12"><a href="#cb120-12" aria-hidden="true"></a>df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">t</span>(<span class="kw">replicate</span>(nsim, <span class="kw">single_draw</span>())))</span>
<span id="cb120-13"><a href="#cb120-13" aria-hidden="true"></a><span class="kw">colnames</span>(df) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;status&quot;</span>, <span class="st">&quot;test_result&quot;</span>)</span>
<span id="cb120-14"><a href="#cb120-14" aria-hidden="true"></a></span>
<span id="cb120-15"><a href="#cb120-15" aria-hidden="true"></a>cond =<span class="st"> </span>(df<span class="op">$</span>test_result <span class="op">==</span><span class="st"> &quot;tS&quot;</span>)</span>
<span id="cb120-16"><a href="#cb120-16" aria-hidden="true"></a>df_cond =<span class="st"> </span>df[cond, ]</span>
<span id="cb120-17"><a href="#cb120-17" aria-hidden="true"></a>(<span class="dt">prob =</span> <span class="kw">mean</span>(df_cond<span class="op">$</span>status <span class="op">==</span><span class="st"> &quot;S&quot;</span>))</span>
<span id="cb120-18"><a href="#cb120-18" aria-hidden="true"></a><span class="co">## [1] 0.09049444</span></span></code></pre></div>
</div>
<div class="problem">
<p>A point is chosen at random, uniformly in the unit cube
<span class="math inline">\([0,1]\times[0,1]\times [0,1]\)</span>. Its distance to the origin <span class="math inline">\((0,0)\)</span> is measured, and
turns out to be equal to <span class="math inline">\(1.5\)</span>.</p>
<p>Use simulations to estimate the shape of the pdf of the conditional distribution
of the point’s <!--'--> distance to <span class="math inline">\((1,1,1)\)</span>. Compare it to the unconditional
case, i.e., the case where no information about the distance to <span class="math inline">\((0,0,0)\)</span> is
known.</p>
<p>Compute the mean of this (conditional) distribution for a few values of
the parameter <span class="math inline">\(\varepsilon\)</span> you use to deal with conditioning in
the continuous case. Make sure you include values of <span class="math inline">\(\varepsilon\)</span> on both sides
of the spectrum - too big, and too small.</p>
</div>
<div class="solution">
<p>We want to vary the parameter <code>eps</code> later, so let’s write a function first:</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true"></a>simulate =<span class="st"> </span><span class="cf">function</span>(nsim, eps, conditional) {</span>
<span id="cb121-2"><a href="#cb121-2" aria-hidden="true"></a>    x =<span class="st"> </span><span class="kw">runif</span>(nsim)</span>
<span id="cb121-3"><a href="#cb121-3" aria-hidden="true"></a>    y =<span class="st"> </span><span class="kw">runif</span>(nsim)</span>
<span id="cb121-4"><a href="#cb121-4" aria-hidden="true"></a>    z =<span class="st"> </span><span class="kw">runif</span>(nsim)</span>
<span id="cb121-5"><a href="#cb121-5" aria-hidden="true"></a>    d1 =<span class="st"> </span><span class="kw">sqrt</span>((<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>x)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>y)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>z)<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb121-6"><a href="#cb121-6" aria-hidden="true"></a>    <span class="cf">if</span> (conditional) {</span>
<span id="cb121-7"><a href="#cb121-7" aria-hidden="true"></a>        d0 =<span class="st"> </span><span class="kw">sqrt</span>(x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>z<span class="op">^</span><span class="dv">2</span>)</span>
<span id="cb121-8"><a href="#cb121-8" aria-hidden="true"></a>        cond =<span class="st"> </span>(d0 <span class="op">&gt;</span><span class="st"> </span><span class="fl">1.5</span> <span class="op">-</span><span class="st"> </span>eps) <span class="op">&amp;</span><span class="st"> </span>(d0 <span class="op">&lt;</span><span class="st"> </span><span class="fl">1.5</span> <span class="op">+</span><span class="st"> </span>eps)</span>
<span id="cb121-9"><a href="#cb121-9" aria-hidden="true"></a>        <span class="kw">return</span>(d1[cond])</span>
<span id="cb121-10"><a href="#cb121-10" aria-hidden="true"></a>    } <span class="cf">else</span> {</span>
<span id="cb121-11"><a href="#cb121-11" aria-hidden="true"></a>        <span class="kw">return</span>(d1)</span>
<span id="cb121-12"><a href="#cb121-12" aria-hidden="true"></a>    }</span>
<span id="cb121-13"><a href="#cb121-13" aria-hidden="true"></a>}</span></code></pre></div>
<p>Histograms may be used as approximations to the pdf of the (conditional) distribution:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">1000000</span></span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true"></a>eps =<span class="st"> </span><span class="fl">0.1</span></span>
<span id="cb122-3"><a href="#cb122-3" aria-hidden="true"></a>d1_cond =<span class="st"> </span><span class="kw">simulate</span>(nsim, eps, <span class="dt">conditional =</span> <span class="ot">TRUE</span>)</span>
<span id="cb122-4"><a href="#cb122-4" aria-hidden="true"></a>d1 =<span class="st"> </span><span class="kw">simulate</span>(nsim, eps, <span class="dt">conditional =</span> <span class="ot">FALSE</span>)</span>
<span id="cb122-5"><a href="#cb122-5" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb122-6"><a href="#cb122-6" aria-hidden="true"></a><span class="kw">hist</span>(d1, <span class="dt">breaks =</span> <span class="dv">50</span>)</span>
<span id="cb122-7"><a href="#cb122-7" aria-hidden="true"></a><span class="kw">hist</span>(d1_cond, <span class="dt">breaks =</span> <span class="dv">50</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-304-1.png" width="672" style="display: block; margin: auto;" />
Note that, in addition to clearly different shapes, the supports of the two distributions differ, too. Unconditionally, the distance to <span class="math inline">\((1,1,1)\)</span> can be any number from <span class="math inline">\(0\)</span> to <span class="math inline">\(\sqrt{3} \approx 1.73\)</span>. If it is known that the distance to <span class="math inline">\((0,0,0)\)</span> is <span class="math inline">\(1.5\)</span>, however, the distance to <span class="math inline">\((1,1,1)\)</span> cannot be larger than <span class="math inline">\(1\)</span>.</p>
<p>Finally, let us compare the results we obtain by varying the parameter <span class="math inline">\(\varepsilon\)</span>, first with <code>nsim=100000</code>:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">100000</span></span>
<span id="cb123-2"><a href="#cb123-2" aria-hidden="true"></a>epss =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>, <span class="fl">0.02</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>)</span>
<span id="cb123-3"><a href="#cb123-3" aria-hidden="true"></a>d1s =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">length =</span> <span class="kw">length</span>(epss))</span>
<span id="cb123-4"><a href="#cb123-4" aria-hidden="true"></a><span class="cf">for</span> (eps <span class="cf">in</span> epss) {</span>
<span id="cb123-5"><a href="#cb123-5" aria-hidden="true"></a>    sims =<span class="st"> </span><span class="kw">simulate</span>(nsim, <span class="dt">eps =</span> eps, <span class="dt">conditional =</span> <span class="ot">TRUE</span>)</span>
<span id="cb123-6"><a href="#cb123-6" aria-hidden="true"></a>    <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Eps = &quot;</span>, eps, <span class="st">&quot;, Draws = &quot;</span>, <span class="kw">length</span>(sims), <span class="st">&quot; Mean = &quot;</span>, <span class="kw">mean</span>(sims)))</span>
<span id="cb123-7"><a href="#cb123-7" aria-hidden="true"></a>}</span>
<span id="cb123-8"><a href="#cb123-8" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  2 , Draws =  100000  Mean =  0.960342228444094&quot;</span></span>
<span id="cb123-9"><a href="#cb123-9" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  1 , Draws =  93544  Mean =  0.929726634933677&quot;</span></span>
<span id="cb123-10"><a href="#cb123-10" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.5 , Draws =  47806  Mean =  0.753283074403369&quot;</span></span>
<span id="cb123-11"><a href="#cb123-11" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.3 , Draws =  20224  Mean =  0.595529938750379&quot;</span></span>
<span id="cb123-12"><a href="#cb123-12" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.2 , Draws =  10658  Mean =  0.496362640888308&quot;</span></span>
<span id="cb123-13"><a href="#cb123-13" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.1 , Draws =  3930  Mean =  0.36209037346251&quot;</span></span>
<span id="cb123-14"><a href="#cb123-14" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.02 , Draws =  647  Mean =  0.310139851430518&quot;</span></span>
<span id="cb123-15"><a href="#cb123-15" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.01 , Draws =  354  Mean =  0.30926108659397&quot;</span></span>
<span id="cb123-16"><a href="#cb123-16" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.001 , Draws =  33  Mean =  0.334224126737702&quot;</span></span>
<span id="cb123-17"><a href="#cb123-17" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.0001 , Draws =  4  Mean =  0.275311021507976&quot;</span></span></code></pre></div>
<p>The same experiment, but with <code>nsim=1000000</code> yields:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">1000000</span></span>
<span id="cb124-2"><a href="#cb124-2" aria-hidden="true"></a>epss =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>, <span class="fl">0.02</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>, <span class="fl">0.0001</span>)</span>
<span id="cb124-3"><a href="#cb124-3" aria-hidden="true"></a>d1s =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">length =</span> <span class="kw">length</span>(epss))</span>
<span id="cb124-4"><a href="#cb124-4" aria-hidden="true"></a><span class="cf">for</span> (eps <span class="cf">in</span> epss) {</span>
<span id="cb124-5"><a href="#cb124-5" aria-hidden="true"></a>    sims =<span class="st"> </span><span class="kw">simulate</span>(nsim, <span class="dt">eps =</span> eps, <span class="dt">conditional =</span> <span class="ot">TRUE</span>)</span>
<span id="cb124-6"><a href="#cb124-6" aria-hidden="true"></a>    <span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Eps = &quot;</span>, eps, <span class="st">&quot;, Draws = &quot;</span>, <span class="kw">length</span>(sims), <span class="st">&quot; Mean = &quot;</span>, <span class="kw">mean</span>(sims)))</span>
<span id="cb124-7"><a href="#cb124-7" aria-hidden="true"></a>}</span>
<span id="cb124-8"><a href="#cb124-8" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  2 , Draws =  1000000  Mean =  0.960506260771782&quot;</span></span>
<span id="cb124-9"><a href="#cb124-9" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  1 , Draws =  934686  Mean =  0.928432784386909&quot;</span></span>
<span id="cb124-10"><a href="#cb124-10" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.5 , Draws =  477123  Mean =  0.753640516540863&quot;</span></span>
<span id="cb124-11"><a href="#cb124-11" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.3 , Draws =  201634  Mean =  0.596081456550207&quot;</span></span>
<span id="cb124-12"><a href="#cb124-12" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.2 , Draws =  103279  Mean =  0.494799342061533&quot;</span></span>
<span id="cb124-13"><a href="#cb124-13" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.1 , Draws =  38529  Mean =  0.361335849359528&quot;</span></span>
<span id="cb124-14"><a href="#cb124-14" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.02 , Draws =  6944  Mean =  0.3064588479179&quot;</span></span>
<span id="cb124-15"><a href="#cb124-15" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.01 , Draws =  3371  Mean =  0.30560770769072&quot;</span></span>
<span id="cb124-16"><a href="#cb124-16" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.001 , Draws =  331  Mean =  0.301734543251831&quot;</span></span>
<span id="cb124-17"><a href="#cb124-17" aria-hidden="true"></a><span class="co">## [1] &quot;Eps =  0.0001 , Draws =  33  Mean =  0.31500400469211&quot;</span></span></code></pre></div>
</div>
<p>⬇︎ In case you were wondering, the text below belongs to footnotes from somewhere high above.⬇︎</p>
<!--chapter:end:02-simulation.Rmd-->
</div>
</div>
<div id="random-walks" class="section level1" number="3">
<h1 number="3"><span class="header-section-number">3</span> Random Walks</h1>
<div style="counter-reset: thechapter 3;">

</div>
<div id="what-are-stochastic-processes" class="section level2" number="3.1">
<h2 number="3.1"><span class="header-section-number">3.1</span> What are stochastic processes?</h2>
<p>A <strong>stochastic process</strong> is a sequence - finite or infinite - of random
variables. We usually write <span class="math inline">\(\{X_n\}_{n\in{\mathbb{N}}_0}\)</span> or <span class="math inline">\(\{X_n\}_{0\leq n \leq T}\)</span>,
depending on whether we are talking about an infinite or a finite sequence. The
number <span class="math inline">\(T\in {\mathbb{N}}_0\)</span> is called the <strong>time horizon</strong>, and we sometimes set
<span class="math inline">\(T=+\infty\)</span> when the sequence is infinite. The index <span class="math inline">\(n\)</span> is often interpreted
as <em>time</em>, so that a stochastic process can be thought of as a model of a random
process evolving in time. The initial value of the index <span class="math inline">\(n\)</span> is often normalized
to <span class="math inline">\(0\)</span>, even though other values may be used. This it usually very clear from
the context.</p>
<p>It is important that all the random variables <span class="math inline">\(X_0, X_1,\dots\)</span> “live” on the
same sample space <span class="math inline">\(\Omega\)</span>. This way, we can talk about the notion of a
<strong>trajectory</strong> or a <strong>sample path</strong> of a stochastic process: it is, simply, the
sequence of numbers <span class="math display">\[X_0(\omega), X_1(\omega), \dots\]</span> but with <span class="math inline">\(\omega\in \Omega\)</span> considered “fixed”. In other words, we can think of a stochastic
process as a random variable whose value is not a number, but sequence of
numbers. This will become much clearer once we introduce enough examples.</p>
</div>
<div id="the-simple-symmetric-random-walk" class="section level2" number="3.2">
<h2 number="3.2"><span class="header-section-number">3.2</span> The Simple Symmetric Random Walk</h2>
<p>A stochastic process <span class="math inline">\(\{X\}_{n\in{\mathbb{N}}_0}\)</span> is said to be a
<strong>simple symmetric random walk (SSRW)</strong> if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X_0=0\)</span>,</p></li>
<li><p>the random variables <span class="math inline">\(\delta_1 = X_1-X_0\)</span>,
<span class="math inline">\(\delta_2 = X_2 - X_1\)</span>, …, called the <strong>steps</strong> of the random walk, are independent</p></li>
<li><p>each <span class="math inline">\(\delta_n\)</span> has a <strong>coin-toss distribution</strong>, i.e., its distribution
is given by <span class="math display">\[{\mathbb{P}}[ \delta_n = 1] = {\mathbb{P}}[ \delta_n=-1] = \tfrac{1}{2} \text{ for each } n.\]</span></p></li>
</ol>
<p>Some comments:</p>
<ul>
<li><p>This definition captures the main features of an
idealized notion of a particle that gets shoved, randomly, in one of
two possible directions, over and over. In other words, these “shoves”
force the particle to take a step, and steps are
modeled by the random variables
variables <span class="math inline">\(\delta_1,\delta_2, \dots\)</span>. The position of the
particle after <span class="math inline">\(n\)</span> steps is <span class="math inline">\(X_n\)</span>; indeed,
<span class="math display">\[X_n = \delta_1 + \delta_2 + \dots + \delta_n \text{ for }n\in {\mathbb{N}}.\]</span> It
is important to assume that any two steps are independent of each
other - the most important properties of random walks depend on this
in a critical way.</p></li>
<li><p>Sometimes, we only need a finite number of steps of a random walk,
so we only care about the random variables <span class="math inline">\(X_0, X_1,\dots, X_T\)</span>. This
stochastic process (now with a finite time horizon <span class="math inline">\(T\)</span>) will also be
called a random walk. If we want to stress that the horizon is not infinite,
we sometimes call it the <strong>finite-horizon random walk</strong>. Whether <span class="math inline">\(T\)</span> is
finite or infinite is usually
clear from the context.</p></li>
<li><p>The starting point <span class="math inline">\(X_0=0\)</span> is just a normalization. Sometimes we
need more flexibility and allow our process to start at <span class="math inline">\(X_0=x\)</span> for
some <span class="math inline">\(x\in {\mathbb{N}}\)</span>. To stress that fact, we talk about the random walk <strong>starting at <span class="math inline">\(x\)</span></strong>.
If no starting point is mentioned, you should assume <span class="math inline">\(X_0=0\)</span>.</p></li>
<li><p>We will talk about <strong>biased</strong> (or <strong>asymmetric</strong>) random walks a bit later.
The only
difference will be that the probabilities of each <span class="math inline">\(\delta_n\)</span> taking
values <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span> will be <span class="math inline">\(p\in (0,1)\)</span> and <span class="math inline">\(1-p\)</span>, and not necessarily <span class="math inline">\(\tfrac{1}{2}\)</span>,
The probability <span class="math inline">\(p\)</span> cannot change
from step to step and the steps <span class="math inline">\(\delta_1, \delta_2, \dots\)</span> will continue to be
independent from each other.</p></li>
<li><p>The word simple in its name refers to the fact that distribution of every
step is a coin toss. You can easily imagine a more complicated mechanism that
would govern each step. For example, not only the direction, but also the
size of the step could be random. In fact, any distribution you can think
of can be used as a step distribution of a random walk. Unfortunately, we will
have very little to say about such, general, random walks in these notes.</p></li>
</ul>
</div>
<div id="how-to-simulate-random-walks" class="section level2" number="3.3">
<h2 number="3.3"><span class="header-section-number">3.3</span> How to simulate random walks</h2>
<p>In addition to being quite simple conceptually, random walks are also easy
to simulate. The fact that the steps <span class="math inline">\(\delta_n = X_n - X_{n-1}\)</span> are independent
coin tosses immediately suggests a feasible strategy:
simulate <span class="math inline">\(T\)</span> independent coin tosses first, and then define each
<span class="math inline">\(X_n\)</span> as the sum of the first <span class="math inline">\(n\)</span> tosses.</p>
<p>Before we implement this idea in R, let us agree on a few conventions which
we will use whenever we simulate a stochastic process:</p>
<ul>
<li>the result of each simulation is a <code>data.frame</code> object</li>
<li>its columns will be the random variables <span class="math inline">\(X_0\)</span>, <span class="math inline">\(X_1\)</span>, It is a good
idea to name your columns <code>X0</code>, <code>X1</code>, <code>X2</code>, etc.</li>
<li>each row will represent one “draw”˜
˜µ
This is best achieved by the following two-stage approach in R:</li>
</ul>
<ol style="list-style-type: lower-alpha">
<li><p>write a function which will simulate a single trajectory
of your process, If
your process comes with parameters, it is a good idea to include them
as arguments to this function.</p></li>
<li><p>use the function <code>replicate</code> to stack together many such simulations and
convert the result to a <code>data.frame</code>. Don’t
forget to transpose first (use the function <code>t</code>) because <code>replicate</code> works column by column,
and not row by row.</p></li>
</ol>
<p>Let’s implement this in the case of a simple random walk. Of course, it is
impossible to simulate a random walk on an infinite horizon (<span class="math inline">\(T=\infty\)</span>)
so we must restrict to finite-horizon random walks<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a>. The function <code>cumsum</code> which
produces partial sums of its input comes in very handy.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true"></a>single_trajectory =<span class="st"> </span><span class="cf">function</span>(T, <span class="dt">p =</span> <span class="fl">0.5</span>) {</span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true"></a>    delta =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="dt">size =</span> T, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p, p))</span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true"></a>    x =<span class="st"> </span><span class="kw">cumsum</span>(delta)</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true"></a>    <span class="kw">return</span>(x)</span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true"></a>}</span></code></pre></div>
<p>Next, we run the same function <code>nsim</code> times and record the results. It is a
lucky break that the default names given to columns are <code>X1</code>, <code>X2</code>, … so we
don’t have to rename them. We do have to add the zero-th column <span class="math inline">\(X_0=0\)</span> because,
formally speaking, the “random variable” <span class="math inline">\(X_0=0\)</span> is a part of the stochastic
process. This needs to be done before other columns are added to maintain the
proper order of columns, which is important when you want to plot trajectories.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" aria-hidden="true"></a>simulate_walk =<span class="st"> </span><span class="cf">function</span>(nsim, T, <span class="dt">p =</span> <span class="fl">0.5</span>) {</span>
<span id="cb126-2"><a href="#cb126-2" aria-hidden="true"></a>  <span class="kw">return</span>(</span>
<span id="cb126-3"><a href="#cb126-3" aria-hidden="true"></a>    <span class="kw">data.frame</span>(</span>
<span id="cb126-4"><a href="#cb126-4" aria-hidden="true"></a>      <span class="dt">X0 =</span> <span class="dv">0</span>,</span>
<span id="cb126-5"><a href="#cb126-5" aria-hidden="true"></a>      <span class="kw">t</span>(<span class="kw">replicate</span>(nsim, <span class="kw">single_trajectory</span>(T, p)))</span>
<span id="cb126-6"><a href="#cb126-6" aria-hidden="true"></a>    ))</span>
<span id="cb126-7"><a href="#cb126-7" aria-hidden="true"></a>}</span>
<span id="cb126-8"><a href="#cb126-8" aria-hidden="true"></a>walk =<span class="st"> </span><span class="kw">simulate_walk</span>(<span class="dt">nsim =</span> <span class="dv">10000</span>, <span class="dt">T =</span> <span class="dv">500</span>)</span></code></pre></div>
</div>
<div id="two-ways-of-looking-at-a-stochastic-proceses" class="section level2" number="3.4">
<h2 number="3.4"><span class="header-section-number">3.4</span> Two ways of looking at a stochastic proceses</h2>
<p>Now that we have the data frame <code>walk</code>, we can explore in at least two qualitatively different ways:</p>
<div id="column-wise-distributionally" class="section level3" number="3.4.1">
<h3 number="3.4.1"><span class="header-section-number">3.4.1</span> Column-wise (distributionally)</h3>
<p>Here we focus on individual random variables (column) or pairs, triplets, etc. of random variables and study their (joint) distributions. For example, we can plot histograms of the random variables <span class="math inline">\(X_5, X_8, X_{30}\)</span> or <span class="math inline">\(X_{500}\)</span>:</p>
<center>
<p><img src="_main_files/figure-html/unnamed-chunk-144-1.png" width="100%" style="display: block; margin: auto;" /></p>
</center>
<p>We can also use various (graphical or not) devices to understand joint distributions of pairs of random variables:</p>
<center>
<img src="_main_files/figure-html/unnamed-chunk-145-1.png" width="672" style="display: block; margin: auto;" />
</center>
</div>
<div id="row-wise-trajectorially-or-path-wise" class="section level3" number="3.4.2">
<h3 number="3.4.2"><span class="header-section-number">3.4.2</span> Row-wise (trajectorially or path-wise)</h3>
<p>If we focus on what is going on in a given row of <code>walk</code>, we are going to see
a different cross-section of our stochastic process. This way we are fixing the
state of the world <span class="math inline">\(\omega\)</span> (represented by a row of <code>walk</code>), i.e., the particular
realization of our process, but
varying the time parameter. A typical picture associated to a trajectory
of a random walk is the following</p>
<p><img src="_main_files/figure-html/unnamed-chunk-146-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>You can try to combine the two approaches (if you must) and plot several trajectories on the same plot.
While this produces pretty pictures (and has one or two genuine applications), it usually leads to a
sensory overload. Note that the trajectories below are jittered a bit. That means that the positions of the points are randomly shifted by a small amount. This allows us to see features of the plot that would otherwise be hidden because of the overlap.</p>
<center>
<p><img src="_main_files/figure-html/unnamed-chunk-147-1.png" width="672" style="display: block; margin: auto;" /></p>
</center>
</div>
</div>
<div id="the-path-space" class="section level2" number="3.5">
<h2 number="3.5"><span class="header-section-number">3.5</span> The path space</h2>
<p>The row-wise (or path-wise or trajectory-wise) view of the random walk described above illustrates a
very important point: the random walk (and random processes in general) can be
seen as random “variable” whose values are not merely numbers; they are sequences of numbers (trajectories).
In other words, a random process is simply a “random trajectory”.
We can simulate this random trajectory as we did above, but simulating the steps and adding them up, but we could also
take a different approach. We could build the set of
all possible trajectories, and then pick a random trajectory out of it.</p>
<p>For a random walk on a finite horizon <span class="math inline">\(T\)</span>, a trajectory is
simply a sequence of natural numbers starting from <span class="math inline">\(0\)</span>. Different
realizations of the coin-tosses <span class="math inline">\(\delta_n\)</span> will lead to different
trajectories, but not every sequence of natural numbers corresponds to a
trajectory. For example <span class="math inline">\((0,3,4,5)\)</span> is not possible because the increments of the random walk can only take values <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>. In fact, a finite
sequence <span class="math inline">\((x_0, x_1, \dots, x_T)\)</span> is a (possible) sample path of a
random walk if and only if <span class="math inline">\(x_0=0\)</span> and <span class="math inline">\(x_{k}-x_{k-1} \in \{-1,1\}\)</span>
for each <span class="math inline">\(k\)</span>. For example, when <span class="math inline">\(T=3\)</span>, there are <span class="math inline">\(8\)</span> possible trajectories:
<span class="math display">\[ \begin{align} \Omega = \{ 
&amp;(0,1,2,3), (0,1,2,1),(0,1,0,2), (0,1,0,-1), \\
&amp; (0,-1,-2,-3), (0,-1,-2,-1), (0,-1,0,-2), (0,-1,0,1)\}
\end{align}\]</span>
When you (mentally) picture them, think of their graphs:</p>
<center>
<img src="_main_files/figure-html/unnamed-chunk-148-1.png" width="672" style="display: block; margin: auto;" />
</center>
<p>Each trajectory corresponds to a particular combinations of the values of the
increments <span class="math inline">\((\delta_1,\dots, \delta_T)\)</span>, each such combination happens with probability <span class="math inline">\(2^{-T}\)</span>.
This means that any two trajectories are equally likely. That is convenient, because
this puts uniform probability on the collection of trajectories. We are now ready to
implement our simulation procedure in R; let us write the function <code>single_trajectory</code> using this
approach and use it to simulate a few trajectories. We assume that a function <code>all_paths(T)</code> which returns
a list of all possible paths with horizon <span class="math inline">\(T\)</span> has already been implemented (more info about a possible
implementation in R is given in a problem below):</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true"></a>T=<span class="dv">5</span></span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true"></a>Omega =<span class="st"> </span><span class="kw">all_paths</span>(T)</span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true"></a>single_trajectory =<span class="st"> </span><span class="cf">function</span>() {</span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true"></a>  <span class="kw">return</span>(<span class="kw">unlist</span>(<span class="kw">sample</span>(<span class="dt">size=</span><span class="dv">1</span>,Omega)))</span>
<span id="cb127-5"><a href="#cb127-5" aria-hidden="true"></a>}</span>
<span id="cb127-6"><a href="#cb127-6" aria-hidden="true"></a></span>
<span id="cb127-7"><a href="#cb127-7" aria-hidden="true"></a>simulate_walk =<span class="st"> </span><span class="cf">function</span>(nsim, <span class="dt">p=</span><span class="fl">0.5</span>) {</span>
<span id="cb127-8"><a href="#cb127-8" aria-hidden="true"></a>   <span class="kw">return</span>(<span class="kw">data.frame</span>(</span>
<span id="cb127-9"><a href="#cb127-9" aria-hidden="true"></a>            <span class="dt">X0=</span><span class="dv">0</span>, </span>
<span id="cb127-10"><a href="#cb127-10" aria-hidden="true"></a>            <span class="kw">t</span>(<span class="kw">replicate</span>(nsim, <span class="kw">single_trajectory</span>()))</span>
<span id="cb127-11"><a href="#cb127-11" aria-hidden="true"></a>        ))</span>
<span id="cb127-12"><a href="#cb127-12" aria-hidden="true"></a>}</span></code></pre></div>
</div>
<div id="the-distribution-of-x_n" class="section level2" number="3.6">
<h2 number="3.6"><span class="header-section-number">3.6</span> The distribution of <span class="math inline">\(X_n\)</span></h2>
<p>Building a path space is not simply an exercise in abstraction. Here is how we can use is to
understand the distribution of the position of the random walk:</p>
<div class="example">
<p>Let <span class="math inline">\(X\)</span> be a simple symmetric random walk with time horizon <span class="math inline">\(T=5\)</span>. What is the probability that <span class="math inline">\(X_{5}=1\)</span>?</p>
</div>
<div class="solution">
<p>Let <span class="math inline">\(\Omega\)</span> be the path space, i.e., the set of all possible trajectories of length <span class="math inline">\(5\)</span> - there are <span class="math inline">\(2^{5}=32\)</span> of them. The probability that <span class="math inline">\(X_{5}=1\)</span> is the probability that a randomly picked path from <span class="math inline">\(\Omega\)</span> will take the value <span class="math inline">\(1\)</span> at <span class="math inline">\(n=5\)</span>. Since all paths are equally likely, we need to <em>count</em>
the number of paths with value <span class="math inline">\(1\)</span> at <span class="math inline">\(n=5\)</span> and then divide by the total number of paths, i.e., <span class="math inline">\(32\)</span>.</p>
<p>So, how many paths are there that take value <span class="math inline">\(1\)</span> at <span class="math inline">\(n=5\)</span>? Each path is built out of steps of absolute value <span class="math inline">\(1\)</span>. Some of them go up (call them up-steps) and some of them go down (down-steps). A moment’s though reveals that the only way to reach <span class="math inline">\(1\)</span> in <span class="math inline">\(5\)</span> steps is if you have exactly <span class="math inline">\(3\)</span> up-steps and <span class="math inline">\(2\)</span> down-steps. Conversely, any path that has <span class="math inline">\(3\)</span> up-steps and <span class="math inline">\(2\)</span> down-steps ends at <span class="math inline">\(1\)</span>.</p>
<p>This realization transforms the problem into the following: how many paths are there with exactly <span class="math inline">\(3\)</span> up-steps (note that we don’t have to specify that there are <span class="math inline">\(2\)</span> down-steps - it will happen automatically). The only difference between different paths with exactly <span class="math inline">\(3\)</span> up-steps is the position of these up-steps. In some of them the up-steps happen right at the start, in some at the very end, and in some they are scattered around. Each path with <span class="math inline">\(3\)</span> up-steps is uniquely determined by the list of positions of those up-steps, i.e., with a size-<span class="math inline">\(3\)</span> subset of <span class="math inline">\(\{1,2,3,4,5\}\)</span>. This is not a surprise at all, since each path is build out of increments, and positions of positive increments clearly determine values of all increments.</p>
<p>The problem has now become purely mathematical: how many size-<span class="math inline">\(3\)</span> subsets of <span class="math inline">\(\{1,2,3,4,5\}\)</span> are there? The answer comes in the form of a <em>binomial coefficient</em> <span class="math inline">\(\binom{5}{3}\)</span> whose value is <span class="math inline">\(10\)</span> - there are exactly ten ways to pick three positions out of five. Therefore,
<span class="math display">\[ {\mathbb{P}}[ X_{5} = 1] = 10 \times 2^{-5} = \frac{5}{16}.\]</span></p>
</div>
<p>Can we do this in general?</p>
<div class="example">
<p>Let <span class="math inline">\(X\)</span> be a simple symmetric random walk with time horizon <span class="math inline">\(T\)</span>. What is the probability that <span class="math inline">\(X_{n}=k\)</span>?</p>
</div>
<div class="solution">
<p>The reasoning from the last example still applies. A trajectory with <span class="math inline">\(u\)</span> up-steps and <span class="math inline">\(d\)</span> down-steps
will end at <span class="math inline">\(u-d\)</span>, so we must have <span class="math inline">\(u-d=k\)</span>. On the other hand <span class="math inline">\(u+d=n\)</span> since all steps that are not up-steps are necessarily down-steps. This gives as a simple linear system with two equations and two unknowns which solves to <span class="math inline">\(u = (n+k)/2\)</span>, <span class="math inline">\(d=(n-k)/2\)</span>. Note the <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> must have the same parity for this solution to be meaningful. Also, <span class="math inline">\(k\)</span> must be between <span class="math inline">\(-n\)</span> and <span class="math inline">\(n\)</span>.</p>
<p>Having figured out how many up-steps is necessary to reach <span class="math inline">\(k\)</span>, all we need to do is count the number of trajectories with that many up-steps. Like before, we can do that by counting the number of
ways we can choose their position among <span class="math inline">\(n\)</span> steps, and, like before, the answer is the binomial coefficient <span class="math inline">\(\binom{n}{u}\)</span> where <span class="math inline">\(u=(n+k)/2\)</span>. Dividing by the total number of trajectories gives us the final answer:
<span class="math display">\[ {\mathbb{P}}[ X_n = k ] = \binom{n}{ (n+k)/2} 2^{-n},\]</span>
for all <span class="math inline">\(k\)</span> between <span class="math inline">\(-n\)</span> and <span class="math inline">\(n\)</span> with same parity as <span class="math inline">\(n\)</span>. For all other <span class="math inline">\(k\)</span>, the probability is <span class="math inline">\(0\)</span>.</p>
</div>
<p>The binomial coefficient and the <span class="math inline">\(n\)</span>-th power suggest that the distribution of <span class="math inline">\(X_n\)</span> might have something
to do with the binomial distribution. It is clearly not the binomial, since it
can take negative values, but it is related. To figure out what is going on, let us first remember what the binomial distribution is all about. Formally, it is a discrete distribution with two parameters <span class="math inline">\(n\in{\mathbb{N}}\)</span> and <span class="math inline">\(p\in (0,1)\)</span>. Its support is <span class="math inline">\(\{0,1,2,\dots, n\}\)</span> and the distribution is given by the following table, where <span class="math inline">\(q=1-p\)</span></p>
<center>
<div style="width: 80%">
<table style="width:100%;">
<colgroup>
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">0</th>
<th align="center">1</th>
<th align="center">2</th>
<th align="center">…</th>
<th align="center">k</th>
<th align="center">…</th>
<th align="center">n</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(\binom{n}{0} q^n\)</span></td>
<td align="center"><span class="math inline">\(\binom{n}{1} p q^{n-1}\)</span></td>
<td align="center"><span class="math inline">\(\binom{n}{2} p^2 q^{n-2}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(\binom{n}{k} p^k q^{n-k}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(\binom{n}{n} p^n\)</span></td>
</tr>
</tbody>
</table>
</div>
</center>
<p>The binomial distribution is best understood, however, when it is expressed as a “number of successes”.
More precisely,</p>
<blockquote>
<p>If <span class="math inline">\(B_1,B_2,\dots, B_n\)</span> are <span class="math inline">\(n\)</span> <em>independent</em> Bernoulli random variables with
<em>the same</em> parameter <span class="math inline">\(p\)</span>, then their sum <span class="math inline">\(B_1+\dots+B_n\)</span> has a binomial distribution with
parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>.</p>
</blockquote>
<p>We think of <span class="math inline">\(B_1, \dots, B_n\)</span> as indicator random variables of “successes” in <span class="math inline">\(n\)</span> independent
“experiments” each of which “succeeds” with probability <span class="math inline">\(p\)</span>. A canonical example is tossing a biased coin
<span class="math inline">\(n\)</span> times and counting the number of “heads”.</p>
<p>We know that the position <span class="math inline">\(X_n\)</span> at time <span class="math inline">\(n\)</span> of the random walk admits the representation
<span class="math display">\[ X_n = \delta_1+\delta_2+\dots+\delta_n,\]</span>
just like the binomial random variable. The distribution of <span class="math inline">\(\delta_k\)</span> is not Bernoulli, though, since it takes the values <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>, and not <span class="math inline">\(0,1\)</span>. This is easily fixed by applying the linear transformation <span class="math inline">\(x\mapsto \frac{1}{2}(x+1)\)</span>; indeed <span class="math inline">\(( -1 +1)/2 = 0\)</span> and <span class="math inline">\(( 1 + 1) / 2 =1\)</span>, and, so,
<span class="math display">\[ \frac{1}{2}(\delta_k+1)\text{ is a Bernoulli random variable with parameter } p=\frac{1}{2}.\]</span>
Consequently, if we add all <span class="math inline">\(B_k = \tfrac{1}{2}(1+\delta_k)\)</span> and remember our discussion from above we get the following statement</p>
<blockquote>
<p>In a simple symmetric random walk the random variable <span class="math inline">\(\frac{1}{2} (n + X_n)\)</span> has
the binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p=1/2\)</span>, for each <span class="math inline">\(n\)</span>.</p>
</blockquote>
<p>Can you use that fact to rederive the distribution of <span class="math inline">\(X_n\)</span>?</p>
</div>
<div id="biased-random-walks" class="section level2" number="3.7">
<h2 number="3.7"><span class="header-section-number">3.7</span> Biased random walks</h2>
<p>If the steps of the random walk preferred one direction to the other,
the definition would need to be tweaked a little bit and the word “symmetric” in the name gets replaced by “biased” (or “asymmetric”):</p>
<p>A stochastic process <span class="math inline">\(\{X\}_{n\in{\mathbb{N}}_0}\)</span> is said to be a
**simple biased random walk with parameter <span class="math inline">\(p\in (0,1)\)</span> if</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X_0=0\)</span>,</p></li>
<li><p>the random variables <span class="math inline">\(\delta_1 = X_1-X_0\)</span>,
<span class="math inline">\(\delta_2 = X_2 - X_1\)</span>, …, called the <strong>steps</strong> of the random walk, are independent and</p></li>
<li><p>each <span class="math inline">\(\delta_n\)</span> has a <strong>biased coin-toss distribution</strong>, i.e., its distribution
is given by
<span class="math display">\[{\mathbb{P}}[ \delta_n = 1] = p \text{ and } {\mathbb{P}}[ \delta_n=-1] = 1-p \text{ for each } n.\]</span></p></li>
</ol>
<p>As far as the distribution of <span class="math inline">\(X_n\)</span> is concerned, we don’t expect
it to be the same as in the symmetric case. After all, the
biased random walk (think <span class="math inline">\(p=0.999\)</span>) will prefer one direction over
the other. Our trick
with writing <span class="math inline">\(\frac{1}{2}(n+X_n)\)</span> as a sum of Bernoulli random variables still works.
We just have to remember
that <span class="math inline">\(p\)</span> is not <span class="math inline">\(\frac{1}{2}\)</span> anymore to conclude that <span class="math inline">\(\tfrac{1}{2}(X_n + n)\)</span> has the binomial
distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>; if we put <span class="math inline">\(u = (n+k)/2\)</span> we get
<span class="math display">\[\begin{align}
{\mathbb{P}}[ X_n = k] &amp;= {\mathbb{P}}[ \tfrac{1}{2}(X_n+n) = u] = \binom{n}{u} p^u q^{n-u}\\ &amp; = \binom{n}{\frac{1}{2}(n+k)} p^{\frac{1}{2}(n+k)} q^{\frac{1}{2}(n-k)}. 
\end{align}\]</span>
Note that be binomial coefficient stays the same as in the symmetric case,
but the factor <span class="math inline">\(2^{-n} = (1/2)^{\frac{1}{2}(n+k)} (1/2)^{\frac{1}{2}(n-k)}\)</span> becomes
<span class="math inline">\(p^{\frac{1}{2}(n+k)} q^{\frac{1}{2}(n-k)}\)</span>.</p>
<p>Can we reuse the sample space <span class="math inline">\(\Omega\)</span> to build a
biased random walk? Yes, we can, but we need to
assign possibly different probabilities to individuals. Indeed, if
<span class="math inline">\(p=0.99\)</span>, the probability that all the increments <span class="math inline">\(\delta\)</span> of a <span class="math inline">\(10\)</span>-step
random walk take the
value <span class="math inline">\(+1\)</span> is <span class="math inline">\((0.99)^{10} \approx 0.90\)</span>. This is much larger
than the probability that all steps take the value <span class="math inline">\(-1\)</span>, which is <span class="math inline">\((0.01)^{10}= 10^{-20}\)</span>.</p>
<p>In general, the probability that a particular path is picked out of <span class="math inline">\(\Omega\)</span>
will depend on the number of up-steps and down-steps; more precisely it equals
<span class="math inline">\(p^u q^{n-u}\)</span> where <span class="math inline">\(u\)</span> is the number of up-steps. The interesting thing is that
the number of up-steps <span class="math inline">\(u\)</span> depends only on the final position <span class="math inline">\(x_n\)</span> of the path; indeed <span class="math inline">\(u = \frac{1}{2}(n+x_n)\)</span>. This way, all paths of length <span class="math inline">\(T=5\)</span> that end up at <span class="math inline">\(1\)</span> get the
same probability of being chosen, namely <span class="math inline">\(p^3 q^2\)</span>. Let us use the awful
seizure-inducing graph with multiple paths for good, and adjust the each path
according to its probability; some jitter has been added to deal with overlap.
The lighter-colored paths are less likely to happen then the darker-colored paths.</p>
<center>
<p><img src="_main_files/figure-html/unnamed-chunk-150-1.png" width="672" style="display: block; margin: auto;" /></p>
</center>
</div>
<div id="additional-problems-for-chapter-3" class="section level2" number="3.8">
<h2 number="3.8"><span class="header-section-number">3.8</span> Additional problems for Chapter 3</h2>
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple symmetric random walk. Which of the following
processes are simple random walks?</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\{2 X_n\}_{n\in {\mathbb{N}}_0}\)</span> ?</p></li>
<li><p><span class="math inline">\(\{X^2_n\}_{n\in {\mathbb{N}}_0}\)</span> ?</p></li>
<li><p><span class="math inline">\(\{-X_n\}_{n\in {\mathbb{N}}_0}\)</span> ?</p></li>
<li><p><span class="math inline">\(\{ Y_n\}_{n\in {\mathbb{N}}_0}\)</span>, where <span class="math inline">\(Y_n = X_{5+n}-X_5\)</span> ?</p></li>
</ol>
<p>How about the case <span class="math inline">\(p\ne \tfrac{1}{2}\)</span>?</p>
</div>
<div class="solution">
<ol style="list-style-type: decimal">
<li><p>No - the support of the distribution of <span class="math inline">\(X_1\)</span> is <span class="math inline">\(\{-2,2\}\)</span> and
not <span class="math inline">\(\{-1,1\}\)</span>.</p></li>
<li><p>No - <span class="math inline">\(X_1^2=1\)</span>, and not <span class="math inline">\(\pm 1\)</span> with equal probabilities.</p></li>
<li><p>Yes - all parts of the definition check out.</p></li>
<li><p>Yes - all parts of the definition check out.</p></li>
</ol>
<p>The answers are the same if <span class="math inline">\(p\ne \tfrac{1}{2}\)</span>, but, in 3., <span class="math inline">\(-X_n\)</span> comes with
probability <span class="math inline">\(1-p\)</span> of an up-step, and not <span class="math inline">\(p\)</span>.</p>
</div>
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple random walk.</p>
<ol style="list-style-type: decimal">
<li><p>Find the distribution of the product <span class="math inline">\(X_1 X_2\)</span></p></li>
<li><p>Compute <span class="math inline">\({\mathbb{P}}[ |X_1 X_2 X_3|]=2\)</span></p></li>
<li><p>Find the probability that <span class="math inline">\(X\)</span> will hit neither the level <span class="math inline">\(2\)</span> nor the level <span class="math inline">\(-2\)</span> until (and including) time <span class="math inline">\(T=3\)</span></p></li>
<li><p>Find the independent pairs of random variables among the following choices:</p>
<ul>
<li><span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span></li>
<li><span class="math inline">\(X_4 - X_2\)</span> and <span class="math inline">\(X_3\)</span></li>
<li><span class="math inline">\(X_4 - X_2\)</span> and <span class="math inline">\(X_6 - X_5\)</span></li>
<li><span class="math inline">\(X_1+X_3\)</span> and <span class="math inline">\(X_2+X_4\)</span>.</li>
</ul></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<p>The possible paths when the time horizon is
<span class="math inline">\(T=2\)</span> are <span class="math display">\[(0,1,2), (0,1,0), (0,-1,-2) \text{ and } (0,-1,0)\]</span>
The values of the product <span class="math inline">\(X_1 X_2\)</span> on those paths
are <span class="math inline">\(2, 0, 2\)</span>, and <span class="math inline">\(0\)</span>, respectively. Each happens with probability
<span class="math inline">\(0.25\)</span>. Therefore <span class="math inline">\({\mathbb{P}}[ X_1 X_2 = 0] = {\mathbb{P}}[ X_1 X_2 = 2] = \tfrac{1}{2}\)</span>, i.e., its distribution is given by</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
0
</th>
<th style="text-align:right;">
2
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
0.5
</td>
</tr>
</tbody>
</table>
<p><part> 2. </part></p>
<p><span class="math inline">\(|X_1 X_2 X_3|=2\)</span> only in the following two cases
<span class="math display">\[ X_1=1, X_2=2, X_3=1 \text{ or } X_1=-1, X_2=-2, X_3=-1.\]</span>
Each of those paths has probability <span class="math inline">\(1/8\)</span> of happening, so <span class="math inline">\({\mathbb{P}}[ |X_1 X_2 X_3| = 2] = 1/4\)</span>.</p>
<p><part> 3. </part></p>
<p>The only chance for <span class="math inline">\(X\)</span> to hit <span class="math inline">\(2\)</span> or <span class="math inline">\(−2\)</span> before or at T = 3 is at
time <span class="math inline">\(n = 2\)</span>. Since <span class="math inline">\(X_2 \in \{ -2, 0, 2\}\)</span>, this happens with probability
<span class="math inline">\({\mathbb{P}}[ X_2 \in \{-2,2\}] = 1 - {\mathbb{P}}[X_2 = 0] = 0.5\)</span>.</p>
<p><part> 4. </part></p>
<p>The only independent pair is <span class="math inline">\(X_4 - X_2\)</span> and <span class="math inline">\(X_6 - X_5\)</span> because
the two random variables are build out of
completely different increments: <span class="math inline">\(X_4 - X_2 = \delta_3+\delta_4\)</span> while <span class="math inline">\(X_6-X_5 = \delta_6\)</span>.</p>
<p>The others are not independent. For example, if we are told that <span class="math inline">\(X_1+X_3 = 4\)</span>, it necessarily
follows that <span class="math inline">\(\delta_1= \delta_2=\delta_3=1\)</span>.
Hence, <span class="math inline">\(X_2+X_4 = 2\delta_1+2\delta_2+\delta_3+\delta_4 = 5+\delta_4\)</span> which
cannot be less than <span class="math inline">\(4\)</span>. On the other hand, without any information, <span class="math inline">\(X_2+X_4\)</span> can easily
be negative.</p>
</div>
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple random walk.</p>
<ol style="list-style-type: decimal">
<li><p>Compute <span class="math inline">\({\mathbb{P}}[ X_{32} = 4| X_8 = 6]\)</span>.</p></li>
<li><p>Compute <span class="math inline">\({\mathbb{P}}[ X_9 = 3 \text{ and } X_{15}=5 ]\)</span></p></li>
<li><p>(extra credit) Compute <span class="math inline">\({\mathbb{P}}[ X_7 + X_{12} = X_1 + X_{16}]\)</span></p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<p>This is the same as <span class="math inline">\({\mathbb{P}}[ X_{32}- X_8 = -2 | X_8=6]\)</span>. The random variables <span class="math inline">\(X_8\)</span> and <span class="math inline">\(X_{32}-X_8\)</span> are independent (as they are built out of different <span class="math inline">\(\delta\)</span>s), so we can remove the conditioning.</p>
<p>It remains to compute <span class="math inline">\({\mathbb{P}}[X_{32} - X_8 = -2]\)</span>. For that, we note that <span class="math inline">\(X_{32} - X_8\)</span> is a sum of <span class="math inline">\(24\)</span> independent coin tosses, so its distribution is the same as that of <span class="math inline">\(X_{24}\)</span>. Therefore, by our formula for the distribution of <span class="math inline">\(X_n\)</span>, we have
<span class="math display">\[ {\mathbb{P}}[X_{32}= 4 | X_8 = 6] = {\mathbb{P}}[X_{24} = -2] = \binom{24}{11} 2^{-24}.\]</span>
<part> 2. </part></p>
<p>We have
<span class="math display">\[\begin{align}
{\mathbb{P}}[ X_9 = 3 \text{ and } X_{15}=5 ] &amp; = {\mathbb{P}}[ X_{15} = 5 | X_9 = 3] \times {\mathbb{P}}[ X_9=3] \\ &amp; = {\mathbb{P}}[ X_6 = 2] \times {\mathbb{P}}[X_9=3] = \binom{6}{4} 2^{-6} \binom{9}{6} 2^{-9},
\end{align}\]</span>
where we used the same ideas as in 1. above</p>
<p><part> 3. </part></p>
<p>We rewrite everything using <span class="math inline">\(\delta\)</span>s:
<span class="math display">\[\begin{align} X_7+X_{12} = X_1+X_{16} &amp;\Leftrightarrow X_7-X_1 = X_{16}-X_{12} \Leftrightarrow \delta_2+\dots+\delta_7 = \delta_{13} + \dots+\delta_{16}\\ &amp; \Leftrightarrow (-\delta_2) + \dots + (-\delta_7) + \delta_{13}+ \dots + \delta_{16} = 0.
\end{align}\]</span></p>
<p>Since <span class="math inline">\(-\delta_k\)</span> has the same distribution as <span class="math inline">\(\delta_k\)</span> (both are coin tosses) and remains independent of all other <span class="math inline">\(\delta_i\)</span>, the left-hand side of the last expression in the chain of equivalences above is a sum of <span class="math inline">\(10\)</span> indepenedent coin tosses. Therefore, the probability that it equals <span class="math inline">\(0\)</span> is the same as <span class="math inline">\({\mathbb{P}}[X_{10}=0] = \binom{10}{5} 2^{-10}\)</span>.</p>
</div>
<!--
  same_value_three_points
-->
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple random walk. For <span class="math inline">\(n\in{\mathbb{N}}\)</span> compute the probability that <span class="math inline">\(X_{2n}\)</span>, <span class="math inline">\(X_{4n}\)</span> and <span class="math inline">\(X_{6n}\)</span> take the same value.</p>
</div>
<div class="solution">
<p>Increments <span class="math inline">\(X_{4n}-X_{2n}\)</span> and
<span class="math inline">\(X_{6n} - X_{4n}\)</span> are independent, and each is a sum of <span class="math inline">\(2n\)</span> independent
coin tosses (therefore has the same distribution as <span class="math inline">\(X_{2n}\)</span>). Hence,
<span class="math display">\[\begin{aligned}
    {\mathbb{P}}[ X_{2n} = X_{4n} \text{ and }X_{4n} = X_{6n} ] 
    &amp;= 
    {\mathbb{P}}[ X_{4n} - X_{2n} = 0  \text{ and }X_{6n} - X_{4n} = 0  ]\\ 
    &amp;=
    {\mathbb{P}}[ X_{4n} - X_{2n} = 0] \times {\mathbb{P}}[ X_{6n} - X_{4n}=0]\\
    &amp;={\mathbb{P}}[ X_{2n}=0] \times {\mathbb{P}}[ X_{2n} =0 ]\\ &amp; = \binom{2n}{n} 2^{-2n}
    \binom{2n}{n} 2^{-2n} =  \binom{2n}{n}^2 2^{-4n}.
  \end{aligned}\]</span></p>
</div>
<!--
  R-impl-all paths
-->
<div class="problemec">
<p>Write an R function (call it <code>all_paths</code>) which takes an integer argument <code>T</code> and returns a list of all possible paths of a random walk with time horizon <span class="math inline">\(T\)</span>.
(Note: Since vectors cannot have other vectors as elements, you will need to use a data structure called <code>list</code> for this. It behaves very much like a vector, so it should not be a problem.)</p>
</div>
<div class="solution">
<p>The implementation below uses the function <code>combn</code> which returns the list of all
subsets of a certain size of a certain vector. Since each path is determined by
the positions of its up-steps, we need to loop through all numbers <span class="math inline">\(i\)</span> from <span class="math inline">\(0\)</span>
to <span class="math inline">\(T\)</span> and then list all subsets of the size <span class="math inline">\(i\)</span>. The next step is to turn a set
of positions to a path of a random walk. This can be done in many ways; one is
implemented implemented in <code>choice_to_path</code> using vector indexing.</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1" aria-hidden="true"></a>choice_to_path =<span class="st"> </span><span class="cf">function</span>(comb, T) {</span>
<span id="cb128-2"><a href="#cb128-2" aria-hidden="true"></a>    increments =<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, T)</span>
<span id="cb128-3"><a href="#cb128-3" aria-hidden="true"></a>    increments[comb] =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb128-4"><a href="#cb128-4" aria-hidden="true"></a>    path =<span class="st"> </span><span class="kw">cumsum</span>(increments)</span>
<span id="cb128-5"><a href="#cb128-5" aria-hidden="true"></a>    <span class="kw">return</span>(path)</span>
<span id="cb128-6"><a href="#cb128-6" aria-hidden="true"></a>}</span>
<span id="cb128-7"><a href="#cb128-7" aria-hidden="true"></a></span>
<span id="cb128-8"><a href="#cb128-8" aria-hidden="true"></a>all_paths =<span class="st"> </span><span class="cf">function</span>(T) {</span>
<span id="cb128-9"><a href="#cb128-9" aria-hidden="true"></a>    Omega =<span class="st"> </span><span class="kw">list</span>(<span class="dv">2</span><span class="op">^</span>T)</span>
<span id="cb128-10"><a href="#cb128-10" aria-hidden="true"></a>    index =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb128-11"><a href="#cb128-11" aria-hidden="true"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>T) {</span>
<span id="cb128-12"><a href="#cb128-12" aria-hidden="true"></a>        choices =<span class="st"> </span><span class="kw">combn</span>(T, i, <span class="dt">simplify =</span> <span class="ot">FALSE</span>)</span>
<span id="cb128-13"><a href="#cb128-13" aria-hidden="true"></a>        <span class="cf">for</span> (choice <span class="cf">in</span> choices) {</span>
<span id="cb128-14"><a href="#cb128-14" aria-hidden="true"></a>            Omega[[index]] =<span class="st"> </span><span class="kw">choice_to_path</span>(choice, T)</span>
<span id="cb128-15"><a href="#cb128-15" aria-hidden="true"></a>            index =<span class="st"> </span>index <span class="op">+</span><span class="st"> </span><span class="dv">1</span></span>
<span id="cb128-16"><a href="#cb128-16" aria-hidden="true"></a>        }</span>
<span id="cb128-17"><a href="#cb128-17" aria-hidden="true"></a>    }</span>
<span id="cb128-18"><a href="#cb128-18" aria-hidden="true"></a>    <span class="kw">return</span>(Omega)</span>
<span id="cb128-19"><a href="#cb128-19" aria-hidden="true"></a>}</span></code></pre></div>
</div>
<!--
  var_cov_corr
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple symmetric random walk. Given <span class="math inline">\(n\in{\mathbb{N}}_0\)</span> and
<span class="math inline">\(k\in{\mathbb{N}}\)</span>, compute <span class="math inline">\(\operatorname{Var}[X_n]\)</span>, <span class="math inline">\(\operatorname{Cov}[X_n, X_{n+k}]\)</span> and
<span class="math inline">\(\operatorname{corr}[X_n, X_{n+k}]\)</span>, where <span class="math inline">\(\operatorname{Cov}\)</span> stands for the covariance and
<span class="math inline">\(\operatorname{corr}\)</span> for the correlation. (Note: look up <span class="math inline">\(\operatorname{Cov}\)</span> and <span class="math inline">\(\operatorname{corr}\)</span> if you forgot what they are).</p>
<p>Compute <span class="math inline">\(\lim_{n\to\infty} \operatorname{corr}[X_n, X_{n+k}]\)</span>. How would you interpret the result you obtained?</p>
</div>
<div class="solution">
<p>We have <span class="math inline">\(\operatorname{Var}[\delta_i] = 1\)</span> for each <span class="math inline">\(i\in{\mathbb{N}}\)</span>, so
<span class="math display">\[\operatorname{Var}[X_n] = \sum_{i=1}^n \operatorname{Var}[\delta_i] = n.\]</span> Since
<span class="math inline">\({\mathbb{E}}[X_n] = {\mathbb{E}}[X_{n+k}]=0\)</span> and <span class="math inline">\(X_{n+k} - X_n\)</span> is independent of <span class="math inline">\(X_n\)</span>,
we have <span class="math display">\[\begin{aligned}
        \operatorname{Cov}[X_n,X_{n+k}] &amp;= {\mathbb{E}}[ X_n X_{n+k}] = {\mathbb{E}}[ X_n (X_{n+k} - X_n)] + {\mathbb{E}}[X_n^2] = {\mathbb{E}}[X_n] {\mathbb{E}}[X_{n+k} - X_n] + {\mathbb{E}}[X_n^2]\\ &amp;= {\mathbb{E}}[X_n^2] = n.
    \end{aligned}\]</span> Finally, <span class="math display">\[\begin{aligned}
         \operatorname{corr}[X_n, X_{n+k}] = \frac{\operatorname{Cov}[X_n, X_{n+k}]}{\sqrt{\operatorname{Var}[X_n]} \sqrt{\operatorname{Var}[X_{n+k}]}}
         = \frac{n}{\sqrt{n(n+k)}} = \sqrt{\frac{n}{n+k}}.
    \end{aligned}\]</span>
When we let <span class="math inline">\(n\to\infty\)</span>, we get <span class="math inline">\(1\)</span>. This means that the positions of the random walk, <span class="math inline">\(k\)</span> steps apart, get closer and close to perfect correlation as <span class="math inline">\(n\to\infty\)</span>. If you know <span class="math inline">\(X_n\)</span> and <span class="math inline">\(n\)</span> is large, you almost know <span class="math inline">\(X_{n+k}\)</span>, at least at the typical scale of <span class="math inline">\(X_n\)</span>.</p>
</div>
<!--
  area_under_walk
  ------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple random walk with <span class="math inline">\({\mathbb{P}}[X_1=1]=p\in (0,1)\)</span>, and
let <span class="math inline">\(A_n\)</span> be the (signed) area under its graph (in the picture below, <span class="math inline">\(A_n\)</span> is
the area of the blue part minus the area of the orange part).</p>
<p><img src="_main_files/figure-html/unnamed-chunk-309-1.png" width="90%" style="display: block; margin: auto;" /></p>
<ol style="list-style-type: decimal">
<li><p>Find a formula for <span class="math inline">\(A_n\)</span> in terms of <span class="math inline">\(X_1,\dots, X_n\)</span>.</p></li>
<li><p>Compute <span class="math inline">\({\mathbb{E}}[A_n]\)</span> and <span class="math inline">\(\operatorname{Var}[A_n]\)</span>, for <span class="math inline">\(n\in{\mathbb{N}}\)</span>. (You will find the following
formulas helpful <span class="math inline">\(\sum_{j=1}^n j = \frac{n(n+1)}{2}\)</span> and <span class="math inline">\(\sum_{j=1}^n j^2=\frac{n(n+1)(2n+1)}{6}\)</span>.)</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<p>The dashed lines divide the area “under” the graph in separate trapezoids, so <span class="math inline">\(A_n\)</span> is the sum of their areas. The trapezoid between <span class="math inline">\(X_{k-1}\)</span> and <span class="math inline">\(X_{k}\)</span> has
area <span class="math inline">\(1 \times (X_{k-1}+X_{k})/2\)</span>, so
<span class="math display">\[ A_n = \sum_{k=1}^n \tfrac{1}{2} (X_{k-1}+X_k) = X_1+X_2+\dots+X_{n-1} + \tfrac{1}{2}X_n.\]</span></p>
<p><part> 2. </part></p>
<p>Let us first represent <span class="math inline">\(A_n\)</span> in terms of the sequence <span class="math inline">\(\{\delta_n\}_{n\in{\mathbb{N}}_0}\)</span>
<span class="math display">\[\begin{align}
A_n &amp;= (\delta_1) + (\delta_1+\delta_2) + \dots + (\delta_1+\dots + \delta_{n-1}) +
\tfrac{1}{2}(\delta_1+ \dots + \delta_n)\\
&amp;= (n-\tfrac{1}{2}) \delta_1 + (n-1-\tfrac{1}{2}) \delta_2 + \dots + \tfrac{1}{2}\delta_n.
\end{align}\]</span></p>
<p>We compute <span class="math inline">\({\mathbb{E}}[\delta_k]=p-q\)</span> so that, by the formulas from the problem,
<span class="math display">\[\begin{align}
  {\mathbb{E}}[A_n]&amp;= \sum_{j=1}^n (j-\tfrac{1}{2}) {\mathbb{E}}[\delta_{n-j}] = (p-q)
   \Big( \tfrac{1}{2}n(n+1) - \tfrac{1}{2}n\Big)\\ &amp; = \frac{p-q}{2} n^2
\end{align}\]</span></p>
<p>Just like above, but relying on the independence of <span class="math inline">\(\{\delta_n\}\)</span> and the fact that <span class="math inline">\(\operatorname{Var}[\delta_k]=1-(2p-1)^2=4pq\)</span>, we have
<span class="math display">\[\begin{align}
\operatorname{Var}[A_n] &amp;=
 \sum_{j=1}^n \operatorname{Var}[(j - \tfrac{1}{2}) \delta_{n-j}]
= \sum_{j=1}^n (j-\tfrac{1}{2})^2 \operatorname{Var}[\delta_k] \\&amp;
= 4pq \sum_{j=1}^n (j-\tfrac{1}{2})^2 = 4pq \Big( \sum_{j=1}^n j^2 - \sum_{j=1}^n j + \frac{1}{4} n \Big)\\
&amp; = 4pq \Big( \frac{n}{n+1}{(2n+1)}{6} - \frac{n (n+1)}{2} + \frac{n}{4})
= \frac{pq}{3} ( 4 n^3 - n)
\end{align}\]</span></p>
</div>
<p>⬇︎ In case you were wondering, the text below belongs to footnotes from somewhere high above.⬇︎</p>
<!--chapter:end:03-random-walks.Rmd-->
</div>
</div>
<div id="more-about-random-walks" class="section level1" number="4">
<h1 number="4"><span class="header-section-number">4</span> More about Random Walks</h1>
<div style="counter-reset: thechapter 4;">

</div>
<div id="the-reflection-principle" class="section level2" number="4.1">
<h2 number="4.1"><span class="header-section-number">4.1</span> The reflection principle</h2>
<p>Counting trajectories in order to compute probabilities is a powerful method,
as our next example shows. It also reveals a potential
weakness of the combinatorial approach: it works best when all <span class="math inline">\(\omega\)</span>
are equally likely (e.g., when <span class="math inline">\(p=\tfrac{1}{2}\)</span> in the case of the random walk).</p>
<p>We start by asking a simple question: what is the typical record value
of the random walk, i.e., how far “up” (or “right” depending on your point of view)
does it typically get? Clearly,
the largest value it can attain is <span class="math inline">\(T\)</span>. This happens only when
all coin tosses came up <span class="math inline">\(+1\)</span>, an extremely unlikely event -
its probability is <span class="math inline">\(2^{-T}\)</span>. On the other hand, this maximal value is at least <span class="math inline">\(0\)</span>, since
<span class="math inline">\(X_0=0\)</span>, already. A bit of thought reveals that any value between those
two extremes is possible, but it is not at all easy to compute their
probabilities.</p>
<p>More precisely, if <span class="math inline">\(\{X_n\}\)</span> is a simple random walk with time horizon
<span class="math inline">\(T\)</span>. We define its <strong>running-maximum process</strong> <span class="math inline">\(\{M_n\}_{n\in {\mathbb{N}}_0}\)</span> by
<span class="math display">\[M_n=\max(X_0,\dots, X_n),\ \text{ for }0 \leq n \leq T,\]</span>
and ask what the probabilities <span class="math inline">\({\mathbb{P}}[M_n = k]\)</span> for <span class="math inline">\(k=0,\dots, n\)</span> are.
An easy numerical solution to this problem can be given by simulation. We reuse the function
<code>simulate_walk</code> defined at the beginning of the chapter, but also employ a new function, called <code>apply</code> which “applies” a function to each row (or column) of a data frame or a matrix. It seems to be tailor-made for our purpose<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a> because we want to compute the maximum of each row of the simulation matrix (remember - the row means keep the realization fixed, but vary the time-index <span class="math inline">\(n\)</span>). The syntax of <code>apply</code> is simple - it needs the data frame, the margin (rows are coded as 1 and columns as 2; so when the margin is 1, the function is applied row-wise and when the margin is 2, the function is applied column-wise) and the function to be applied (<code>max</code> in our case). The output is a vector of size <code>nsim</code> with all row-wise maxima:</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true"></a>walk =<span class="st"> </span><span class="kw">simulate_walk</span>(<span class="dt">nsim =</span> <span class="dv">100000</span>, <span class="dt">T =</span> <span class="dv">12</span>, <span class="dt">p =</span> <span class="fl">0.5</span>)</span>
<span id="cb129-2"><a href="#cb129-2" aria-hidden="true"></a>M =<span class="st"> </span><span class="kw">apply</span>(walk, <span class="dv">1</span>, max)</span>
<span id="cb129-3"><a href="#cb129-3" aria-hidden="true"></a><span class="kw">hist</span>(M, <span class="dt">breaks =</span> <span class="kw">seq</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">12.5</span>, <span class="dv">1</span>), <span class="dt">probability =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-167-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The overall shape of the distribution is as we expected; the support is <span class="math inline">\(\{0,1,2,\dots, 12\}\)</span> and the
probabilities tend to decrease as <span class="math inline">\(k\)</span> gets larger. The unexpected feature is that <span class="math inline">\({\mathbb{P}}[ M_{12} = 1]\)</span> seems
to be the same as <span class="math inline">\({\mathbb{P}}[ M_{12} = 2]\)</span>. It drops after that for <span class="math inline">\(k=3\)</span>, but it looks like
<span class="math inline">\({\mathbb{P}}[ M_{12} = 3] = {\mathbb{P}}[ M_{12}=4]\)</span> again. Somehow the probability does not seem to change at
all from <span class="math inline">\(2i-1\)</span> to <span class="math inline">\(2i\)</span>.</p>
<p>Fortunately, there is an explicit formula for the distribution of <span class="math inline">\(M_n\)</span> and we can derive it
by a nice counting trick known as <strong>the reflection principle</strong>.</p>
<p>As usual, we may assume without loss of generality that <span class="math inline">\(n=T\)</span> since the
values of <span class="math inline">\(\delta_{n+1}, \dots, \delta_T\)</span> do not affect <span class="math inline">\(M_n\)</span> at all.
We start by picking a level <span class="math inline">\(l\in\{1,\dots, n\}\)</span> and first compute
the probability <span class="math inline">\({\mathbb{P}}[M_n\geq l]\)</span> - it will turn out to be easier than
attacking <span class="math inline">\({\mathbb{P}}[ M_n=l]\)</span> directly. The symmetry assumption <span class="math inline">\(p=1/2\)</span> ensures that
all trajectories are equally likely, so we can do this by counting the
number of trajectories whose maximal level reached is at least <span class="math inline">\(l\)</span>, and
then multiply by <span class="math inline">\(2^{-n}\)</span>.</p>
<p>What makes the computation of <span class="math inline">\({\mathbb{P}}[M_n \geq l]\)</span> a bit easier than that
of <span class="math inline">\({\mathbb{P}}[ M_n = l]\)</span> is the following equivalence</p>
<p><span class="math display">\[M_n\geq l \text{ if and only if } X_k=l \text{ for some } k.\]</span></p>
<p>In words, the set of trajectories whose maximum is at least <span class="math inline">\(l\)</span> is
exactly the same as the set of trajectories that hit the level <span class="math inline">\(l\)</span> at
some time. Let us denote the set of trajectories <span class="math inline">\(\omega\)</span> with this property by
<span class="math inline">\(A_l\)</span>, so that <span class="math inline">\({\mathbb{P}}[ M_n \geq l] = {\mathbb{P}}[A_l]\)</span>.
We can further split <span class="math inline">\(A_l\)</span> into three disjoint events <span class="math inline">\(A_l^{&gt;}\)</span>,
<span class="math inline">\(A_l^{=}\)</span> and <span class="math inline">\(A_l^{&lt;}\)</span>, depending on whether <span class="math inline">\(X_n&lt;l\)</span>, <span class="math inline">\(X_n=l\)</span> or <span class="math inline">\(X_n&gt;l\)</span>.
In the picture below, the red trajectory is in <span class="math inline">\(A_l^{&gt;}\)</span>, the green trajectory in <span class="math inline">\(A_l^=\)</span>
the orange one in <span class="math inline">\(A_l^{&lt;}\)</span>, while the blue one is not in <span class="math inline">\(A_l\)</span> at all.</p>
<center>
<p><img src="_main_files/figure-html/unnamed-chunk-168-1.png" width="80%" style="display: block; margin: auto;" /></p>
</center>
<p>With the set of all trajectories <span class="math inline">\(\Omega\)</span> partitioned into four disjoint classes, namely <span class="math inline">\(A^&gt;_l, A^=_l, A^&lt;_l\)</span> and <span class="math inline">\((A_l)^c\)</span>, we are ready to reveal the main idea behind the reflection principle:</p>
<center style="margin-bottom: 20px;">
<span class="math inline">\(A_l^&lt;\)</span> and <span class="math inline">\(A_l^&gt;\)</span> have exactly the same number of elements, i.e., <span class="math inline">\(\# A^&gt;_l = \# A_l^&lt;\)</span>.
</center>
<p>To see why that is true, start by choosing a trajectory <span class="math inline">\(\omega\in A_l^{&gt;}\)</span> and denoting by
<span class="math inline">\(\tau_l(\omega)\)</span> the <em>first time</em> <span class="math inline">\(\omega\)</span> visits the
level <span class="math inline">\(l\)</span>. Since <span class="math inline">\(\omega \in A^&gt;\)</span> such a time clearly exists.
Then we associate to <span class="math inline">\(\omega\)</span> another trajectory, call it <span class="math inline">\(\bar{\omega}\)</span>, obtained from <span class="math inline">\(\omega\)</span>
in the following way:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\bar{\omega}\)</span> and <span class="math inline">\(\omega\)</span> are the same until the time <span class="math inline">\(\tau_l(\omega)\)</span>.</li>
<li>After that, <span class="math inline">\(\bar{\omega}\)</span> is the reflection of <span class="math inline">\(\omega\)</span> around the level <span class="math inline">\(l\)</span>.</li>
</ol>
<p>Equivalently the increments of <span class="math inline">\(\omega\)</span> and <span class="math inline">\(\bar{\omega}\)</span> are exactly the same up to time <span class="math inline">\(\tau(\omega)\)</span>, and exactly the opposite afterwards. In the picture below - the orange trajectory is <span class="math inline">\(\omega\)</span> and the green trajectory is its
“reflection” <span class="math inline">\(\bar{\omega}\)</span>; note that they overlap until time <span class="math inline">\(5\)</span>:</p>
<center>
<p><img src="_main_files/figure-html/unnamed-chunk-169-1.png" width="80%" style="display: block; margin: auto;" /></p>
</center>
<p>Convince yourself that this procedure establishes
a bijection between the sets <span class="math inline">\(A_l^{&gt;}\)</span> and <span class="math inline">\(A_l^{&lt;}\)</span>, making these two
sets equal in size.</p>
<p>So why is it important to know that <span class="math inline">\(\# A_l^&gt; = \# A_l^&lt;\)</span>? Because the trajectories in
<span class="math inline">\(A_l^&gt;\)</span> (as well as in <span class="math inline">\(A_l^=\)</span>) are easy to count.
For them, the requirement that the level
<span class="math inline">\(l\)</span> is hit at a certain point is redundant; if you are at or above <span class="math inline">\(l\)</span>
at the very end, you must have hit <span class="math inline">\(l\)</span> at a certain point.<br />
Therefore, <span class="math inline">\(A_l^{&gt;}\)</span> is simply the family of those trajectories
<span class="math inline">\(\omega\)</span> whose final positions <span class="math inline">\(X_n(\omega)\)</span> are somewhere strictly above <span class="math inline">\(l\)</span>. Hence,
<span class="math display">\[\begin{align}
       {\mathbb{P}}[A_l^{&gt;}] &amp;= {\mathbb{P}}[ X_n=l+1 \text{ or } X_n = l+2 \text{ or } \dots \text{ or }
     X_n=n]\\ &amp; = \sum_{k=l+1}^n {\mathbb{P}}[X_n = k]
\end{align}\]</span></p>
<p>Similarly, <span class="math display">\[\begin{aligned}
     {\mathbb{P}}[ A_l^{=}] = {\mathbb{P}}[X_n=l].\end{aligned}\]</span>
Finally, by the reflection principle,
<span class="math display">\[\begin{aligned}
    {\mathbb{P}}[ A_l^{&lt;}] = {\mathbb{P}}[A_l^{&gt;}] = \sum_{k=l+1}^n {\mathbb{P}}[X_n=k].\end{aligned}\]</span></p>
<p>Putting all of this
together, we get <span class="math display">\[\begin{aligned}
    {\mathbb{P}}[ A_l ] = {\mathbb{P}}[ X_n=l] + 2 \sum_{k=l+1}^n {\mathbb{P}}[X_n=k],\end{aligned}\]</span>
so that <span class="math display">\[\begin{aligned}
    {\mathbb{P}}[ M_n = l ] &amp;= {\mathbb{P}}[ M_n \geq l] - {\mathbb{P}}[ M_n \geq l+1]\\ &amp; = {\mathbb{P}}
    [A_l] - {\mathbb{P}}
    [A_{l+1}]\\ &amp; =
    {\mathbb{P}}[ X_n = l] + 2 {\mathbb{P}}[X_n = l+1] + 2{\mathbb{P}}[X_n = l+2]+ \dots + 2{\mathbb{P}}[ X_n=n] -\\
    &amp; \qquad \qquad  \quad \  -
    {\mathbb{P}}[ X_n = l+1] - 2 {\mathbb{P}}[X_n = l+2] - \dots - 2{\mathbb{P}}[ X_n=n]\\
    &amp;= {\mathbb{P}}[ X_n=l] + {\mathbb{P}}[X_n=l+1]
    \end{aligned}\]</span></p>
<p>Now that we have the explicit expression
<span class="math display">\[ {\mathbb{P}}[ M_n = l ] = {\mathbb{P}}[ X_n=l] + {\mathbb{P}}[X_n = l+1] \text{ for } l=0,1,\dots, n,\]</span>
we can shed some light on the fact on the shape of the histogram for <span class="math inline">\(M_n\)</span> we plotted above.
Since <span class="math inline">\({\mathbb{P}}[X_n=l]\)</span> is <span class="math inline">\(0\)</span> if <span class="math inline">\(n\)</span> and <span class="math inline">\(l\)</span> don’t have the same parity, it is clear that only
one of the probabilities <span class="math inline">\({\mathbb{P}}[X_n=l]\)</span> and <span class="math inline">\({\mathbb{P}}[X_n=l+1]\)</span> can be positive. It follows that, for
<span class="math inline">\(n\)</span> even, we have
<span class="math display">\[\begin{align}
{\mathbb{P}}[ M_n =0] &amp;= {\mathbb{P}}[X_n=0] + {\mathbb{P}}[X_n=1] = {\mathbb{P}}[X_n=0]\\
{\mathbb{P}}[M_n=1] &amp;= {\mathbb{P}}[ X_n=1] + {\mathbb{P}}[X_n=2] = {\mathbb{P}}[X_n=2]\\
{\mathbb{P}}[M_n=2] &amp;= {\mathbb{P}}[ X_n=2] + {\mathbb{P}}[X_n=3] = {\mathbb{P}}[X_n=2]\\
{\mathbb{P}}[M_n=3] &amp;= {\mathbb{P}}[ X_n=3] + {\mathbb{P}}[X_n=4] = {\mathbb{P}}[X_n=4]\\
{\mathbb{P}}[M_n=4] &amp;= {\mathbb{P}}[ X_n=4] + {\mathbb{P}}[X_n=5] = {\mathbb{P}}[X_n=4] \text{ etc.}
\end{align}\]</span>
In a similar way, for <span class="math inline">\(n\)</span> odd, we have
<span class="math display">\[\begin{align}
{\mathbb{P}}[ M_n =0] &amp;= {\mathbb{P}}[X_n=0] + {\mathbb{P}}[X_n=1] = {\mathbb{P}}[X_n=1]\\
{\mathbb{P}}[M_n=1] &amp;= {\mathbb{P}}[ X_n=1] + {\mathbb{P}}[X_n=2] = {\mathbb{P}}[X_n=1]\\
{\mathbb{P}}[M_n=2] &amp;= {\mathbb{P}}[ X_n=2] + {\mathbb{P}}[X_n=3] = {\mathbb{P}}[X_n=3]\\
{\mathbb{P}}[M_n=3] &amp;= {\mathbb{P}}[ X_n=3] + {\mathbb{P}}[X_n=4] = {\mathbb{P}}[X_n=3]\\
{\mathbb{P}}[M_n=4] &amp;= {\mathbb{P}}[ X_n=4] + {\mathbb{P}}[X_n=5] = {\mathbb{P}}[X_n=5] \text{ etc.}
\end{align}\]</span></p>
<p>Here is a example of a typical problem where the reflection principle (i.e., the formula for <span class="math inline">\({\mathbb{P}}[M_n=k]\)</span>) is used:</p>
<div class="problem">
<p>Let <span class="math inline">\(X\)</span> be a simple symmetric random walk.
What is the probability that <span class="math inline">\(X_n\leq 0\)</span> for all <span class="math inline">\(0\leq n \leq T\)</span>?</p>
</div>
<div class="solution">
<p>This is really a question about the maximum, but in disguise. The walk will stay negative or <span class="math inline">\(0\)</span> if and only if its running maximum <span class="math inline">\(M_T\)</span> at time <span class="math inline">\(T\)</span> takes the value <span class="math inline">\(0\)</span>. By our formula for <span class="math inline">\({\mathbb{P}}[M_n=l]\)</span> we have
<span class="math display">\[ {\mathbb{P}}[M_T=0] = {\mathbb{P}}[X_T=0] + {\mathbb{P}}[X_T = 1].\]</span>
When <span class="math inline">\(T=2N\)</span> this evaluates to <span class="math inline">\(\binom{2N}{N} 2^{-2N}\)</span>, and when <span class="math inline">\(T=2N-1\)</span> to
<span class="math inline">\(\binom{2N-1}{N} 2^{-(2N-1)}\)</span>.</p>
</div>
<div class="problem">
<p>What is the probability that a simple symmetric random walk will reach the level <span class="math inline">\(l=1\)</span> in <span class="math inline">\(T\)</span> steps or fewer?
What happens when <span class="math inline">\(T\to\infty\)</span>?</p>
</div>
<div class="solution">
<p>The first question is exactly the opposite of the question in our previous example, so the answer is
<span class="math display">\[ 1 - {\mathbb{P}}[M_T=0] = 1- {\mathbb{P}}[X_T=0] - {\mathbb{P}}[X_T=1].\]</span>
As above, this evaluates to <span class="math inline">\(\binom{2N}{N} 2^{-2N}\)</span> when <span class="math inline">\(T=2N\)</span> is even (we skip the case of odd <span class="math inline">\(T\)</span> because it is very similar).
When <span class="math inline">\(N\to\infty\)</span>, we expect <span class="math inline">\(\binom{2N}{N}\)</span> to go to <span class="math inline">\(+\infty\)</span> and <span class="math inline">\(2^{-2N}\)</span> to go to
<span class="math inline">\(0\)</span>, so it is not immediately clear which term will win.
One way to make a guess is to think about it probabilistically: we are looking at the
probability <span class="math inline">\({\mathbb{P}}[X_{2N}=0]\)</span> that the random walk takes the
value <span class="math inline">\(0\)</span> after exactly <span class="math inline">\(2N\)</span> steps. Even though no other (single) value is more
likely to happen, there are so many other values <span class="math inline">\(X_{2N}\)</span> could take (anything
from <span class="math inline">\(-2N\)</span> to <span class="math inline">\(2N\)</span> except for <span class="math inline">\(0\)</span>) that we conjecture that
its probability converges to <span class="math inline">\(0\)</span>. A formal mathematical argument which proves that
our conjecture is, indeed correct, involves <strong>Stirling’s formula</strong>:</p>
<p><span class="math display">\[ N! \sim \sqrt{2 \pi N} \left( \frac{N}{e} \right)^N \text{ where }
   A_N \sim B_N \text{ means that } \lim_{N\to\infty} \frac{A_N}{B_N}=1. \]</span></p>
<p>We write <span class="math inline">\(\binom{2N}{N} = \tfrac{(2N)!}{N! N!}\)</span> and apply Stirling’s formula to each factorial (let’s skip the details)
to conclude that
<span class="math display">\[ 
  \binom{2N}{N} 2^{-2n}\sim \frac{1}{\sqrt{N \pi}} 
  \text{ so that }  \lim_{N\to\infty}
  \binom{2N}{N} 2^{-2n}
  = 0 \]</span></p>
</div>
<p>The result of the previous problem implies the following important fact:</p>
<blockquote>
<p>The simple symmetric random walk will reach the level <span class="math inline">\(1\)</span>,
with certainty, given enough time.</p>
</blockquote>
<p>Indeed, we just proved that the probability of this not happening during the first <span class="math inline">\(T\)</span> steps
shrinks down to <span class="math inline">\(0\)</span> as <span class="math inline">\(T\to\infty\)</span>.</p>
<p>But wait, there is more! By symmetry, the level <span class="math inline">\(1\)</span> can be replaced by <span class="math inline">\(-1\)</span>. Also, once we hit
<span class="math inline">\(1\)</span>, the random walk “renews itself” (this property is called the Strong
Markov Property and we will talk about it later), so it will eventually
hit the level <span class="math inline">\(2\)</span>, as well. Continuing the same way, we get the
following remarkable result</p>
<blockquote>
<p><strong>Sooner or later, the symple symmetric random walk will visit any level.</strong></p>
</blockquote>
<p>We close this chapter with an application of the reflection principle
to a classical problem in probability and combinatorics. Feel free to skip it
if you want to.</p>
<div class="problemec">
<p>Suppose that two
candidates, Daisy and Oscar, are running for office, and <span class="math inline">\(T \in{\mathbb{N}}\)</span>
voters cast their ballots. Votes are counted the old-fashioned way,
namely by the same official, one by one, until all <span class="math inline">\(T\)</span> of them have been
processed. After each ballot is opened, the official records the number
of votes each candidate has received so far. At the end, the official
announces that Daisy has won by a margin of <span class="math inline">\(k&gt;0\)</span> votes, i.e., that
Daisy got <span class="math inline">\((T+k)/2\)</span> votes and Oscar the remaining <span class="math inline">\((T-k)/2\)</span> votes. What
is the probability that at no time during the counting has Oscar been in
the lead?</p>
</div>
<div class="solution">
<p>We assume that the order in which the official counts the votes is
completely independent of the actual votes, and that each voter chooses
Daisy with probability <span class="math inline">\(p\in (0,1)\)</span> and Oscar with probability <span class="math inline">\(q=1-p\)</span>.
We don’t know <em>a priori</em> what <span class="math inline">\(p\)</span> is, and, as it turns out, we don’t need to!</p>
<p>For <span class="math inline">\(0 \leq n \leq T\)</span>, let <span class="math inline">\(X_n\)</span> be the number of votes received by
Daisy <em>minus</em> the number of votes received by Oscar in the first <span class="math inline">\(n\)</span>
ballots. When the <span class="math inline">\(n+1\)</span>-st vote is counted, <span class="math inline">\(X_n\)</span> either increases by
<span class="math inline">\(1\)</span> (if the vote was for Daisy), or decreases by 1 otherwise. The votes
are independent of each other and <span class="math inline">\(X_0=0\)</span>, so <span class="math inline">\(X_n\)</span>, <span class="math inline">\(0\leq n \leq T\)</span> is
a simple random walk with the time horizon <span class="math inline">\(T\)</span>. The probability of an
up-step is <span class="math inline">\(p\in (0,1)\)</span>, so this random walk is not necessarily
symmetric. The ballot problem can now be restated as follows:</p>
<p><em>For a simple random walk <span class="math inline">\(\{X_n\}_{0\leq n \leq T}\)</span>, what is the
probability that <span class="math inline">\(X_n\geq 0\)</span> <strong>for all</strong> <span class="math inline">\(n\)</span> with <span class="math inline">\(0\leq n \leq T\)</span>, given that
<span class="math inline">\(X_T=k\)</span>?</em></p>
<p>The first step towards understanding the solution is the realization
that the exact value of <span class="math inline">\(p\)</span> does not matter. Indeed, we are interested
in the conditional probability <span class="math inline">\({\mathbb{P}}[ F|G]={\mathbb{P}}[F\cap G]/{\mathbb{P}}[G]\)</span>, where
<span class="math inline">\(F\)</span> denotes the set of <span class="math inline">\(\omega\)</span> whose corresponding trajectories always
stay non-negative, while the trajectories corresponding to <span class="math inline">\(\omega\in G\)</span>
reach <span class="math inline">\(k\)</span> at time <span class="math inline">\(T\)</span>. Each <span class="math inline">\(\omega \in G\)</span> consists of exactly <span class="math inline">\((T+k)/2\)</span>
up-steps (<span class="math inline">\(1\)</span>s) and <span class="math inline">\((T-k)/2\)</span> down steps (<span class="math inline">\(-1\)</span>s), so its probability
weight is equal to <span class="math inline">\(p^{ (T+k)/2} q^{(T-k)/2}\)</span>. Therefore, with <span class="math inline">\(\# A\)</span>
denoting the number of elements in the set <span class="math inline">\(A\)</span>, we get <span class="math display">\[\begin{aligned}
 {\mathbb{P}}[ F|G]=\frac{{\mathbb{P}}[F\cap G]}{{\mathbb{P}}[G]}=\frac{\# (F\cap G) \ p^{
    (T+k)/2} q^{(T-k)/2}}{ \# G \ p^{ (T+k)/2}
  q^{(T-k)/2}}=\frac{\#(F\cap G)}{\# G}.\end{aligned}\]</span> This is quite
amazing in and of itself. This conditional probability does not depend
on <span class="math inline">\(p\)</span> at all!</p>
<p>Since we already know how to count the number of elements in <span class="math inline">\(G\)</span> (there
are <span class="math inline">\(\binom{T}{(T+k)/2}\)</span>), “all” that remains to be done is to count the
number of elements in <span class="math inline">\(G\cap F\)</span>. The elements in <span class="math inline">\(G \cap F\)</span> form a
portion of all the elements in <span class="math inline">\(G\)</span> whose trajectories don’t hit the
level <span class="math inline">\(l=-1\)</span>; this way, <span class="math inline">\(\#(G\cap F)=\#G-\#H\)</span>, where <span class="math inline">\(H\)</span> is the set of
all paths which finish at <span class="math inline">\(k\)</span>, but cross (or, at least, touch) the level
<span class="math inline">\(l=-1\)</span> in the process. Can we use the reflection principle to find
<span class="math inline">\(\# H\)</span>? Yes, we can. In fact, you can convince yourself that the
reflection of any trajectory corresponding to <span class="math inline">\(\omega \in H\)</span> around the
level <span class="math inline">\(l=-1\)</span> after its last hitting time of that level produces a
trajectory that starts at <span class="math inline">\(0\)</span> and ends at <span class="math inline">\(-k-2\)</span>, and vice versa.</p>
<p><img src="_main_files/figure-html/unnamed-chunk-170-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>The
number of paths from <span class="math inline">\(0\)</span> to <span class="math inline">\(-k-2\)</span> is easy to count - it is equal to
<span class="math inline">\(\binom{T}{(T+k)/2+1}\)</span>. Putting everything together, we get
<span class="math display">\[{\mathbb{P}}[ F|G]=\frac{\binom{T}{n_1}-\binom{T}{n_1+1}}
{\binom{T}{n_1}}=\frac{k+1}{n_1+1},\text{ where }n_1=\frac{T+k}{2}.\]</span>
The last equality follows from the definition of binomial coefficients
<span class="math inline">\(\binom{T}{i}=\frac{T!}{i!(T-i)!}\)</span>.</p>
<p>The Ballot problem has a long history (going back to at least 1887) and
has spurred a lot of research in combinatorics and probability. In fact,
people still write research papers on some of its generalizations. When
posed outside the context of probability, it is often phrased as “<em>in
how many ways can the counting be performed …</em>” (the difference being
only in the normalizing factor <span class="math inline">\(\binom{T}{n_1}\)</span> appearing in Example
above). A special case <span class="math inline">\(k=0\)</span> seems to be even
more popular - the number of <span class="math inline">\(2n\)</span>-step paths from <span class="math inline">\(0\)</span> to <span class="math inline">\(0\)</span> never going
below zero is called the <strong><span class="math inline">\(n\)</span>-th Catalan number</strong> and equals
<span class="math display">\[\begin{align}
   C_n=\frac{1}{n+1} \binom{2n}{n}.
 \end{align}\]</span></p>
</div>
<div class="problemec">
<p>Given <span class="math inline">\(n\in{\mathbb{N}}\)</span>, compute <span class="math inline">\({\mathbb{P}}[ \tau_1 = 2n+1 ]\)</span> for a simple, but possibly biased, random walk. (Note: Clearly, <span class="math inline">\({\mathbb{P}}[ \tau_1=2n]=0\)</span>.)</p>
</div>
<div class="solution">
<p>Let <span class="math inline">\(A\)</span> denote the set of all trajectories of length <span class="math inline">\(2n+1\)</span> that hit <span class="math inline">\(1\)</span> for the first time at time <span class="math inline">\(2n+1\)</span>, and let <span class="math inline">\(A&#39;\)</span> be the set of all trajectories of length <span class="math inline">\(2n\)</span> which stay at or below <span class="math inline">\(0\)</span> at all times and take the value <span class="math inline">\(0\)</span> at time <span class="math inline">\(2n\)</span>.
Clearly, each trajectory in <span class="math inline">\(A\)</span> is a trajectory in <span class="math inline">\(A&#39;\)</span> with <span class="math inline">\(1\)</span> attached at the very end, so that <span class="math inline">\(\# A = \# A&#39;\)</span>.</p>
<p>By the (last part) of the previous problem, <span class="math inline">\(\# A&#39; = \frac{1}{n+1} \binom{2n}{n}\)</span> (the <span class="math inline">\(n^{\text{th}}\)</span> Catalan number).
As above, all paths in <span class="math inline">\(A\)</span> have the same probability weight, namely <span class="math inline">\(p^{n+1} q^n\)</span>, so
<span class="math display">\[ {\mathbb{P}}[ \tau_1 = 2n+1]= p^{n+1} q^n \frac{1}{n+1} \binom{2n}{n}.\]</span></p>
</div>
<div class="problemec">
<p>Given <span class="math inline">\(p\in (0,1)\)</span>,</p>
<ol style="list-style-type: decimal">
<li>compute <span class="math inline">\({\mathbb{P}}[\tau_1&lt;\infty]\)</span>;</li>
<li>decide whether or not <span class="math inline">\({\mathbb{E}}[\tau_1]&lt;\infty\)</span>;</li>
<li>compute, heuristically, the value of <span class="math inline">\({\mathbb{E}}[\tau_1]\)</span> for those <span class="math inline">\(p\)</span> for which it is finite.</li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part>
Using the previous problem, we need to sum the following series
<span class="math display">\[\sum_{k=0}^{\infty}  {\mathbb{P}}[\tau_1=k] = \sum_{n=0}^{\infty} {\mathbb{P}}[ \tau_1 = 2n+1] = 
  \sum_{n=0}^{\infty} p^{n+1} q^{n} \frac{1}{n+1} \binom{2n}{n} = p \sum_{n=0}^{\infty} (pq)^n \frac{1}{n+1} \binom{2n}{n}.\]</span>
The sum looks difficult, so let us plot a numerical approximation of its value for different values of the parameter <span class="math inline">\(p\)</span> (the true value is plotted in orange):</p>
<p><img src="_main_files/figure-html/unnamed-chunk-171-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>We conjecture that <span class="math inline">\({\mathbb{P}}[ \tau_1 &lt;\infty ] = 1\)</span> for <span class="math inline">\(p\geq \tfrac{1}{2}\)</span>, but <span class="math inline">\({\mathbb{P}}[ \tau_1&lt;\infty]&lt;1\)</span> for <span class="math inline">\(p&lt;\tfrac{1}{2}\)</span>.
Indeed, using methods beyond the scope of these notes, it can be shown that our conjecture is true and that
<span class="math display">\[ {\mathbb{P}}[ \tau_1&lt;\infty ] =\begin{cases} 1, &amp; p \geq \tfrac{1}{2}\\ \frac{p}{q}, &amp; p&lt;\tfrac{1}{2}. \end{cases} \]</span></p>
<p><part> 2. </part></p>
<p>Since <span class="math inline">\({\mathbb{P}}[ \tau_1= \infty]&gt;0\)</span> for <span class="math inline">\(p&lt;\tfrac{1}{2}\)</span>, we can immediately conclude that <span class="math inline">\({\mathbb{E}}[\tau_1]=\infty\)</span> in that
case. Therefore, we assume that <span class="math inline">\(p\geq \tfrac{1}{2}\)</span>, and consider the sum
<span class="math display">\[ {\mathbb{E}}[\tau_1] = \sum_{k=0}^{\infty} k {\mathbb{P}}[\tau_1 = k] = \sum_{n=0}^{\infty} (2n+1) {\mathbb{P}}[ \tau_1 = 2n+1] = \sum_{n=0}^{\infty} p^{n+1} q^{n} \frac{2n+1}{n+1} \binom{2n}{n}.\]</span>
We have already seen that (by Stirling’s formula) we have <span class="math inline">\(\binom{2n}{n} \sim \frac{2^{2n}}{\sqrt{\pi n}}\)</span>, so the question reduces to the one about convergence of the following, simpler, series:
<span class="math display">\[ \sum_{n=1}^{\infty} \frac{1}{\sqrt{n}} p^n q^{n} 2^{2n} = \sum_{n=1}^{\infty} \frac{1}{\sqrt{n}} (4pq)^n.\]</span>
When <span class="math inline">\(p=\tfrac{1}{2}\)</span>, we have <span class="math inline">\(4pq=1\)</span>, and the series above becomes a <span class="math inline">\(p\)</span>-series with <span class="math inline">\(p=\tfrac{1}{2}\)</span>. Hence, it diverges. On the other hand, when <span class="math inline">\(p&gt;\tfrac{1}{2}\)</span>, <span class="math inline">\(4pq&lt;1\)</span>, the terms of the series are dominated by the terms of the convergent geometric series <span class="math inline">\(\sum_{n=1}^{\infty} (4pq)^n\)</span>. Therefore, it, itself, must converge. All in all:
<span class="math display">\[ {\mathbb{E}}[\tau_1] = \begin{cases} \infty, &amp; p\leq \tfrac{1}{2}, \\ &lt;\infty, &amp; p &gt; \tfrac{1}{2}. \end{cases}. \]</span>
<part> 3. </part></p>
<p>Let <span class="math inline">\(a_j = {\mathbb{E}}^{j}[\tau_1]\)</span>, where <span class="math inline">\({\mathbb{E}}^{j}\)</span> means that
the random walk starts from the level <span class="math inline">\(j\)</span>, i.e., <span class="math inline">\(X_0=j\)</span>, instead of the usual <span class="math inline">\(X_0=0\)</span>. Think about why it is plausible that the
following relations hold for the sequence <span class="math inline">\(a_n\)</span>:
<span class="math display">\[a_1 = 0,\text{ and } a_j = 1 + p a_{j+1} + q a_{j-1}.\]</span>
We guess that <span class="math inline">\(a_j\)</span> has the form <span class="math inline">\(a_j = c(1-j)\)</span>, for <span class="math inline">\(j&lt;1\)</span> (why?) and plug that guess into the above equation to get:
<span class="math display">\[ c(1-j) = 1 + p c (-j) + q c (2-j) = 1 - c - 2 c q + c(1-j).\]</span>
It follows that <span class="math inline">\(c = \tfrac{1}{1-2q} = \tfrac{1}{p-q}\)</span>. Thus, if you believe the heuristic, we have
<span class="math display">\[ {\mathbb{E}}[ \tau_1 ] = \begin{cases} \frac{1}{p-q}, &amp; p&gt;\tfrac{1}{2}, \\ + \infty, &amp; p\leq \tfrac{1}{2}. \end{cases}\]</span>
(Note: If you have never seen it before, the approach we took here seems very unusual. Indeed, in order to find the value of <span class="math inline">\(a_0\)</span> we decided to compute values for the elements <em>of the whole sequence</em> <span class="math inline">\(a_n\)</span>. This kind of
thinking will appear many times later in the chapters on Markov Chains.)</p>
</div>
</div>
<div id="stopping-times" class="section level2" number="4.2">
<h2 number="4.2"><span class="header-section-number">4.2</span> Stopping times</h2>
<p>A <strong>random time</strong> is simply a random variable which takes values in the set <span class="math inline">\({\mathbb{N}}_0\)</span> - it is random, and
it can be interpreted as a point in time. Not all random times are created equal, though: here are three
examples based on a simple symmetric random walk <span class="math inline">\(X\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\tau = 3\)</span>. This is the simplest random time - it always takes the value <span class="math inline">\(3\)</span>, no matter what. It is random
only in the formal sense of the word (just as the constant random vairbale <span class="math inline">\(X=3\)</span> <em>is</em> a random variable, but not a very interesting one). Constant random times, like <span class="math inline">\(\tau=3\)</span>, are called <strong>deterministic times</strong>.</p></li>
<li><p><span class="math inline">\(\tau=\tau_1\)</span> where <span class="math inline">\(\tau_1\)</span> is the first time <span class="math inline">\(X\)</span> hits the level <span class="math inline">\(1\)</span>. It is no longer constant - it clearly depends on the underlying trajectory of the random walk: sometimes <span class="math inline">\(\tau_1=1\)</span>; other times it can be very large.</p></li>
<li><p><span class="math inline">\(\tau=\tau_{\max}\)</span> where <span class="math inline">\(\tau_{\max}\)</span> is the first time <span class="math inline">\(X\)</span> takes its maximal value in the interval <span class="math inline">\(\{0,1,\dots, 100\}\)</span>. The random time <span class="math inline">\(\tau_{\max}\)</span> is clearly non-constant, but it differs from <span class="math inline">\(\tau=3\)</span> or <span class="math inline">\(\tau=\tau_1\)</span> in a significant way.</p></li>
</ol>
<p>Indeed, the first two examples have the following property:</p>
<blockquote>
<p>Given a time <span class="math inline">\(n\)</span>, you can tell whether <span class="math inline">\(\tau=n\)</span> or not using only the information you have gathered by time <span class="math inline">\(n\)</span>.</p>
</blockquote>
<p>The third one does <em>not</em>. Random times with this property are called <strong>stopping times</strong>. Here is a more precise, mathematical, definition. You should note that we allow our stopping times to take the value <span class="math inline">\(+\infty\)</span>. The usual interpretation is that whatever the stopping time is modeling never happens.</p>
<p><strong>Definition.</strong> A random variable <span class="math inline">\(\tau\)</span> taking
values in <span class="math inline">\({\mathbb{N}}_0\cup\{+\infty\} = \{0,1,2,\dots, +\infty\}\)</span> is said to be a <strong>stopping time</strong> with respect to the process
<span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> if for each <span class="math inline">\(n\in{\mathbb{N}}_0\)</span> there exists a function
<span class="math inline">\(G^n:{\mathbb{R}}^{n+1}\to \{0,1\}\)</span> such that
<span class="math display">\[\mathbf{1}_{\{\tau=n\}}=G^n(X_0,X_1,\dots, X_n), \text{ for all } n\in{\mathbb{N}}_0.\]</span></p>
<p>The functions <span class="math inline">\(G^n\)</span> are called the <strong>decision functions</strong>, and should be thought of as a black
box which takes the values of the process <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> observed up to the
present point and outputs either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. The value <span class="math inline">\(0\)</span> means <em>keep
going</em> and <span class="math inline">\(1\)</span> means <em>stop</em>. The whole point is that the decision has to be
based only on the available observations and not on the future ones.</p>
<p>Alternatively, you can think of a stopping time as an R function whose input is
a vector which represents a trajectory <span class="math inline">\(\omega\)</span> of a random walk (or any other
process) and the output is a nonnegative integer. This function needs to be such
that if it “decides” to output the value <span class="math inline">\(k\)</span>, it had to have based its decision
only on the first <span class="math inline">\(k\)</span> components of <span class="math inline">\(\omega\)</span>. This means that if the output
corresponding to the input trajectory <span class="math inline">\(\omega\)</span> is <span class="math inline">\(k\)</span>, and <span class="math inline">\(\omega&#39;\)</span> is
another trajectory whose first components match those of <span class="math inline">\(\omega\)</span>, then the
output corresponding to <span class="math inline">\(\omega\)</span>’ must also be <span class="math inline">\(k\)</span>.</p>
<p>Now that we know how to spot stopping times, let’s list some examples:</p>
<ol style="list-style-type: decimal">
<li><p>The simplest examples of stopping times are (non-random)
<strong>deterministic times</strong>. Just set <span class="math inline">\(\tau=5\)</span> (or <span class="math inline">\(\tau=723\)</span> or <span class="math inline">\(\tau=n_0\)</span> for any
<span class="math inline">\(n_0\in{\mathbb{N}}_0\cup\{+\infty\}\)</span>), no matter what the state of the
world <span class="math inline">\(\omega\in\Omega\)</span> is. The family of decision rules is easy to
construct:
<span class="math display">\[G^n(x_0,x_1,\dots, x_n)=\begin{cases} 1,&amp; n=n_0, \\ 0, &amp; n\not=
  n_0.\end{cases}.\]</span> Decision functions <span class="math inline">\(G^n\)</span> do not depend on the
values of <span class="math inline">\(X_0,X_1,\dots, X_n\)</span> <em>at all</em>. A gambler who stops gambling
after 20 games, no
matter what the winnings or losses are uses such a rule.</p></li>
<li><p>Probably the most well-known examples of stopping times are <strong>(first)
hitting times</strong>. They can be defined for general stochastic
processes, but we will stick to simple random walks for the purposes
of this example. So, let <span class="math inline">\(X_n=\sum_{k=0}^n \delta_k\)</span> be a simple random
walk, and let <span class="math inline">\(\tau_l\)</span> be the first time <span class="math inline">\(X\)</span> hits the level <span class="math inline">\(l\in{\mathbb{N}}\)</span>.
More precisely, we use the following slightly non-intuitive but
mathematically correct definition
<span class="math display">\[\tau_l=\min \{ n\in{\mathbb{N}}_0\, : \, X_n=l\}.\]</span> The set <span class="math inline">\( \{ n\in{\mathbb{N}}_0\, : \, X_n=l\}\)</span>
is the collection of all time-points at which <span class="math inline">\(X\)</span> visits the level
<span class="math inline">\(l\)</span>. The earliest one - the minimum of that set - is the first
hitting time of <span class="math inline">\(l\)</span>. In states of the world <span class="math inline">\(\omega\in\Omega\)</span> in
which the level <span class="math inline">\(l\)</span> just never gets reached, i.e., when
<span class="math inline">\( \{ n\in{\mathbb{N}}_0\, : \, X_n=l\}\)</span> is an empty set, we set
<span class="math inline">\(\tau_l(\omega)=+\infty\)</span>.</p>
<p>In order to show that <span class="math inline">\(\tau_l\)</span> is indeed a
stopping time, we need to construct the decision functions <span class="math inline">\(G^n\)</span>,
<span class="math inline">\(n\in{\mathbb{N}}_0\)</span>. Let us start with <span class="math inline">\(n=0\)</span>. We would have <span class="math inline">\(\tau_l=0\)</span> only in the
(impossible) case <span class="math inline">\(X_0=l\)</span>, so we always have <span class="math inline">\(G^0(X_0)=0\)</span>. How about
<span class="math inline">\(n\in{\mathbb{N}}\)</span>. For the value of <span class="math inline">\(\tau_l\)</span> to be equal to exactly <span class="math inline">\(n\)</span>, two
things must happen:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(X_n=l\)</span> (the level <span class="math inline">\(l\)</span> must actually be hit at time <span class="math inline">\(n\)</span>), and</p></li>
<li><p><span class="math inline">\(X_{n-1}\not = l\)</span>, <span class="math inline">\(X_{n-2}\not= l\)</span>, …, <span class="math inline">\(X_{1}\not=l\)</span>,
<span class="math inline">\(X_0\not=l\)</span> (the level <span class="math inline">\(l\)</span> has not been hit before).</p></li>
</ol>
<p>Therefore, <span class="math display">\[G^n(x_0,x_1,\dots, x_n)=\begin{cases}
1,&amp; x_0\not=l, x_1\not= l, \dots, x_{n-1}\not=l, x_n=l\\
0,&amp;\text{otherwise}.
\end{cases}\]</span> The hitting time <span class="math inline">\(\tau_2\)</span> of the level <span class="math inline">\(l=2\)</span> for a
particular trajectory of a symmetric simple random walk is depicted
below:</p></li>
</ol>
<p><img src="_main_files/figure-html/unnamed-chunk-172-1.png" width="80%" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: decimal">
<li><p>How about something that is <em>not</em> a stopping time? Let <span class="math inline">\(T\in{\mathbb{N}}\)</span> be
an arbitrary time-horizon and let <span class="math inline">\(\tau_{\max}\)</span> be the last time during
<span class="math inline">\(0,\dots, T\)</span> that the random walk visits its maximum during
<span class="math inline">\(0,\dots, T\)</span>:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-173-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>If you bought a share of a stock
at time <span class="math inline">\(n=0\)</span>, had to sell it some time before or at <span class="math inline">\(T\)</span> and had the
ability to predict the future, this is one of the points you would
choose to sell it at. Of course, it is impossible in general to
decide whether <span class="math inline">\(\tau_{\max}=n\)</span>, for some <span class="math inline">\(n\in0,\dots, T-1\)</span> without the
knowledge of the values of the random walk after <span class="math inline">\(n\)</span>.</p>
<p>More
precisely, let us sketch the proof of the fact that <span class="math inline">\(\tau_{\max}\)</span> is not a
stopping time. Suppose, to the contrary, that it is, and let <span class="math inline">\(G^n\)</span>
be the associated family of decision functions. Consider the following two
trajectories: <span class="math inline">\((0,1,2,3,\dots,  T-1,T)\)</span> and <span class="math inline">\((0,1,2,3,\dots, T-1,T-2)\)</span>. They differ only in the
direction of the last step. They also differ in the fact that
<span class="math inline">\(\tau_{\max}=T\)</span> for the first one and <span class="math inline">\(\tau_{\max}=T-1\)</span> for the second one. On the
other hand, by the definition of the decision functions, we have
<span class="math display">\[\mathbf{1}_{\{\tau_{\max}=T-1\}}=G^{T-1}(X_0,\dots, X_{T-1}).\]</span> The right-hand side
is equal for both trajectories, while the left-hand side equals to
<span class="math inline">\(0\)</span> for the first one and <span class="math inline">\(1\)</span> for the second one. A contradiction.</p></li>
</ol>
</div>
<div id="walds-identity-and-gamblers-ruin" class="section level2" number="4.3">
<h2 number="4.3"><span class="header-section-number">4.3</span> Wald’s identity and Gambler’s ruin</h2>
<p>One of the superpowers of stopping times is that they often behave just like deterministic times. The best way to understand this statement is in the context of the beautiful <em>martingale theory</em>. Unfortunately, learning about martingales would take an entire semester, so we have to settle for an illustrative example, namely, Wald’s identity.</p>
<p>Let <span class="math inline">\(\{\xi_n\}_{n\in{\mathbb{N}}}\)</span> be a sequence of independent and identically distributed random variables. The example you
should keep in mind is <span class="math inline">\(\xi_n = \delta_n\)</span>, where <span class="math inline">\(\delta_n\)</span> are coin tosses in the definition of a random walk. We set <span class="math inline">\(X_n = \sum_{k=1}^n \xi_k\)</span> and note that it is easy to compute <span class="math inline">\({\mathbb{E}}[X_n]\)</span>:
<span class="math display">\[ {\mathbb{E}}[ X_n ] = {\mathbb{E}}[ \xi_1+\dots + \xi_n] = {\mathbb{E}}[\xi_1] + \dots + {\mathbb{E}}[\xi_n] = n \mu, \text{ where } \mu = {\mathbb{E}}[\xi_1]={\mathbb{E}}[\xi_2]=\dots\]</span>
provided <span class="math inline">\({\mathbb{E}}[\xi_1]\)</span> exists. The expected value <span class="math inline">\(\mu\)</span> is the same for all <span class="math inline">\(\xi_1,\xi_2,\dots\)</span> because they all have the same distribution. In words, the equality above tells us that the expected value of <span class="math inline">\(X\)</span> moves with <em>speed</em> <span class="math inline">\(\mu\)</span>. Wald’s identity tells us that the same thing is true when the deterministic time <span class="math inline">\(n\)</span> is replaced by a stopping time. To understand its statement below, we must first introduce a bit more notation. Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a
stochastic process, and let <span class="math inline">\(\tau\)</span> be a random time which never takes the value <span class="math inline">\(+\infty\)</span>. Remember that <span class="math inline">\(X_0, X_1, \dots\)</span> are random variables, i.e., functions of the elementary outcome <span class="math inline">\(\omega\in\Omega\)</span>. The same is true for <span class="math inline">\(\tau\)</span>. Therefore, in order to define the <em>random variable</em> <span class="math inline">\(X_{\tau}\)</span> we need to specify what its value is for any given <span class="math inline">\(\omega\)</span>:
<span class="math display">\[ X_{\tau} (\omega) = X_{n}(\omega) \text{ where } n=\tau(\omega).\]</span>
This is exactly what you would expect; the elementary outcome <span class="math inline">\(\omega\)</span> not only tells us which trajectory of the process to consider, but also the time at which to do it. Note that when <span class="math inline">\(\tau=n\)</span> is a deterministic time, <span class="math inline">\(X_{\tau}\)</span> is exactly <span class="math inline">\(X_n\)</span>.</p>
<p><strong>Theorem.</strong> (Wald’s identity) Let <span class="math inline">\(\{\xi_n\}_{n\in{\mathbb{N}}}\)</span> be a sequence of independent and identically distributed random variables, and let <span class="math inline">\(X_n = \sum_{k=1}^n \xi_k\)</span> be the associated random walk. If <span class="math inline">\({\mathbb{E}}[ |\xi_n|]&lt;\infty\)</span> and <span class="math inline">\(\tau\)</span> is a stopping time for <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> such that <span class="math inline">\({\mathbb{E}}[\tau]&lt;\infty\)</span>, then
<span class="math display">\[ {\mathbb{E}}[X_{\tau}] = {\mathbb{E}}[\tau] \mu \text{ where } \mu = {\mathbb{E}}[\xi_1] = {\mathbb{E}}[\xi_2] = \dots \]</span></p>
<p>Before we prove this theorem, here is a handy identity:</p>
<div class="problem">
<p>(The “tail formula” for the expectation) Let <span class="math inline">\(\tau\)</span> be an
<span class="math inline">\({\mathbb{N}}_0\)</span>-valued random variable. Show that
<span class="math display">\[{\mathbb{E}}[\tau]=\sum_{k=1}^{\infty} {\mathbb{P}}[\tau \geq k].\]</span></p>
</div>
<div class="solution">
<p>Clearly, <span class="math inline">\({\mathbb{P}}[\tau\geq k] = {\mathbb{P}}[ \tau=k] + {\mathbb{P}}[\tau=k+1]+\dots\)</span>.
Therefore,</p>
<p><span class="math display">\[ 
\begin{array}{cccccccc}
\sum_{k=1}^{\infty} {\mathbb{P}}[\tau \geq k] &amp;=&amp; {\mathbb{P}}[ \tau=1]   &amp;+&amp; {\mathbb{P}}[\tau=2] &amp;+&amp; {\mathbb{P}}[\tau=3] &amp;+&amp; \dots  \\
&amp;&amp; &amp;+&amp; {\mathbb{P}}[\tau=2] &amp;+&amp; {\mathbb{P}}[\tau=3] &amp;+&amp; \dots \\
&amp;&amp; &amp;&amp;  &amp;+&amp; {\mathbb{P}}[\tau=3] &amp;+&amp; \dots \\
&amp;&amp; &amp;&amp; &amp;&amp; &amp;+&amp; \dots
\end{array}
\]</span>
If you look at the “columns”, you will realize that the expression <span class="math inline">\({\mathbb{P}}[\tau=1]\)</span> appears in this sum once, <span class="math inline">\({\mathbb{P}}[\tau=2]\)</span> twice, <span class="math inline">\({\mathbb{P}}[\tau=3]\)</span> three times, etc.
Hence
<span class="math display">\[\sum_{k=1}^{\infty} {\mathbb{P}}[ \tau\geq k] = \sum_{n=1}^{\infty} n {\mathbb{P}}[\tau=n] = {\mathbb{E}}[\tau].\]</span></p>
</div>
<div class="problemec">
<p>Prove Wald’s identity.</p>
</div>
<div class="solution">
<p>Here is another representation of the random variable <span class="math inline">\(X_{\tau}\)</span>:
<span class="math display">\[X_{\tau} = \sum_{k=1}^{\tau} \xi_k=\sum_{k=1}^{\infty} \xi_k \mathbf{1}_{\{k\leq \tau\}}.\]</span> The idea
behind it is simple: add all the values of <span class="math inline">\(\xi_k\)</span> for <span class="math inline">\(k\leq \tau\)</span> and keep adding zeros (since <span class="math inline">\(\xi_k \mathbf{1}_{\{k\leq \tau\}}=0\)</span> for <span class="math inline">\(k&gt;\tau\)</span>)
after that. Taking expectation of both sides and switching <span class="math inline">\({\mathbb{E}}\)</span> and
<span class="math inline">\(\sum\)</span> (this can be justified, but the argument is technical and we omit
it here) yields: <span class="math display">\[
 {\mathbb{E}}[\sum_{k=1}^{\tau} \xi_k]=\sum_{k=1}^{\infty} {\mathbb{E}}[ \mathbf{1}_{\{k\leq \tau\}}\xi_k].
\]</span> Let us examine the term <span class="math inline">\({\mathbb{E}}[\xi_k\mathbf{1}_{\{k\leq \tau\}}]\)</span> in
some detail. We first note that
<span class="math display">\[\mathbf{1}_{\{k\leq \tau\}}=1-\mathbf{1}_{\{k&gt;\tau\}}=1-\mathbf{1}_{\{k-1\geq
  \tau\}}=1-\sum_{j=0}^{k-1}\mathbf{1}_{\{\tau=j\}},\]</span>
so that
<span class="math display">\[
  {\mathbb{E}}[\xi_k \mathbf{1}_{\{k\leq \tau\}}]={\mathbb{E}}[\xi_k]-\sum_{j=0}^{k-1}{\mathbb{E}}[ \xi_k
\mathbf{1}_{\{\tau=j\}} ].\]</span> By the assumption that <span class="math inline">\(\tau\)</span> is a stopping time, the
indicator <span class="math inline">\(\mathbf{1}_{\{\tau=j\}}\)</span> can be represented as
<span class="math inline">\(\mathbf{1}_{\{\tau=j\}}=G^j(X_0,\dots, X_j)\)</span>, and, because each <span class="math inline">\(X_i\)</span> is just a sum
of the increments <span class="math inline">\(\xi_1, \dots, \xi_i\)</span>, we can actually write <span class="math inline">\(\mathbf{1}_{\{\tau=j\}}\)</span> as a function of
<span class="math inline">\(\xi_1,\dots, \xi_j\)</span> only: <span class="math inline">\(\mathbf{1}_{\{\tau=j\}}=H^j(\xi_1,\dots, \xi_j).\)</span> By
the independence of <span class="math inline">\((\xi_1,\dots, \xi_j)\)</span> from <span class="math inline">\(\xi_k\)</span> (because <span class="math inline">\(j&lt;k\)</span>)
we have
<span class="math display">\[\begin{align}
    {\mathbb{E}}[\xi_k \mathbf{1}_{\{\tau=j\}}]&amp;={\mathbb{E}}[ \xi_k H^j(\xi_1,\dots, \xi_j)]=
   {\mathbb{E}}[\xi_k] {\mathbb{E}}[ H^j(\xi_1,\dots, \xi_j)]={\mathbb{E}}[\xi_k] {\mathbb{E}}[\mathbf{1}_{\{\tau=j\}}]=
   {\mathbb{E}}[\xi_k]{\mathbb{P}}[T=j].
\end{align}\]</span>
Therefore,
<span class="math display">\[\begin{align}
    {\mathbb{E}}[\xi_k \mathbf{1}_{\{k\leq \tau\}}]&amp;={\mathbb{E}}[\xi_k]-\sum_{j=0}^{k-1} {\mathbb{E}}[\xi_k]
   {\mathbb{P}}[\tau=j]={\mathbb{E}}[\xi_k] {\mathbb{P}}[\tau\geq k] =\mu {\mathbb{P}}[\tau\geq k],
\end{align}\]</span>
where the last equality follows from the fact that all <span class="math inline">\(\xi_k\)</span> have the same expectation, namely <span class="math inline">\(\mu\)</span>.</p>
<p>Putting it all together, we get
<span class="math display">\[\begin{align}
    {\mathbb{E}}[X_{\tau}]&amp;={\mathbb{E}}[\sum_{k=1}^{\tau} \xi_k]=\sum_{k=1}^{\infty}
   \mu {\mathbb{P}}[\tau\geq k]=\mu \sum_{k=1}^{\infty} {\mathbb{P}}[\tau\geq
   k]= {\mathbb{E}}[\tau] \mu,
\end{align}\]</span>
where we use the “tail formula” to get the last equality.</p>
</div>
<div class="problem">
<p>Show, by giving an example, that Wald’s identity does not necessarily hold if <span class="math inline">\(\tau\)</span> is not a stopping time.</p>
</div>
<div class="solution">
<p>Let <span class="math inline">\(X\)</span> be a simple symmetric random walk, and let <span class="math inline">\(\tau\)</span> be a random time constructed like this:
<span class="math display">\[\begin{align}
    \tau = \begin{cases} 1, &amp; X_1=1 \\ 0,&amp; X_1=-1. \end{cases}
  \end{align}\]</span>
Then,
<span class="math display">\[\begin{align}
    X_{\tau} = \begin{cases} X_1, &amp; X_1=1 \\ X_0, &amp; X_1=-1, \end{cases} = 
    \begin{cases} 1, &amp; X_1=1 \\ 0,&amp; X_1=-1. \end{cases}
  \end{align}\]</span>
and, therefore, <span class="math inline">\({\mathbb{E}}[ X_{\tau}] = 1 \cdot 1/2 + 0 \cdot 1/2 = 1/2\)</span>. On the other hand <span class="math inline">\(\mu={\mathbb{E}}[\xi_1]=0\)</span> and <span class="math inline">\({\mathbb{E}}[\tau] = 1/2\)</span>, so <span class="math inline">\(1/2 = {\mathbb{E}}[X_{\tau}] \ne {\mathbb{E}}[\tau] \mu = 0\)</span>.</p>
<p>It is clear that <span class="math inline">\(\tau\)</span> cannot be a stopping time, since Wald’s identity would hold for it if it were.
To see that it is not more directly, consider the event when <span class="math inline">\(\tau=0\)</span>.
Its occurrence depends on whether <span class="math inline">\(X_1=1\)</span> or not, which is not known at time <span class="math inline">\(0\)</span>.</p>
</div>
<p>A famous use of Wald’s identity is in the solution of the following classical problem:</p>
<div class="problem">
<p>A gambler starts with <span class="math inline">\(\$x\)</span> dollars and repeatedly plays a game in
which she wins a dollar with probability <span class="math inline">\(\tfrac{1}{2}\)</span> and loses a dollar with
probability <span class="math inline">\(\tfrac{1}{2}\)</span>. She decides to stop when one of the following two
things happens:</p>
<ol style="list-style-type: decimal">
<li><p>she goes bankrupt, i.e., her wealth hits <span class="math inline">\(0\)</span>, or</p></li>
<li><p>she makes enough money, i.e., her wealth reaches some predetermined level <span class="math inline">\(a&gt;x\)</span>.</p></li>
</ol>
<p>The “Gambler’s ruin” problem (dating at least to 1600s) asks
the following question: what is the probability that the gambler will
make <span class="math inline">\(a\)</span> dollars before she goes bankrupt?</p>
</div>
<div class="solution">
<p>Let the gambler’s “wealth” <span class="math inline">\(\{W_n\}_{n\in {\mathbb{N}}_0}\)</span> be
modeled by a simple random walk starting
from <span class="math inline">\(x\)</span>, whose increments <span class="math inline">\(\xi_k=\delta_k\)</span> are coin-tosses. Then
<span class="math inline">\(W_n=x+X_n\)</span>, where <span class="math inline">\(X_n = \sum_{k=1}^n \xi_k\)</span> is a SSRW. Let <span class="math inline">\(\tau\)</span> be the
time the gambler stops. We can represent <span class="math inline">\(\tau\)</span> in two different (but
equivalent) ways. On the one hand, we can think of <span class="math inline">\(T\)</span> as the smaller of
the two hitting times <span class="math inline">\(\tau_{-x}\)</span> and <span class="math inline">\(\tau_{a-x}\)</span> of the levels <span class="math inline">\(-x\)</span> and
<span class="math inline">\(a-x\)</span> for the random walk <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> (remember that <span class="math inline">\(W_n=x+X_n\)</span>, so
these two correspond to the hitting times for the process <span class="math inline">\(\{W_n\}_{n\in {\mathbb{N}}_0}\)</span> of
the levels <span class="math inline">\(0\)</span> and <span class="math inline">\(a\)</span>). On the other hand, we can think of <span class="math inline">\(\tau\)</span> as the
first hitting time of the two-element <em>set</em> <span class="math inline">\(\{-x,a-x\}\)</span> for the
process <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>. In either case, it is quite clear that <span class="math inline">\(\tau\)</span> is a
stopping time (can you write down the decision functions?).</p>
<p>When we
talked about the maximum of the simple symmetric random walk, we proved
that it hits any value if given enough time. Therefore, the probability
that the gambler’s wealth will remain strictly between <span class="math inline">\(0\)</span> and <span class="math inline">\(a\)</span>
forever is zero. So, <span class="math inline">\({\mathbb{P}}[T&lt;\infty]=1\)</span>.</p>
<p>What can we say about the random variable <span class="math inline">\(X_{\tau}\)</span> - the gambler’s wealth
(minus <span class="math inline">\(x\)</span>) at the <em>random</em> time <span class="math inline">\(\tau\)</span>? Clearly, it is either equal to
<span class="math inline">\(-x\)</span> or to <span class="math inline">\(a-x\)</span>, and the probabilities <span class="math inline">\(p_0\)</span> and <span class="math inline">\(p_a\)</span> with which it
takes these values are exactly what we are after in this problem. We
know that, since there are no other values <span class="math inline">\(X_{\tau}\)</span> can take, we must have
<span class="math inline">\(p_0+p_a=1\)</span>. Wald’s identity gives us another equation for
<span class="math inline">\(p_0\)</span> and <span class="math inline">\(p_a\)</span>:
<span class="math display">\[{\mathbb{E}}[X_{\tau}]={\mathbb{E}}[\xi_1] {\mathbb{E}}[\tau]=0\cdot {\mathbb{E}}[\tau]=0 \text{ so that }
0 = {\mathbb{E}}[X_{\tau}]=p_0 (-x)+p_a (a-x).\]</span></p>
<p>We now have a system of two linear equations with two unknowns, and solving it yields
<span class="math display">\[p_0= \frac{a-x}{a}, \ p_a=\frac{x}{a}.\]</span>
It is remarkable that the two probabilities are proportional to the amounts of
money the gambler needs to make (lose) in the two outcomes. The
situation is different when <span class="math inline">\(p\not=\tfrac{1}{2}\)</span>.</p>
</div>
<p>In order to be able to use Wald’s identity, we need to check its conditions. We have already seen that
<span class="math inline">\(\tau\)</span> needs to be a stopping time, and not just any old random time. There are also two conditions about the expected values of <span class="math inline">\(\tau\)</span> and of <span class="math inline">\(\xi_1\)</span>. If you read the above solution carefully, you will realize that we never checked whether <span class="math inline">\({\mathbb{E}}[\tau]&lt;\infty\)</span>. We should have, but we did not because we still don’t have the mathematical tools to do it. We will see later that, indeed, <span class="math inline">\({\mathbb{E}}[\tau]&lt;\infty\)</span> for this particular stopping time. In general, the condition that <span class="math inline">\({\mathbb{E}}[\tau]&lt;\infty\)</span> is important, as the following simple example shows:</p>
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple symmetric random walk, and let <span class="math inline">\(\tau_1\)</span> be the first hitting time of the level <span class="math inline">\(1\)</span>.
Use Wald’s identity to show that <span class="math inline">\({\mathbb{E}}[\tau]=+\infty\)</span>.</p>
</div>
<div class="solution">
<p>Suppose, to the contrary, that <span class="math inline">\({\mathbb{E}}[\tau]&lt;\infty\)</span>. Since <span class="math inline">\({\mathbb{E}}[\delta_1]&lt;\infty\)</span> and <span class="math inline">\(\tau_1\)</span> is a stopping time, Wald’s identity applies:
<span class="math display">\[ {\mathbb{E}}[X_{\tau_1}] = {\mathbb{E}}[ \delta_1] \cdot {\mathbb{E}}[\tau_1].\]</span>
The right hand side is then equal to <span class="math inline">\(0\)</span> because <span class="math inline">\({\mathbb{E}}[\delta_1]=0\)</span>. On the other hand, <span class="math inline">\(X_{\tau_1}=1\)</span>: the value of <span class="math inline">\(X_n\)</span> when it first hits the level <span class="math inline">\(1\)</span> is, of course, <span class="math inline">\(1\)</span>. This leads to a contradiction <span class="math inline">\(1={\mathbb{E}}[X_{\tau_1}] = {\mathbb{E}}[\delta_1] {\mathbb{E}}[\tau_1] = 0\)</span>. Therefore, our initial assumption that <span class="math inline">\({\mathbb{E}}[\tau_1]&lt;\infty\)</span> was wrong!</p>
</div>
</div>
<div id="additional-problems-for-chapter-4" class="section level2" number="4.4">
<h2 number="4.4"><span class="header-section-number">4.4</span> Additional problems for Chapter 4</h2>
<!--
  3-max-problems
  ------------------------------------------------
-->
<div class="problem">
<ol style="list-style-type: decimal">
<li><p>Let <span class="math inline">\(\{X_n\}_{0\leq n \leq 10}\)</span> be a simple symmetric random walk with
time horizon <span class="math inline">\(T=10\)</span>. What is the probability it will never reach the
level <span class="math inline">\(5\)</span>?</p></li>
<li><p>A fair coin is tossed repeatedly, with the first toss resulting in <span class="math inline">\(H\)</span>
(i.e., heads). After that, each time the outcome of the coin matches the
previous outcome, the player gets a dollar. If the two do not match, the
player has to pay a dollar. The player stops playing once she “earns”
<span class="math inline">\(10\)</span> dollars. What is the probability that she will need at least 20
tosses (including the first one) to achieve that?</p></li>
<li><p>A fair coin is tossed repeatedly and the record of the outcomes is kept.
Tossing stops the moment the total number of heads obtained so far
exceeds the total number of tails by 3. For example, a possible sequence
of tosses could look like <em>HHTTTHHTHHTHH</em>. What is the probability that
the length of such a sequence is at most 10?</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part>
This is the same as asking that its maximum <span class="math inline">\(M_{10}\)</span> up to time <span class="math inline">\(T=10\)</span>
be <span class="math inline">\(\leq 4\)</span>. This can be further computed using the formula from class:
<span class="math display">\[\begin{aligned}
{\mathbb{P}}[ M_{10}\leq 4 ] &amp;= {\mathbb{P}}[ M_{10}=0] + {\mathbb{P}}[ M_{10}=1] + {\mathbb{P}}[ M_{10} = 2]
+ {\mathbb{P}}[M_{10} = 3] + {\mathbb{P}}[ M_{10} = 4] \\ &amp; = 
  ({\mathbb{P}}[ X_{10} = 0] + {\mathbb{P}}[ X_{10} = 1] ) +
({\mathbb{P}}[ X_{10} = 1] + {\mathbb{P}}[ X_{10} = 2] ) \\ &amp; +
  ({\mathbb{P}}[ X_{10} = 2] + {\mathbb{P}}[ X_{10} = 3] ) +
({\mathbb{P}}[ X_{10} = 3] + {\mathbb{P}}[ X_{10} = 4] )\\ &amp; +
  ({\mathbb{P}}[ X_{10} = 4] + {\mathbb{P}}[ X_{10} = 5] ) \\ &amp;=
  2 ({\mathbb{P}}[ X_{10}=4]  + {\mathbb{P}}[X_{10} = 2]) + {\mathbb{P}}[X_{10} =0] \\&amp;=
  2^{-10}( 2 \binom{10}{7} + 2 \binom{10}{6} + \binom{10}{5})\end{aligned}\]</span></p>
<p><part> 2. </part></p>
<p>Let the outcomes of the coin tosses be denoted by <span class="math inline">\(\gamma_1 = H\)</span>,
<span class="math inline">\(\gamma_2, \gamma_3, \dots\)</span>. We define the random variables
<span class="math inline">\(\delta_1,\delta_2,\dots\)</span> as follows: <span class="math inline">\(\delta_1 = 1\)</span> if <span class="math inline">\(\gamma_2 = T\)</span>
and <span class="math inline">\(\delta_1 = -1\)</span>, otherwise. Similarly, <span class="math inline">\(\delta_2 = 1\)</span> if <span class="math inline">\(\gamma_3 =  \gamma_2\)</span> and <span class="math inline">\(-1\)</span> otherwise. It is clear that
<span class="math inline">\(\delta_1,\delta_2,\dots\)</span> is an iid sequence of coin tosses (just like
in the definition of) of a simple symmetric random walk. After <span class="math inline">\(n\)</span>
tosses ( the first one), our gambler has
<span class="math inline">\(X_n = \delta_1+\delta_2 + \dots + \delta_n\)</span> dollars. She will need at
least 19 tosses (excluding the first one) to reach <span class="math inline">\(10\)</span> dollars if and
only if the value of the running maximum process at time <span class="math inline">\(n=18\)</span> is at
most <span class="math inline">\(9\)</span>. Using the formula from the formula sheet, this evaluates to
<span class="math display">\[\begin{aligned}
    {\mathbb{P}}[ M_{18}\leq 9] &amp;= \sum_{k=0}^{9} {\mathbb{P}}[ M_{18} = k] = 
    \sum_{k=0}^{9} ({\mathbb{P}}[X_{18}=k] + {\mathbb{P}}[ X_{18} = k+1])\\ 
                       &amp; = {\mathbb{P}}[ X_{18} =0 ]
      + 2\, {\mathbb{P}}[X_{18} = 2] + 2\, {\mathbb{P}}[X_{18} = 4] + \dots \\ &amp; \qquad   \dots + 2\, {\mathbb{P}}[ X_{18} = 8] +
    {\mathbb{P}}[ X_{18} = 10] \\
                       &amp;= 2^{-18}\left( \binom{18}{9} + 2 \binom{18}{10} +
                       2\binom{18}{11} + 2 \binom{18}{12} + 2
                       \binom{18}{13} + \binom{18}{14}\right)
  \end{aligned}\]</span> Btw, you could have gotten a seemingly different
answer. Since it is impossible to reach <span class="math inline">\(10\)</span> in exactly <span class="math inline">\(19\)</span> steps (the
parity is wrong), the required probability is also equal to
<span class="math display">\[\begin{align}
  {\mathbb{P}}[ M_{19}\leq 9] &amp;= \sum_{k=0}^9 \Big( {\mathbb{P}}[ X_{19} = k] +  {\mathbb{P}}[ X_{19} =
    k+1] \Big)
    = 2^{-19} \times 2 \times \sum_{k=1}^9 \binom{19}{(19+k)/2}\\
    &amp;=
    2^{-18} \times \left( \binom{19}{10} + \binom{19}{11} + \dots +
  \binom{19}{16} \right).
\end{align}\]</span></p>
<p><part> 3. </part></p>
<p>Let <span class="math inline">\(X_n\)</span>, <span class="math inline">\(n\in{\mathbb{N}}_0\)</span> be the number of heads <em>minus</em> the number of tails
obtained so far. Then, <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> is a simple symmetric random walk, and
we stop tossing the coin when <span class="math inline">\(X\)</span> hits <span class="math inline">\(3\)</span> for the first time. This will
happen during the first 10 tosses, if and only if <span class="math inline">\(M_{10} \geq 3\)</span>,
where <span class="math inline">\(M_n\)</span> denotes the (running) maximum of <span class="math inline">\(X\)</span>. According to the
reflection principle, <span class="math display">\[\nonumber
    \begin{split}
  {\mathbb{P}}[M_{10} \geq 3]&amp;= {\mathbb{P}}[ X_{10} \geq 3 ] + {\mathbb{P}}[ X_{10} \geq 4]\\ &amp;
 = 2(
 {\mathbb{P}}[X_{10}= 4]
 +{\mathbb{P}}[X_{10}= 6]
 +{\mathbb{P}}[X_{10}= 8]
 +{\mathbb{P}}[X_{10}= 10])\\
 &amp;= 2^{-9} \left[
 \binom{10}{3}+\binom{10}{2}+\binom{10}{1}+\binom{10}{0}
 \right] =  {\frac{11}{32}}.
    \end{split}\]</span></p>
</div>
<!--
  time_until_hit
  ------------------------------------------------
-->
<div class="problem">
<p>The purpose of this problem is to understand how long we have to wait util a simple symmetric random walk hits the level <span class="math inline">\(1\)</span>. Theory presented so far guarantees that this will happen sooner or later, but it gives no indication of the length of the wait. As usual, we denote by <span class="math inline">\(\tau_1\)</span> the (random) first time the SSRW <span class="math inline">\(\{X_n\}_{n\in{\mathbb{N}}_0}\)</span> hits the level <span class="math inline">\(1\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Write an R function that simulates a trajectory of a random walk, but only until the first time it hits level <span class="math inline">\(1\)</span>. You don’t have to record the trajectory itself - just keep tossing coins until the trajectory hits <span class="math inline">\(1\)</span> and return the number of steps needed. Your function needs to accept an argument, <code>T</code>, such that your simulation stops if <span class="math inline">\(1\)</span> has not been reached in the first <code>T</code> steps.</p></li>
<li><p>Pick a large-ish value of the parameter <code>T</code> (say <span class="math inline">\(100\)</span>) and <code>replicate</code> the simulation from 1. above sufficiently many times (say <span class="math inline">\(10,000\)</span>). Draw a histogram of your results.</p></li>
<li><p>Repeat the simulation for the following values of <span class="math inline">\(T\)</span>: <span class="math inline">\(500\)</span>, <span class="math inline">\(1,000\)</span>, <span class="math inline">\(10,000\)</span>, <span class="math inline">\(50,000\)</span>, <span class="math inline">\(100,000\)</span>, and compute the mean and the standard deviation of your simulations. Display your results in two tables. Are these numbers underestimates or overestimates of <span class="math inline">\({\mathbb{E}}[\tau_1]\)</span> and <span class="math inline">\(\operatorname{Var}[\tau_1]\)</span>? Explain why. (Note: Decrease the number <code>nsim</code> of simulations to <span class="math inline">\(1000\)</span> or even <span class="math inline">\(100\)</span> if <span class="math inline">\(10,000\)</span> is taking too long.)</p></li>
<li><p>Repeat all of the above, but for the first time the <strong>absolute value</strong> of your random walk reaches level <span class="math inline">\(5\)</span>. What is the most glaring difference between the two cases? What does that mean for the amount of time you are going to have to wait to hit <span class="math inline">\(1\)</span>, vs. for the absolute value to hit <span class="math inline">\(5\)</span>? More precisely, what do you think their means and standard deviations are?</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true"></a>simulate_tau =<span class="st"> </span><span class="cf">function</span>(T) {</span>
<span id="cb130-2"><a href="#cb130-2" aria-hidden="true"></a>    X =<span class="st"> </span><span class="dv">0</span></span>
<span id="cb130-3"><a href="#cb130-3" aria-hidden="true"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>T) {</span>
<span id="cb130-4"><a href="#cb130-4" aria-hidden="true"></a>        X =<span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="dt">size =</span> <span class="dv">1</span>)</span>
<span id="cb130-5"><a href="#cb130-5" aria-hidden="true"></a>        <span class="cf">if</span> (X <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) </span>
<span id="cb130-6"><a href="#cb130-6" aria-hidden="true"></a>            <span class="cf">break</span></span>
<span id="cb130-7"><a href="#cb130-7" aria-hidden="true"></a>    }</span>
<span id="cb130-8"><a href="#cb130-8" aria-hidden="true"></a>    <span class="kw">return</span>(n)</span>
<span id="cb130-9"><a href="#cb130-9" aria-hidden="true"></a>}</span></code></pre></div>
<p><part> 2. </part></p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true"></a>T =<span class="st"> </span><span class="dv">100</span></span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true"></a>tau =<span class="st"> </span><span class="kw">replicate</span>(nsim, <span class="kw">simulate_tau</span>(T))</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true"></a><span class="kw">hist</span>(tau, <span class="dt">probability =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-311-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><part> 3. </part></p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true"></a>T =<span class="st"> </span><span class="kw">c</span>(<span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">50000</span>, <span class="dv">100000</span>)</span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true"></a>Mean =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">length =</span> <span class="dv">5</span>)</span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true"></a>StDev =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">length =</span> <span class="dv">5</span>)</span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true"></a></span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {</span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true"></a>    tau =<span class="st"> </span><span class="kw">replicate</span>(nsim, <span class="kw">simulate_tau</span>(T[i]))</span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true"></a>    Mean[i] =<span class="st"> </span><span class="kw">mean</span>(tau)</span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true"></a>    StDev[i] =<span class="st"> </span><span class="kw">sd</span>(tau)</span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true"></a>}</span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true"></a>df =<span class="st"> </span><span class="kw">data.frame</span>(T, Mean, StDev)</span>
<span id="cb132-12"><a href="#cb132-12" aria-hidden="true"></a><span class="kw">options</span>(<span class="dt">scipen =</span> <span class="dv">50</span>)  <span class="co"># no scientific (e) notation</span></span>
<span id="cb132-13"><a href="#cb132-13" aria-hidden="true"></a><span class="kw">print</span>(df)</span>
<span id="cb132-14"><a href="#cb132-14" aria-hidden="true"></a><span class="co">##        T    Mean      StDev</span></span>
<span id="cb132-15"><a href="#cb132-15" aria-hidden="true"></a><span class="co">## 1    500  31.387   97.52087</span></span>
<span id="cb132-16"><a href="#cb132-16" aria-hidden="true"></a><span class="co">## 2   1000  43.499  158.33426</span></span>
<span id="cb132-17"><a href="#cb132-17" aria-hidden="true"></a><span class="co">## 3  10000 196.021 1156.39220</span></span>
<span id="cb132-18"><a href="#cb132-18" aria-hidden="true"></a><span class="co">## 4  50000 264.321 2864.91406</span></span>
<span id="cb132-19"><a href="#cb132-19" aria-hidden="true"></a><span class="co">## 5 100000 426.744 5077.33921</span></span></code></pre></div>
<p><part> 4. </part></p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true"></a>simulate_tau_abs =<span class="st"> </span><span class="cf">function</span>(T) {</span>
<span id="cb133-2"><a href="#cb133-2" aria-hidden="true"></a>    X =<span class="st"> </span><span class="dv">0</span></span>
<span id="cb133-3"><a href="#cb133-3" aria-hidden="true"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>T) {</span>
<span id="cb133-4"><a href="#cb133-4" aria-hidden="true"></a>        X =<span class="st"> </span>X <span class="op">+</span><span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="dt">size =</span> <span class="dv">1</span>)</span>
<span id="cb133-5"><a href="#cb133-5" aria-hidden="true"></a>        <span class="cf">if</span> (<span class="kw">abs</span>(X) <span class="op">==</span><span class="st"> </span><span class="dv">5</span>) </span>
<span id="cb133-6"><a href="#cb133-6" aria-hidden="true"></a>            <span class="cf">break</span></span>
<span id="cb133-7"><a href="#cb133-7" aria-hidden="true"></a>    }</span>
<span id="cb133-8"><a href="#cb133-8" aria-hidden="true"></a>    <span class="kw">return</span>(n)</span>
<span id="cb133-9"><a href="#cb133-9" aria-hidden="true"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb134-2"><a href="#cb134-2" aria-hidden="true"></a>T =<span class="st"> </span><span class="dv">100</span></span>
<span id="cb134-3"><a href="#cb134-3" aria-hidden="true"></a>tau_abs =<span class="st"> </span><span class="kw">replicate</span>(nsim, <span class="kw">simulate_tau_abs</span>(T))</span>
<span id="cb134-4"><a href="#cb134-4" aria-hidden="true"></a><span class="kw">hist</span>(tau_abs, <span class="dt">probability =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-314-1.png" width="672" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true"></a>T =<span class="st"> </span><span class="kw">c</span>(<span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">10000</span>, <span class="dv">50000</span>, <span class="dv">100000</span>)</span>
<span id="cb135-2"><a href="#cb135-2" aria-hidden="true"></a>Mean =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">length =</span> <span class="dv">5</span>)</span>
<span id="cb135-3"><a href="#cb135-3" aria-hidden="true"></a>StDev =<span class="st"> </span><span class="kw">vector</span>(<span class="dt">length =</span> <span class="dv">5</span>)</span>
<span id="cb135-4"><a href="#cb135-4" aria-hidden="true"></a></span>
<span id="cb135-5"><a href="#cb135-5" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">10000</span></span>
<span id="cb135-6"><a href="#cb135-6" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {</span>
<span id="cb135-7"><a href="#cb135-7" aria-hidden="true"></a>    tau_abs =<span class="st"> </span><span class="kw">replicate</span>(nsim, <span class="kw">simulate_tau_abs</span>(T[i]))</span>
<span id="cb135-8"><a href="#cb135-8" aria-hidden="true"></a>    Mean[i] =<span class="st"> </span><span class="kw">mean</span>(tau_abs)</span>
<span id="cb135-9"><a href="#cb135-9" aria-hidden="true"></a>    StDev[i] =<span class="st"> </span><span class="kw">sd</span>(tau_abs)</span>
<span id="cb135-10"><a href="#cb135-10" aria-hidden="true"></a>}</span>
<span id="cb135-11"><a href="#cb135-11" aria-hidden="true"></a>df =<span class="st"> </span><span class="kw">data.frame</span>(T, Mean, StDev)</span>
<span id="cb135-12"><a href="#cb135-12" aria-hidden="true"></a><span class="kw">options</span>(<span class="dt">scipen =</span> <span class="dv">50</span>)  <span class="co"># no scientific (e) notation</span></span>
<span id="cb135-13"><a href="#cb135-13" aria-hidden="true"></a><span class="kw">print</span>(df)</span>
<span id="cb135-14"><a href="#cb135-14" aria-hidden="true"></a><span class="co">##        T    Mean    StDev</span></span>
<span id="cb135-15"><a href="#cb135-15" aria-hidden="true"></a><span class="co">## 1    500 24.9444 20.01138</span></span>
<span id="cb135-16"><a href="#cb135-16" aria-hidden="true"></a><span class="co">## 2   1000 25.1092 19.88474</span></span>
<span id="cb135-17"><a href="#cb135-17" aria-hidden="true"></a><span class="co">## 3  10000 25.2926 20.09310</span></span>
<span id="cb135-18"><a href="#cb135-18" aria-hidden="true"></a><span class="co">## 4  50000 24.9758 19.94961</span></span>
<span id="cb135-19"><a href="#cb135-19" aria-hidden="true"></a><span class="co">## 5 100000 25.2972 20.35415</span></span></code></pre></div>
<p>The most glaring difference between two tables is that the mean and st-dev estimates seem to grow with <span class="math inline">\(T\)</span> in the first, but not in the second case. It suggests that the random variable
<span class="math inline">\(\tau\)</span> takes such large values that no “cap” <span class="math inline">\(T\)</span> can “contain them”. More precisely, the random variable <span class="math inline">\(\tau\)</span> has <strong>infinite expectation</strong> (and also <strong>infinite standard deviation</strong>). Indeed, it its expectation were finite, the value in the “Mean” column would stabilize towards it. Since they don’t, this expectation is infinite. Same for standard deviation. The moral of the story is that even though simple symmetric random walks hit every level eventually, you may have to wait a long time for that to happen.</p>
<p>This does not happen for <code>tau_abs</code>. Indeed, it can be shown that both its expectation and standard deviation are finite. The time you are going wait until you hit either <span class="math inline">\(-5\)</span> or <span class="math inline">\(5\)</span> is much shorter “on average” than the time needed to hit <span class="math inline">\(1\)</span>.</p>
</div>
<!-- 
  Luke_cookies
  -------------------------------
-->
<div class="problem">
<p>Luke starts a random walk, where each step takes him to the left or to
the right, with the two alternatives being equally likely and
independent of the previous steps. <span class="math inline">\(11\)</span> steps to his right is a cookie
jar, and Luke gets to take a (single) cookie every time he reaches that
position. He performs exactly <span class="math inline">\(15\)</span> steps, and then stops.</p>
<ol style="list-style-type: decimal">
<li><p>What is the probability that Luke will be exactly by the cookie jar
when he stops?</p></li>
<li><p>What is the probability that Luke stops with with exactly <span class="math inline">\(3\)</span>
cookies in his hand?</p></li>
<li><p>What is the probability that Luke stops with at least one cookie in
his hand?</p></li>
<li><p>Suppose now that we place a bowl of broccoli soup one step to the
right of the cookie jar. It smells so bad that, if reached, Luke
will throw away all the cookies he is currently carrying (if any)
and run away pinching his nose. What is the probability that Luke
will finish his <span class="math inline">\(15\)</span>-step walk without ever encountering the yucky
bowl of broccoli soup and with at least one cookie in his hand?</p></li>
</ol>
</div>
<div class="solution">
<p>Let the position at time <span class="math inline">\(n\)</span> be denoted by <span class="math inline">\(X_n\)</span>, so that
<span class="math inline">\(\{X_n\}_{0\leq n \leq 15}\)</span> is a simple symmetric random walk with the
time horizon <span class="math inline">\(T=15\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>This is simply <span class="math inline">\({\mathbb{P}}[ X_{15} = 11] = \binom{15}{2} 2^{-15} =  \binom{15}{13} 2^{-15}\)</span>.</p></li>
<li><p>The only way for Luke to return with <span class="math inline">\(3\)</span> cookies is to go straight
to <span class="math inline">\(11\)</span>, step away from it, return, step away from it and return
again. There are exactly <span class="math inline">\(4\)</span> paths that do that. They all start with
<span class="math inline">\(11\)</span> <span class="math inline">\(+1\)</span>s (or "up"s or "right"s) and then continue in one of
the following 4 ways
<span class="math display">\[(+1,-1,+1,-1), (+1,-1,-1,+1), (-1,+1,-1,+1) \text{ and }
    (-1,+1,+1,-1).\]</span> Therefore, the probability is
<span class="math inline">\(4/2^{15} = 2^{-13}\)</span>.</p></li>
<li><p>Luke will stop with at least one cookie in his hand, if and only if
the maximal (i.e., right-most) position of his walk is <span class="math inline">\(11\)</span> or
above. Therefore, the required probability is
<span class="math inline">\({\mathbb{P}}[ M_{15} \geq 11]\)</span>. Using the formula
<span class="math inline">\({\mathbb{P}}[ M_T=k] = {\mathbb{P}}[X_T = k]  + {\mathbb{P}}[ X_T = k+1]\)</span> we get <span class="math display">\[\begin{aligned}
        {\mathbb{P}}[ M_{15} \geq 11] 
        &amp;= {\mathbb{P}}[ M_{15} = 11] + {\mathbb{P}}[ M_{15} = 12] +
        \dots + {\mathbb{P}}[ M_{15}=15]\\ &amp; = {\mathbb{P}}[ X_{15}=11] + 2 {\mathbb{P}}[ X_{15} = 13] + 2
        {\mathbb{P}}[X_{15}=15] \\
                                  &amp;= 2^{-15}\Big( \binom{15}{2} + 2
                                  \binom{15}{1} + 2\binom{15}{0}\Big).
     \end{aligned}\]</span></p></li>
<li><p>Here, we want Luke to reach the position <span class="math inline">\(11\)</span> (to get a cookie), but
not the position <span class="math inline">\(12\)</span> (where the bowl of broccoli soup is). This
corresponds to the maximum being exactly <span class="math inline">\(11\)</span>. By the formula
<span class="math inline">\({\mathbb{P}}[ M_T = k] =  {\mathbb{P}}[X_T=k] + {\mathbb{P}}[X_T = k+1]\)</span>, we get
<span class="math display">\[{\mathbb{P}}[ M_{15}=11] = {\mathbb{P}}[X_{15}=11] = \binom{15}{2}2^{-15}.\]</span></p></li>
</ol>
</div>
<!--
  catalan
  ------------------------------------------------
-->
<div class="problemec">
<p>Let <span class="math inline">\(C_n = \frac{1}{n+1}\binom{2n}{n}\)</span> denote the <span class="math inline">\(n\)</span>-th Catalan number, as defined at the end of the discussion of the Balot problem above.</p>
<ol style="list-style-type: decimal">
<li><p>Use the reflection principle to show that <span class="math inline">\(C_n\)</span> is the number
trajectories <span class="math inline">\((x_0,\dots, x_{2n})\)</span> of a random
walk with time horizon <span class="math inline">\(T=2n\)</span> such that <span class="math inline">\(x_k  \geq 0\)</span>, for all <span class="math inline">\(k\in\{0,1,\dots, 2n\}\)</span> and <span class="math inline">\(x_{2n}=0\)</span>.</p></li>
<li><p>Prove the <em>Segner’s recurrence formula</em>
<span class="math inline">\(C_{n+1} = \sum_{i=0}^n C_{i} C_{n-i}\)</span>. .</p></li>
<li><p>Show that <span class="math inline">\(C_n\)</span> is the number of ways the vertices of a regular
<span class="math inline">\(2n\)</span>-gon can be paired so that the line segments joining paired
vertices do not intersect.</p></li>
</ol>
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/01_Random_Walks/catalan_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->
<!--
  no_return_to_zero
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple symmetric random walk. Given <span class="math inline">\(n\in{\mathbb{N}}\)</span>, what
is the probability that <span class="math inline">\(X\)</span> does not visit <span class="math inline">\(0\)</span> during the time interval
<span class="math inline">\(1,\dots, n\)</span>.</p>
</div>
<div class="solution">
<p>Let us denote the required probability by <span class="math inline">\(p_n\)</span>, i.e.,
<span class="math display">\[p_n={\mathbb{P}}[ X_1\not= 0, X_2\not=0, \dots, X_n\not=0].\]</span> For <span class="math inline">\(n=1\)</span>,
<span class="math inline">\(p_1=1\)</span>, since <span class="math inline">\(X_1\)</span> is either <span class="math inline">\(1\)</span> or <span class="math inline">\(-1\)</span>. For <span class="math inline">\(n&gt;1\)</span>, let <span class="math inline">\(\delta_1\)</span> be
the first increment <span class="math inline">\(\delta_1=X_1-X_0=X_1\)</span>. If <span class="math inline">\(\delta_1=-1\)</span>, we need to
compute that probability that a random walk of length <span class="math inline">\(n-1\)</span>, starting at
<span class="math inline">\(-1\)</span>, does not hit <span class="math inline">\(0\)</span>. This probability is, in turn, the same as the
probability that a random walk of length <span class="math inline">\(n-1\)</span>, starting from <span class="math inline">\(0\)</span>, never
hits <span class="math inline">\(1\)</span>. By the symmetry of the increments, the same reasoning works
for the case <span class="math inline">\(\delta_1=1\)</span>. Therefore,
<span class="math display">\[\begin{align}
  p_n &amp;=
  {\mathbb{P}}[ X_1\leq 0, X_2\leq 0, \dots, X_{n-1}\leq 0]\, {\mathbb{P}}[\delta_1=-1]\\ 
    &amp; \quad+ 
  {\mathbb{P}}[ X_1\geq 0, X_2\geq 0, \dots, X_{n-1}\leq 0]\, {\mathbb{P}}[\delta_1=1]\\
    &amp;=\tfrac{1}{2}{\mathbb{P}}[ M_{n-1}=0] + \tfrac{1}{2}{\mathbb{P}}[M_{n-1}=0] = {\mathbb{P}}[ M_{n-1}=0], 
\end{align}\]</span>
where <span class="math inline">\(M_n=\max\{X_0,\dots, X_n\}\)</span>. Using the formula from the notes,
this probability is given by <span class="math display">\[p_n= 2^{-n+1} \binom{n-1}{ \lfloor n/2 \rfloor},\]</span>
where <span class="math inline">\(\lfloor x \rfloor\)</span> denotes the largest integer <span class="math inline">\(\leq x\)</span>.</p>
</div>
<!--
  hit_times_1
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(\tau_{-1}\)</span> be the hitting time of the level <span class="math inline">\({-1}\)</span> for a simple biased
random walk <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>. Choose the correct answer(s) (they will depend on the value of the
parameter <span class="math inline">\(p\)</span>):</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\({\mathbb{P}}[\tau_{-1}&lt;\infty] &lt; 1\)</span></li>
<li><span class="math inline">\({\mathbb{E}}[\tau_{-1}]&lt;\infty\)</span>,</li>
<li><span class="math inline">\({\mathbb{P}}[\tau_{-1}&lt;\infty]=1\)</span> and <span class="math inline">\({\mathbb{E}}[\tau_{-1}]=\infty\)</span>,</li>
<li><span class="math inline">\({\mathbb{P}}[\tau_{-1}&lt;\infty]&lt;1\)</span> and <span class="math inline">\({\mathbb{E}}[\tau_{-1}]=\infty\)</span>,</li>
<li>none of the above.</li>
</ol>
</div>
<div class="solution">

<p>Hitting the level <span class="math inline">\(-1\)</span> for a biased random walk with parameter <span class="math inline">\(p\)</span> is equivalent to hitting the level <span class="math inline">\(1\)</span> for a biased random walk with parameter <span class="math inline">\(1-p\)</span>. The correct answers are:</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:center;">
p
</th>
<th style="text-align:left;">
answers
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;">
<span class="math inline">\(&lt;\frac{1}{2}\)</span>
</td>
<td style="text-align:left;">
<ol start="2" style="list-style-type: lower-alpha">
<li></td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(=\frac{1}{2}\)</span>
</td>
<td style="text-align:left;">
<ol start="3" style="list-style-type: lower-alpha">
<li></td>
</tr>
<tr>
<td style="text-align:center;">
<span class="math inline">\(&gt;\frac{1}{2}\)</span>
</td>
<td style="text-align:left;">
a., d. 
</td>
</tr>
</tbody>
</table></li>
</ol></li>
</ol>
</div>
<!--
  min_stopping_times
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(\tau\)</span> and <span class="math inline">\(\tilde{\tau}\)</span> be two stopping times. Which of the following are necessarily stopping times, as well:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\max(\tau,\widetilde{\tau})\)</span></li>
<li><span class="math inline">\(\min(\tau, \widetilde{\tau})\)</span></li>
<li><span class="math inline">\(\tau+\widetilde{\tau}\)</span></li>
<li><span class="math inline">\(\tau-\widetilde{\tau}\)</span> (assuming that <span class="math inline">\(\tau \geq \widetilde{\tau}\)</span>)</li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part>
Yes. Wait for both of them to happen and then stop.</p>
<p><part> 2. </part>
Yes. Wait for one of them to happen and stop immediately.</p>
<p><part> 3. </part>
Yes. Wait for the earlier one to happen, and write down the time when it happens (call it <span class="math inline">\(n\)</span>). Then wait for the later one to happen, and wait additional <span class="math inline">\(n\)</span> units of time after that. Then stop.</p>
<p><part> 4. </part>
No. Let <span class="math inline">\(\tau=\tau_2\)</span> and <span class="math inline">\(\widetilde{\tau} = \tau_1\)</span> (the hitting times of levels <span class="math inline">\(2\)</span> and <span class="math inline">\(1\)</span>). It may happen that <span class="math inline">\(\tau_1=13\)</span> and then <span class="math inline">\(\tau_2=14\)</span>, but you don’t know whether it will at time <span class="math inline">\(\widetilde{\tau} - \tau = 1\)</span>.</p>
</div>
<!--
  stopping_times_2
  ------------------------------------------------
-->
<div class="problem">
<p>Either one of the following <span class="math inline">\(4\)</span> random times is <em>not</em> a stopping
time for a simple random walk <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>, or they all are. Choose the
one which is not in the first case, or choose e. if you think they all
are.</p>
<ol style="list-style-type: lower-alpha">
<li>the first hitting time of level <span class="math inline">\(4\)</span>,</li>
<li>the first time <span class="math inline">\(n \geq 1\)</span> such that <span class="math inline">\(X_{n}-X_{n-1}\not=X_{1}\)</span>,</li>
<li>the first time the walk hits the level <span class="math inline">\(2\)</span> or the first time the walk sinks below
<span class="math inline">\(-5\)</span>, whatever happens <em>first</em>,</li>
<li>the second time the walk hits the level <span class="math inline">\(5\)</span> or the third time the walk hits the level <span class="math inline">\(-2\)</span>,
whatever happens <em>last</em></li>
<li>none of the above.</li>
</ol>
</div>
<div class="solution">
<p>The correct answer is e. The first, second, third, or … hitting times of a level are
stopping times, and so are their minima or maxima. Note that for two
stopping times <span class="math inline">\(\tau_1\)</span> and <span class="math inline">\(\tau_2\)</span>, the one that happens <em>first</em> is
<span class="math inline">\(\min(\tau_1,\tau_2)\)</span> and the one that happens <em>last</em> is <span class="math inline">\(\max(\tau_1,\tau_2)\)</span>.</p>
</div>
<!--
  stopping_times_3
  ------------------------------------------------
-->
<div class="problem">
<p>At most one of the following <span class="math inline">\(4\)</span> random times <em>is</em> a stopping time
for a simple random walk <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>. Either choose the one which you
think is a stopping time, or choose e. if you think there are no
stopping times among them.</p>
<ol style="list-style-type: lower-alpha">
<li>the first time <span class="math inline">\(n\)</span> such that <span class="math inline">\(X_{n+1} &gt; X_n\)</span>,</li>
<li>the first time <span class="math inline">\(n\geq 1\)</span> such that <span class="math inline">\(X_{n}-X_{n-1}=X_{3}\)</span>,</li>
<li>the first time <span class="math inline">\(n\geq 0\)</span> such that <span class="math inline">\(X_n\)</span> gets (strictly) above its average
<span class="math inline">\(\frac{1}{101}\sum_{k=0}^{100} X_k\)</span>,</li>
<li><span class="math inline">\(\tfrac{1}{2}(\tau_1 + \tau_3)\)</span>, where <span class="math inline">\(\tau_a\)</span> denotes the first hitting time of the level <span class="math inline">\(a\)</span>.</li>
<li>none of the above are stopping times.</li>
</ol>
</div>
<div class="solution">
<p>The correct answer is e. For each of the random times in a.-d. you need
to know something about the future to tell whether they happened when
they happened. For example, for <span class="math inline">\(c.\)</span>, you have no way of knowing (in
general) whether or not <span class="math inline">\(X_{2} - X_1\)</span> equals <span class="math inline">\(X_3\)</span> at time <span class="math inline">\(2\)</span>.</p>
</div>
<p>⬇︎ In case you were wondering, the text below belongs to footnotes from somewhere high above.⬇︎</p>
<!--chapter:end:04-more-random-walks.Rmd-->
</div>
</div>
<div id="markov-chains" class="section level1" number="5">
<h1 number="5"><span class="header-section-number">5</span> Markov Chains</h1>
<div style="counter-reset: thechapter 5;">

</div>
<div id="the-markov-property" class="section level2" number="5.1">
<h2 number="5.1"><span class="header-section-number">5.1</span> The Markov property</h2>
<p>Simply put, a stochastic process has the <strong>Markov property</strong> if probabilities governing its
future evolution depend only on its current position, and not on how it
got there. Here is a more precise, mathematical, definition. It will be
assumed throughout this course that any stochastic process <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>
takes values in a countable set <span class="math inline">\(S\)</span> called the <strong>state space</strong>. <span class="math inline">\(S\)</span> will always be either
finite, or countable, and a generic element of <span class="math inline">\(S\)</span> will be denoted by
<span class="math inline">\(i\)</span> or <span class="math inline">\(j\)</span>.</p>
<p>A stochastic process <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> taking values in a countable state space
<span class="math inline">\(S\)</span> is called a <strong>Markov chain</strong> if
<span class="math display">\[\begin{equation}
 {\mathbb{P}}[ X_{n+1}=j|X_n=i, X_{n-1}=i_{n-1},\dots, X_1=i_1, X_0=i_0]=
 {\mathbb{P}}[
X_{n+1}=j|X_n=i],
(\#eq:markov)
\end{equation}\]</span>
for all times <span class="math inline">\(n\in{\mathbb{N}}_0\)</span>, all states
<span class="math inline">\(i,j,i_0, i_1, \dots, i_{n-1} \in S\)</span>, whenever the two conditional
probabilities are well-defined, i.e., when
<span class="math display">\[\begin{equation}
{\mathbb{P}}[ X_n=i, \dots, X_1=i_1, X_0=i_0]&gt;0.
(\#eq:markov-well-defined)
\end{equation}\]</span></p>
<p>The Markov property is typically checked in the following way: one
computes the left-hand side of @ref(eq:markov)
and shows that its value does not
depend on <span class="math inline">\(i_{n-1},i_{n-2}, \dots, i_1, i_0\)</span> (why is that enough?). The
condition @ref(eq:markov-well-defined)
will be assumed (without explicit mention) every time we write a conditional
expression like to one in @ref(eq:markov).</p>
<p>All chains in this course will be
<strong>homogeneous</strong>, i.e., the conditional
probabilities <span class="math inline">\({\mathbb{P}}[X_{n+1}=j|X_{n}=i]\)</span> will not depend on the current
time <span class="math inline">\(n\in{\mathbb{N}}_0\)</span>, i.e., <span class="math inline">\({\mathbb{P}}[X_{n+1}=j|X_{n}=i]={\mathbb{P}}[X_{m+1}=j|X_{m}=i]\)</span>,
for <span class="math inline">\(m,n\in{\mathbb{N}}_0\)</span>.</p>
<p>Markov chains are (relatively) easy to work with because the Markov
property allows us to compute all the probabilities, expectations,
etc. we might be interested in by using only two ingredients.</p>
<ol style="list-style-type: decimal">
<li><p>The <strong>initial distribution</strong>: <span class="math inline">\({a}^{(0)}= \{ {a}^{(0)}_i\, : \, i\in S\}\)</span>,
<span class="math inline">\({a}^{(0)}_i={\mathbb{P}}[X_0=i]\)</span> - the initial probability distribution of the
process, and</p></li>
<li><p><strong>Transition probabilities</strong>: <span class="math inline">\(p_{ij}={\mathbb{P}}[X_{n+1}=j|X_n=i]\)</span> - the
mechanism that the process uses to jump around.</p></li>
</ol>
<p>Indeed, if you know <span class="math inline">\({a}^{(0)}_i\)</span> and <span class="math inline">\(p_{ij}\)</span> for all <span class="math inline">\(i,j\in S\)</span> and want to compute
a joint distribution <span class="math inline">\({\mathbb{P}}[X_n=i_n, X_{n-1}=i_{n-1}, \dots, X_0=i_0]\)</span>, you can use the definition of conditional probability
and the Markov property several times (the <em>multiplication theorem</em> from
your elementary probability course) as follows:
<span class="math display">\[\begin{align}
    {\mathbb{P}}[X_n=i_n, \dots, X_0=i_0] 
       &amp;= {\mathbb{P}}[X_n=i_n| X_{n-1}=i_{n-1}, \dots,X_0=i_0] \cdot {\mathbb{P}}[X_{n-1}=i_{n-1},
     \dots,X_0=i_0] \\ &amp; 
       = {\mathbb{P}}[X_n=i_n| X_{n-1}=i_{n-1}] \cdot {\mathbb{P}}[X_{n-1}=i_{n-1}, \dots,X_0=i_0]\\
       &amp;= p_{i_{n-1} i_{n}} {\mathbb{P}}[X_{n-1}=i_{n-1}, \dots,X_0=i_0]
\end{align}\]</span>
If we repeat the same procedure <span class="math inline">\(n-2\)</span> more times (and flip the order of factors), we get
<span class="math display">\[\begin{align}
{\mathbb{P}}[X_n=i_n, \dots, X _0=i_0] &amp;= {a}^{(0)}_{i_0} \cdot p_{i_0 i_1} \cdot p_{i_1 i_2}\cdot  \ldots \cdot p_{i_{n-1} i_{n}}
\end{align}\]</span>
Think of it this way: the probability of the process taking the trajectory <span class="math inline">\((i_0, i_1, \dots, i_n)\)</span> is:</p>
<ol style="list-style-type: decimal">
<li>the probability of starting at <span class="math inline">\(i_0\)</span> (which is <span class="math inline">\({a}^{(0)}_{i_0}\)</span>),</li>
<li>multiplied by the probability of transitioning from <span class="math inline">\(i_0\)</span> to <span class="math inline">\(i_1\)</span> (which is <span class="math inline">\(p_{i_0 i_1}\)</span>),</li>
<li>multiplied by the probability of transitioning from <span class="math inline">\(i_1\)</span> to <span class="math inline">\(i_2\)</span> (which is <span class="math inline">\(p_{i_1 i_2}\)</span>),</li>
<li>etc.</li>
</ol>
<p>When <span class="math inline">\(S\)</span> is finite, there is no loss of generality in
assuming that <span class="math inline">\(S=\{1,2,\dots, n\}\)</span>, and then we usually organize the
entries of <span class="math inline">\({a}^{(0)}\)</span> into a <em>row vector</em> <span class="math display">\[{a}^{(0)}=({a}^{(0)}_1,{a}^{(0)}_2,\dots, {a}^{(0)}_n),\]</span>
and the transition probabilities <span class="math inline">\(p_{ij}\)</span> into a <em>square matrix</em> <span class="math inline">\({\mathbf P}\)</span>,
where <span class="math display">\[{\mathbf P}=\begin{bmatrix}
  p_{11} &amp; p_{12} &amp; \dots &amp; p_{1n} \\
  p_{21} &amp; p_{22} &amp; \dots &amp; p_{2n} \\
  \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\
  p_{n1} &amp; p_{n2} &amp; \dots &amp; p_{nn} \\
\end{bmatrix}\]</span> In the general case (<span class="math inline">\(S\)</span> possibly infinite), one can
still use the vector and matrix notation as before, but it becomes quite
clumsy. For example, if <span class="math inline">\(S={\mathbb{Z}}\)</span>, then <span class="math inline">\({\mathbf P}\)</span> is an
infinite matrix <span class="math display">\[{\mathbf P}=\begin{bmatrix}
  \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp;   \\
  \dots &amp; p_{-1\, -1} &amp; p_{-1\, 0} &amp; p_{-1\, 1} &amp; \dots \\
  \dots &amp; p_{0\, -1} &amp; p_{0\, 0} &amp; p_{0\, 1} &amp; \dots \\
  \dots &amp; p_{1\, -1} &amp; p_{1\, 0} &amp; p_{1\, 1} &amp; \dots \\
    &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots \\
\end{bmatrix}\]</span></p>
</div>
<div id="first-examples" class="section level2" number="5.2">
<h2 number="5.2"><span class="header-section-number">5.2</span> First Examples</h2>
<p>Here are some examples of Markov chains - you will see many more in problems and
later chapters. Markov chains with a small number of states are often depicted
as <em>weighted directed graphs</em>, whose nodes are the chain’s states, and the
weight of the directed edge between <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is <span class="math inline">\(p_{ij}\)</span>. Such graphs are
called <em>transition graphs</em> and are an excellent way to visualize a number of
important properties of the chain. A transition graph is included for most of
the examples below. Edges are color-coded according to the probability assigned
to them. Black is always <span class="math inline">\(1\)</span>, while other colors are uniquely assigned to
different probabilities (edges carrying the same probability get the same
color).</p>
<div id="random-walks" class="section level3" number="5.2.1">
<h3 number="5.2.1"><span class="header-section-number">5.2.1</span> Random walks</h3>
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple (possibly biased) random walk. Let us show
that it indeed has the Markov property @ref(eq:markov).
Remember, first, that
<span class="math display">\[X_n=\sum_{k=1}^n \delta_k \text{ where }\delta_k \text{ are independent
(possibly biased) coin-tosses.}\]</span> For a choice of
<span class="math inline">\(i_0, \dots, i_n, j=i_{n+1}\)</span> (such that <span class="math inline">\(i_0=0\)</span> and
<span class="math inline">\(i_{k+1}-i_{k}=\pm 1\)</span>) we have
<span class="math display">\[%\label{equ:}
    \nonumber 
   \begin{split}
  {\mathbb{P}}[ X_{n+1}=i_{n+1}&amp;|X_n=i_n, X_{n-1}=i_{n-1},\dots, X_1=i_1, X_0=i_0]\\ = &amp;
  {\mathbb{P}}[ X_{n+1}-X_{n}=i_{n+1}-i_{n}|X_n=i_n, X_{n-1}=i_{n-1},\dots, X_1=i_1, X_0=i_0] \\ = &amp;
  {\mathbb{P}}[ \delta_{n+1}=i_{n+1}-i_{n}|X_n=i_n, X_{n-1}=i_{n-1},\dots, X_1=i_1, X_0=i_0] \\= &amp;
  {\mathbb{P}}[ \delta_{n+1}=i_{n+1}-i_n],
   \end{split}\]</span></p>
<p>where the last equality follows from the fact that the
increment <span class="math inline">\(\delta_{n+1}\)</span> is independent of the previous increments, and,
therefore, also of the values of <span class="math inline">\(X_1,X_2, \dots, X_n\)</span>. The last line
above does not depend on <span class="math inline">\(i_{n-1}, \dots, i_1, i_0\)</span>, so <span class="math inline">\(X\)</span> indeed has
the Markov property.</p>
<p>The state space <span class="math inline">\(S\)</span> of <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> is the set <span class="math inline">\({\mathbb{Z}}\)</span> of all integers, and
the initial distribution <span class="math inline">\({a}^{(0)}\)</span> is very simple: we start at <span class="math inline">\(0\)</span> with
probability <span class="math inline">\(1\)</span> (so that <span class="math inline">\({a}^{(0)}_0=1\)</span> and <span class="math inline">\({a}^{(0)}_i=0\)</span>, for <span class="math inline">\(i\not= 0\)</span>.). The
transition probabilities are simple to write down
<span class="math display">\[p_{ij}= \begin{cases} p, &amp; j=i+1 \\ q, &amp; j=i-1 \\ 0, &amp; \text{otherwise.}
\end{cases}\]</span> If you insist, these can be written down in an infinite matrix,
<span class="math display">\[{\mathbf P}=\begin{bmatrix}
  \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;  \\
  \dots  &amp; 0 &amp; p  &amp; 0 &amp; 0 &amp; 0 &amp; \dots \\
  \dots  &amp; q &amp; 0  &amp; p &amp; 0 &amp; 0 &amp; \dots \\
  \dots  &amp;0  &amp;q   &amp; 0  &amp; p  &amp; 0  &amp; \dots \\
  \dots  &amp;0  &amp;0  &amp; q&amp; 0 &amp; p&amp; \dots \\
  \dots  &amp;0  &amp; 0 &amp;0 &amp; q&amp; 0&amp; \dots \\
  \dots  &amp;0  &amp; 0 &amp;0 &amp; 0&amp; q&amp; \dots \\
   &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots \\
\end{bmatrix}\]</span> but this representation is typically not as useful as in the finite case.</p>
<p>Here is a (portion of) a transition graph for a simple random walk. Instead of writing probabilities
on top of the edges, we color code them as follows: green is <span class="math inline">\(p\)</span> and orange is <span class="math inline">\(1-p\)</span>.</p>
<center>
<img src="_main_files/figure-html/unnamed-chunk-193-1.png" width="672" style="margin-top:-20%; margin-bottom: -20%" style="display: block; margin: auto;" />
</center>
</div>
<div id="gambler" class="section level3" number="5.2.2">
<h3 number="5.2.2"><span class="header-section-number">5.2.2</span> Gambler’s ruin</h3>
<p>In Gambler’s ruin, a gambler starts with <span class="math inline">\(\$x\)</span>, where
<span class="math inline">\(0\leq x \leq a\in{\mathbb{N}}\)</span> and in each play wins a dollar (with probability
<span class="math inline">\(p\in (0,1)\)</span>) and loses a dollar (with probability <span class="math inline">\(q=1-p\)</span>). When the
gambler reaches either <span class="math inline">\(0\)</span> or <span class="math inline">\(a\)</span>, the game stops. For mathematical
convenience, it is usually a good idea to keep the chain defined, even
after the modeled phenomenon stops. This is usually accomplished by
simply assuming that the process “stays alive” but remains “frozen in place”
instead of disappearing. In our case, once the gambler reaches either of
the states <span class="math inline">\(0\)</span> and <span class="math inline">\(a\)</span>, he/she simply stays there forever.</p>
<p>Therefore, the transition probabilities are similar to those of a random
walk, but differ from them at the boundaries <span class="math inline">\(0\)</span> and <span class="math inline">\(a\)</span>. The state
space is finite <span class="math inline">\(S=\{0,1,\dots, a\}\)</span> and the matrix <span class="math inline">\({\mathbf P}\)</span> is given by
<span class="math display">\[{\mathbf P}=\begin{bmatrix}
  1 &amp; 0 &amp; 0 &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; 0 \\
  q &amp; 0 &amp; p &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; q &amp; 0 &amp; p &amp; \dots &amp; 0 &amp; 0 &amp; 0 \\
  0 &amp; 0 &amp; q &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; 0 \\
  \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots \\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; \dots &amp; 0 &amp; p &amp; 0 \\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; \dots &amp; q &amp; 0 &amp; p \\
  0 &amp; 0 &amp; 0 &amp; 0 &amp; \dots &amp; 0 &amp; 0 &amp; 1 \\
\end{bmatrix}\]</span></p>
<p>In the picture below, green denotes the probability <span class="math inline">\(p\)</span> and orange probability <span class="math inline">\(1-p\)</span>. As always, black is <span class="math inline">\(1\)</span>.</p>
<center>
<img src="_main_files/figure-html/unnamed-chunk-194-1.png" width="720" style="margin-top:-20%; margin-bottom: -20%" style="display: block; margin: auto;" />
</center>
<p>The initial distribution is deterministic: <span class="math display">\[{a}^{(0)}_i=
\begin{cases}
  1,&amp; i=x,\\ 0,&amp; i\not= x.
\end{cases}\]</span></p>
</div>
<div id="regime-switching" class="section level3" number="5.2.3">
<h3 number="5.2.3"><span class="header-section-number">5.2.3</span> Regime Switching</h3>
<p>Consider a system with two different states; think about a simple
weather forecast (rain/no rain), high/low water level in a reservoir,
high/low volatility regime in a financial market, high/low level of
economic growth, the political party in power, etc. Suppose that the
states are called <span class="math inline">\(1\)</span> and <span class="math inline">\(2\)</span> and the probabilities <span class="math inline">\(p_{12}\)</span> and
<span class="math inline">\(p_{21}\)</span> of switching states are given. The probabilities
<span class="math inline">\(p_{11}=1-p_{12}\)</span> and <span class="math inline">\(p_{22}=1-p_{21}\)</span> correspond to the system staying
in the same state. The transition matrix for this Markov chain with
<span class="math inline">\(S=\{1,2\}\)</span> is <span class="math display">\[{\mathbf P}=
\begin{bmatrix}
  p_{11} &amp; p_{12} \\ p_{21} &amp; p_{22}.
\end{bmatrix}\]</span> When <span class="math inline">\(p_{12}\)</span> and <span class="math inline">\(p_{21}\)</span> are large (close to <span class="math inline">\(1\)</span>) the
system nervously jumps between the two states. When they are small,
there are long periods of stability (staying in the same state).</p>
<center>
<img src="_main_files/figure-html/unnamed-chunk-195-1.png" width="720" style="margin-top:-20%; margin-bottom: -20%" style="display: block; margin: auto;" />
</center>
<p>One of the assumptions behind regime-switching models is that the
transitions (switches) can only happen in regular intervals (once a
minute, once a day, once a year, etc.). This is a feature of all
<em>discrete-time</em> Markov chains. One would need to use a <em>continuous-time</em>
model to allow for the transitions between states at any point in time.</p>
</div>
<div id="deterministically-monotone-markov-chain" class="section level3" number="5.2.4">
<h3 number="5.2.4"><span class="header-section-number">5.2.4</span> Deterministically monotone Markov chain</h3>
<p>A stochastic process <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> with state space <span class="math inline">\(S={\mathbb{N}}_0\)</span> such that
<span class="math inline">\(X_n=n\)</span> for <span class="math inline">\(n\in{\mathbb{N}}_0\)</span> (no randomness here) is called Deterministically
monotone Markov chain (DMMC). The transition matrix looks like this
<span class="math display">\[{\mathbf P}=
\begin{bmatrix}
  0 &amp; 1 &amp; 0 &amp; 0 &amp; \dots \\
  0 &amp; 0 &amp; 1 &amp; 0 &amp; \dots \\
  0 &amp; 0 &amp; 0 &amp; 1 &amp; \dots \\
  \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots 
\end{bmatrix}\]</span></p>
<center>
<img src="_main_files/figure-html/unnamed-chunk-196-1.png" width="720" style="margin-top:-20%; margin-bottom: -20%" style="display: block; margin: auto;" />
</center>
<p>It is a pretty boring chain; its main use is as a counterexample.</p>
</div>
<div id="not-a-markov-chain" class="section level3" number="5.2.5">
<h3 number="5.2.5"><span class="header-section-number">5.2.5</span> Not a Markov chain</h3>
<p>Consider a frog jumping from a lily pad to a lily pad in a small forest
pond. Suppose that there are <span class="math inline">\(N\)</span> lily pads so that the state space can
be described as <span class="math inline">\(S=\{1,2,\dots, N\}\)</span>. The frog starts on lily pad 1 at
time <span class="math inline">\(n=0\)</span>, and jumps around in the following fashion: at time <span class="math inline">\(0\)</span> it
chooses any lily pad except for the one it is currently sitting on (with
equal probability) and then jumps to it. At time <span class="math inline">\(n&gt;0\)</span>, it chooses any
lily pad other than the one it is sitting on <em>and the one it visited
immediately before</em> (with equal probability) and jumps to it. The
position <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> of the frog is not a Markov chain. Indeed, we have
<span class="math display">\[{\mathbb{P}}[X_3=1|X_2=2, X_1=3]= \frac{1}{N-2},\]</span> while
<span class="math display">\[{\mathbb{P}}[X_3=1|X_2=2, X_1=1]=0.\]</span></p>
<p>A more dramatic version of this example would be the one where the frog
remembers all the lily pads it had visited before, and only chooses
among the remaining ones for the next jump.</p>
</div>
<div id="turning-a-non-markov-chain-into-a-markov-chain" class="section level3" number="5.2.6">
<h3 number="5.2.6"><span class="header-section-number">5.2.6</span> Turning a non-Markov chain into a Markov chain</h3>
<p>How can we turn the process the previous example into a Markov chain.
Obviously, the problem is that the frog has to remember the number of
the lily pad it came from in order to decide where to jump next. The way
out is to make this information a part of the state. In other words, we
need to change the state space. Instead of just <span class="math inline">\(S=\{1,2,\dots, N\}\)</span>,
we set <span class="math inline">\(S= \{ (i_1,  i_2)\, : \, i_1,i_2 \in\{1,2,\dots N\}\}\)</span>. In words, the state of the
process will now contain not only the number of the current lily pad
(i.e., <span class="math inline">\(i_2\)</span>) but also the number of the lily pad we came from (i.e.,
<span class="math inline">\(i_1\)</span>). This way, the frog will be in the state <span class="math inline">\((i_1,i_2)\)</span> if it is
currently on the lily pad number <span class="math inline">\(i_2\)</span>, and it arrived here from <span class="math inline">\(i_1\)</span>.
There is a bit of freedom with the initial state, but we simply assume
that we start from <span class="math inline">\((1,1)\)</span>. Starting from the state <span class="math inline">\((i_1,i_2)\)</span>, the
frog can jump to any state of the form <span class="math inline">\((i_2, i_3)\)</span>, <span class="math inline">\(i_3\not= i_1,i_2\)</span>
(with equal probabilities). Note that some states will never be visited
(like <span class="math inline">\((i,i)\)</span> for <span class="math inline">\(i\not = 1\)</span>), so we could have reduced the state space
a little bit right from the start.</p>
<p>It is important to stress that the passage to the new state space defines
a whole new stochastic process. It is therefore, not quite accurate, as the title suggests,
to say that we
“turned” a non-Markov process into a Markov process. Rather, we replaced a non-Markovian
<em>model</em> of a given situation by a different, Markovian, one.</p>
</div>
<div id="deterministic-functions-of-markov-chains-do-not-need-to-be-markov-chains" class="section level3" number="5.2.7">
<h3 number="5.2.7"><span class="header-section-number">5.2.7</span> Deterministic functions of Markov chains do not need to be Markov chains</h3>
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a Markov chain on the state space <span class="math inline">\(S\)</span>, and let
<span class="math inline">\(f:S\to T\)</span> be a function. The stochastic process <span class="math inline">\(Y_n= f(X_n)\)</span> takes
values in <span class="math inline">\(T\)</span>; is it necessarily a Markov chain?</p>
<p>We will see in this
example that the answer is <em>no</em>. Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>
be a simple symmetric random walk, with the usual state space <span class="math inline">\(S = {\mathbb{Z}}\)</span>. With
<span class="math inline">\(r(m) = m\  (\text{mod } 3)\)</span>
denoting the remainder after the division by <span class="math inline">\(3\)</span>, we first define the process <span class="math inline">\(R_n = r(X_n)\)</span> so that
<span class="math display">\[R_n=\begin{cases} 
0, &amp; \text{ if $X_n$ is divisible by 3,}\\
1, &amp; \text{ if $X_n-1$ is divisible by 3,}\\
2, &amp; \text{ if $X_n-2$ is divisible by 3.}
\end{cases}\]</span>
Using <span class="math inline">\(R_n\)</span> we define <span class="math inline">\(Y_n = (X_n-R_n)/3\)</span> to be the corresponding quotient, so that <span class="math inline">\(Y_n\in{\mathbb{Z}}\)</span> and
<span class="math display">\[3 Y_n \leq X_n &lt;3 (Y_n+1).\]</span>
The process <span class="math inline">\(Y\)</span> is of the form <span class="math inline">\(Y_n = f(X_n)\)</span>, where <span class="math inline">\(f(i)= \lfloor i/3 \rfloor\)</span>, and <span class="math inline">\(\lfloor x \rfloor\)</span> is the largest integer not exceeding
<span class="math inline">\(x\)</span>.</p>
<p>To show that <span class="math inline">\(Y\)</span> is not a Markov chain, let us consider the the event
<span class="math inline">\(A=\{Y_2=0, Y_1=0\}\)</span>. The only way for this to happen is if <span class="math inline">\(X_1=1\)</span>
and <span class="math inline">\(X_2=2\)</span> or <span class="math inline">\(X_1=1\)</span> and <span class="math inline">\(X_2=0\)</span>, so that <span class="math inline">\(A=\{X_1=1\}\)</span>. Also
<span class="math inline">\(Y_3=1\)</span> if and only if <span class="math inline">\(X_3=3\)</span>. Therefore
<span class="math display">\[{\mathbb{P}}[ Y_3=1|Y_2=0, Y_1=0]={\mathbb{P}}[ X_3=3| X_1=1]= 1/4.\]</span> On the other hand,
<span class="math inline">\(Y_2=0\)</span> if and only if <span class="math inline">\(X_2=0\)</span> or <span class="math inline">\(X_2=2\)</span>, so <span class="math inline">\({\mathbb{P}}[Y_2=0]= 3/4\)</span>.
Finally, <span class="math inline">\(Y_3=1\)</span> and <span class="math inline">\(Y_2=0\)</span> if and only if <span class="math inline">\(X_3=3\)</span> and so
<span class="math inline">\({\mathbb{P}}[Y_3=1, Y_2=0]= 1/8\)</span>. Hence,
<span class="math display">\[{\mathbb{P}}[ Y_3=1|Y_2=0]={\mathbb{P}}[Y_3=1, Y_2=0]/{\mathbb{P}}[Y_2=0]= \frac{1/8}{3/4}=
\frac{1}{6}.\]</span> Therefore, <span class="math inline">\(Y\)</span> is not a Markov chain. If you want a more dramatic
example, try to modify this
example so that one of the probabilities above is positive, but the
other is zero.</p>
<p>The important property of the function <span class="math inline">\(f\)</span> we applied to <span class="math inline">\(X\)</span> is that it is <em>not one-to-one</em>. In other words,
<span class="math inline">\(f\)</span> collapses several states of <span class="math inline">\(X\)</span> into a single state of <span class="math inline">\(Y\)</span>. This way, the “present” may end up containing
so little information that the past suddenly becomes relevant for the dynamics of the future evolution.</p>
</div>
<div id="a-game-of-tennis" class="section level3" number="5.2.8">
<h3 number="5.2.8"><span class="header-section-number">5.2.8</span> A game of tennis</h3>
<p>In a game of tennis, the scoring system is as follows: both players start with the score of <span class="math inline">\(0\)</span>. Each time
player 1 wins a point (a.k.a. <em>a rally</em>), her score moves a step up in the
following hierarchy <span class="math display">\[0 \mapsto 15 \mapsto 30 \mapsto 40.\]</span> Once she
reaches <span class="math inline">\(40\)</span> and scores a point, three things can happen:</p>
<ol style="list-style-type: decimal">
<li><p>if the score of player 2 is <span class="math inline">\(30\)</span> or less, player 1 wins the game.</p></li>
<li><p>if the score of player 2 <span class="math inline">\(40\)</span>, the score of player 1 moves up to “advantage”,
and</p></li>
<li><p>if the score of player 2 is “advantage”, nothing happens to the score of player 1
but the score of player 2 falls back to <span class="math inline">\(40\)</span>.</p></li>
</ol>
<p>Finally, if the score of player 1 is “advantage” and she wins a point, she
wins the game. The situation is entirely symmetric for player 2. We suppose
that the probability that player 1 wins each point is <span class="math inline">\(p\in (0,1)\)</span>,
independently of the current score. A situation like this is a typical
example of a Markov chain in an applied setting. What are the states of
the process? We obviously need to know both players’ scores and we also
need to know if one of the players has won the game. Therefore, a
possible state space is the following:</p>
<p><span class="math display">\[\begin{align}
      S=
      \Big\{ &amp;(0,0), (0,15), (0,30), (0,40), (15,0), (15,15), (15,30), (15,40), (30,0),
      (30,15),\\ &amp;  (30,30),  (30,40), (40,0), (40,15), (40,30), (40,40), (40,A), (A,40), W_1, W_2 \Big\}
\end{align}\]</span></p>
<p>where <span class="math inline">\(A\)</span> stands for <em>“advantage”</em> and <span class="math inline">\(W_1\)</span>
(resp., <span class="math inline">\(W_2\)</span>) denotes the state where player 1 (resp., player 2) wins. It is not hard
to assign probabilities to transitions between states. Once we reach
either <span class="math inline">\(W_1\)</span> or <span class="math inline">\(W_2\)</span> the game stops. We can assume that the
chain remains in that state forever, i.e., the state is absorbing.</p>
<center>
<img src="_main_files/figure-html/unnamed-chunk-197-1.png" width="576" style="margin-top:-10%; margin-bottom: -10%" style="display: block; margin: auto;" />
</center>
<p>The
initial distribution is quite simple - we always start from the same
state <span class="math inline">\((0,0)\)</span>, so that <span class="math inline">\({a}^{(0)}_{(0,0)}=1\)</span> and
<span class="math inline">\({a}^{(0)}_i=0\)</span> for all <span class="math inline">\(i\in S\setminus\{(0,0)\}\)</span>.</p>
<p>How about the transition matrix? When the number of states is big
(<span class="math inline">\(\# S=20\)</span> in this case), transition matrices are useful in computer
memory, but not so much on paper. Just for the fun of it, here is the
transition matrix for our game-of-tennis chain (I am going to leave it up to you
to figure out how rows/columns of the matrix match to states) <span class="math display">\[\tiny
{\mathbf P}=
\begin{bmatrix}
 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 &amp; 0 \\
 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 \\
 p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
 p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 &amp; 0 \\
 p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; p \\
 0 &amp; q &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; p &amp; 0 &amp; 0 \\
 p &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; q &amp; 0 &amp; 0 \\
\end{bmatrix} \]</span></p>
<p>Does the structure of a game of tennis make is easier or harder for the better
player to win? In other words, if you had to play against the best tennis player
in the world (I am rudely assuming that he or she is better than you), would you
have a better chance of winning if you only played a point (rally), or if you
played the whole game? We will give a precise answer to this question in a little while. In the
meantime, try to guess.</p>
</div>
</div>
<div id="chapman-kolmogorov-equations" class="section level2" number="5.3">
<h2 number="5.3"><span class="header-section-number">5.3</span> Chapman-Kolmogorov equations</h2>
<p>The transition probabilities <span class="math inline">\(p_{ij}\)</span>, <span class="math inline">\(i,j\in S\)</span> tell us how a Markov
chain jumps from one state to another in a single step. Think of it as a description of
the <em>local</em> behavior of the chain. This is the information one can usually obtain from observations and modeling assumptions. On the other hand, it is the <em>global</em> (long-time) behavior of the model that
provides the most interesting insights. In that spirit, we turn our attention
to probabilities like this:
<span class="math display">\[{\mathbb{P}}[X_{k+n}=j|X_k=i] \text{ for } n = 1,2,\dots.\]</span>
Since we are assuming that all of our
chains are homogeneous (transition probabilities do not change with
time), this probability does not depend on the time <span class="math inline">\(k\)</span>, so we can define the <strong>multi-step transition probabilities</strong> <span class="math inline">\(p^{(n)}_{ij}\)</span> as follows:
<span class="math display">\[p^{(n)}_{ij}={\mathbb{P}}[X_{k+n}=j|X_{k}=i]={\mathbb{P}}[ X_{n}=j|X_0=i].\]</span> We allow <span class="math inline">\(n=0\)</span> under
the useful convention that
<span class="math display">\[p^{(0)}_{ij}=\begin{cases} 1, &amp; i=j,\\ 0,&amp;
  i\not = j.
\end{cases}\]</span>
We note right away that the numbers <span class="math inline">\(p^{(n)}_{ij}\)</span>, <span class="math inline">\(i,j\in S\)</span> naturally fit into an <span class="math inline">\(N\times N\)</span>-matrix which we denote by <span class="math inline">\({\mathbf P}^{(n)}\)</span>. We note right away that
<span class="math display">\[\begin{equation}
  {\mathbf P}^{(0)}= \operatorname{Id}\text{ and } {\mathbf P}^{(1)}= {\mathbf P},
(\#eq:Przo)
\end{equation}\]</span>
where <span class="math inline">\(\operatorname{Id}\)</span> denotes the <span class="math inline">\(N\times N\)</span> identity matrix.</p>
<p>The central result of this section is the following sequence of equalities connecting <span class="math inline">\({\mathbf P}^{(n)}\)</span> for different values of <span class="math inline">\(n\)</span>, know as the <strong>Chapman-Kolmogorov equations</strong>:
<span class="math display">\[\begin{equation}
  {\mathbf P}^{(m+n)} = {\mathbf P}^{(m)} {\mathbf P}^{(n)}, \text{ for all } m,n \in {\mathbb{N}}_0.
(\#eq:CK)
\end{equation}\]</span>
To see why this is true we start by computing
<span class="math inline">\({\mathbb{P}}[ X_{n+m} = j, X_0=i]\)</span>. Since each trajectory from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> in <span class="math inline">\(n+m\)</span> steps has be somewhere at time <span class="math inline">\(n\)</span>, we can write
<span class="math display">\[\begin{equation}
  {\mathbb{P}}[ X_{n+m}= j, X_0 = i] = \sum_{k\in S} {\mathbb{P}}[X_{n+m} = j, X_{n} = k, X_0 = i].
(\#eq:one-CK)
\end{equation}\]</span>
By the multiplication rule, we have
<span class="math display">\[\begin{multline}
{\mathbb{P}}[X_{n+m} = j, X_{n} = k, X_0 = i] = {\mathbb{P}}[ X_{n+m} = j | X_{n}=k, X_{0}=i] {\mathbb{P}}[X_{n}=k, X_0 = i],
(\#eq:two-CK)
\end{multline}\]</span>
and then, by the Markov property:
<span class="math display">\[\begin{equation}
  {\mathbb{P}}[ X_{n+m} = j | X_{n}=k, X_{0}=i] = {\mathbb{P}}[ X_{n+m} = j | X_n = k].
(\#eq:three-CK)
\end{equation}\]</span>
Combining @ref(eq:one-CK), @ref(eq:two-CK) and @ref(eq:three-CK) we obtain the following
equality:
<span class="math display">\[\begin{equation}
  {\mathbb{P}}[ X_{n+m}= j, X_0 = i] = \sum_{k\in S} {\mathbb{P}}[ X_{n+m} = j | X_n = k] {\mathbb{P}}[X_{n}=k, X_0 = i].
\end{equation}\]</span>
which is nothing but @ref(eq:CK); to see that, just remember how matrices are multiplied.</p>
<p>The punchline is that @ref(eq:CK), together with @ref(eq:Przo) imply that
<span class="math display">\[\begin{equation}
  {\mathbf P}^{(n)}= {\mathbf P}^n,
(\#eq:Prn-Pn)
\end{equation}\]</span>
where the left-hand side is the matrix composed of the <span class="math inline">\(n\)</span>-step transition
probabilities, and the right hand side is the <span class="math inline">\(n\)</span>-th (matrix) power of the
(<span class="math inline">\(1\)</span>-step) transition matrix <span class="math inline">\({\mathbf P}\)</span>. Using @ref(eq:Prn-Pn) allows us to
write a simple expression for the
distribution of the random variable <span class="math inline">\(X_n\)</span>, for <span class="math inline">\(n\in{\mathbb{N}}_0\)</span>. Remember that
the initial distribution (the distribution of <span class="math inline">\(X_0\)</span>) is denoted by
<span class="math inline">\({a}^{(0)}=({a}^{(0)}_i)_{i\in S}\)</span>. Analogously, we define the vector
<span class="math inline">\({a}^{(n)}=({a}^{(n)}_i)_{i\in S}\)</span> by <span class="math display">\[{a}^{(n)}_i={\mathbb{P}}[X_n=i],\ i\in S.\]</span> Using
the law of total probability, we have
<span class="math display">\[{a}^{(n)}_i={\mathbb{P}}[X_n=i]=\sum_{k\in S} {\mathbb{P}}[ X_0=k] {\mathbb{P}}[ X_n=i|X_0=k]=
\sum_{k\in S} {a}^{(0)}_k p^{(n)}_{ki}.\]</span> We usually interpret <span class="math inline">\({a}^{(0)}\)</span> as a (row)
vector, so the above relationship can be expressed using vector-matrix
multiplication <span class="math display">\[{a}^{(n)}={a}^{(0)}{\mathbf P}^n.\]</span></p>
<div class="problem">
<p>Find an explicit expression for <span class="math inline">\({\mathbf P}^{(n)}\)</span> in the case of the regime-switching
chain introduced above. Feel free to assume that <span class="math inline">\(p_{ij}&gt;0\)</span> for all <span class="math inline">\(i,j\)</span>.</p>
</div>
<div class="solution">
<p>It is often difficult to compute <span class="math inline">\({\mathbf P}^n\)</span> for a general transition
matrix <span class="math inline">\({\mathbf P}\)</span> and a large <span class="math inline">\(n\)</span>. We will see later that it will be easier
to find the limiting values <span class="math inline">\(\lim_{n\to\infty}p^{(n)}_{ij}\)</span>. The regime-switching
chain is one of the few examples where everything can be done by hand.</p>
<p>By @ref(eq:Prn-Pn), we need to compute the <span class="math inline">\(n\)</span>-th matrix power of the transition
matrix <span class="math inline">\({\mathbf P}\)</span>. To make the notation a bit nicer, let us write <span class="math inline">\(a\)</span> for <span class="math inline">\(p_{12}\)</span> and
<span class="math inline">\(b\)</span> for <span class="math inline">\(p_{21}\)</span>, so that we can write
<span class="math display">\[{\mathbf P}=
\begin{bmatrix}
  1-a &amp; a \\ b &amp; 1-b
\end{bmatrix}\]</span></p>
<p>The winning idea is to use diagonalization, and for that we start by writing down the
characteristic equation <span class="math inline">\(\det (\lambda I-{\mathbf P})=0\)</span> of the
matrix <span class="math inline">\({\mathbf P}\)</span>:
<span class="math display">\[\label{equ:}
    \nonumber 
   \begin{split}
 0&amp;=\det(\lambda I-{\mathbf P})=
\begin{vmatrix}
\lambda-1+a &amp; -a \\ -b &amp; \lambda-1+b
\end{vmatrix}\\ &amp;
=((\lambda-1)+a)((\lambda-1)+b)-ab
=(\lambda-1)(\lambda-(1-a-b)). 
   \end{split}\]</span> The eigenvalues are, therefore, <span class="math inline">\(\lambda_1=1\)</span> and
<span class="math inline">\(\lambda_2=1-a-b\)</span>, and the corresponding eigenvectors are <span class="math inline">\(v_1=\binom{1}{1}\)</span> and
<span class="math inline">\(v_2=\binom{a}{-b}\)</span>. Therefore,
if we define <span class="math display">\[V=
\begin{bmatrix}
1 &amp; a \\ 1 &amp; -b
\end{bmatrix} 
\text{ and }D=
\begin{bmatrix}
  \lambda_1 &amp; 0 \\ 0 &amp; \lambda_2
\end{bmatrix}=
\begin{bmatrix}
  1 &amp; 0 \\ 0 &amp; (1-a-b)
\end{bmatrix}\]</span> we have <span class="math display">\[{\mathbf P}V =
V D,\text{ i.e., } {\mathbf P}= V D V^{-1}.\]</span> This representation is very useful
for taking (matrix) powers: <span class="math display">\[\label{equ:60C4}
 \begin{split}
    {\mathbf P}^n &amp;= (V D V^{-1})( V D V^{-1}) \dots (V D V^{-1})= V D^n V^{-1}
  \\ &amp; =
   V
   \begin{bmatrix}
     1  &amp; 0 \\ 0 &amp; (1-a-b)^n
   \end{bmatrix} V^{-1}
 \end{split}\]</span> We assumed that all <span class="math inline">\(p_{ij}\)</span> are positive which means, in particular, that <span class="math inline">\(a+b&gt;0\)</span> and
<span class="math display">\[V^{-1} = \tfrac{1}{a+b}
\begin{bmatrix}
  b &amp; a \\ 1 &amp; -1
\end{bmatrix},\]</span> and so
<span class="math display">\[\begin{align}
 {\mathbf P}^n &amp;= V D^n V^{-1}= 
\begin{bmatrix}
1 &amp; a \\ 1 &amp; -b
\end{bmatrix}
\ 
\begin{bmatrix}
1 &amp; 0 \\ 0 &amp; (1-a-b)^n
\end{bmatrix}
\ 
\tfrac{1}{a+b}
\begin{bmatrix}
  b &amp; a \\ 1 &amp; -1
\end{bmatrix}\\
&amp;=
 \frac{1}{a+b} 
  \begin{bmatrix}
   b &amp; a \\ b &amp; a 
  \end{bmatrix}
+
 \frac{(1-a-b)^n}{a+b} 
  \begin{bmatrix}
    a &amp; -a \\ b &amp; -b
  \end{bmatrix}\\
&amp;=
\begin{bmatrix}
  \frac{b}{a+b}+(1-a-b)^n \frac{a}{a+b} &amp;   \frac{a}{a+b}-(1-a-b)^n \frac{a}{a+b}\\
  \frac{b}{a+b}+(1-a-b)^n \frac{b}{a+b} &amp;   \frac{a}{a+b}-(1-a-b)^n \frac{b}{a+b}
\end{bmatrix}
\end{align}\]</span></p>
<p>The expression for <span class="math inline">\({\mathbf P}^n\)</span> above tells us a lot about the structure of
the multi-step probabilities <span class="math inline">\(p^{(n)}_{ij}\)</span> for large <span class="math inline">\(n\)</span>. Note that the
second matrix on the right-hand side above comes multiplied by
<span class="math inline">\((1-a-b)^n\)</span> which tends to <span class="math inline">\(0\)</span> as <span class="math inline">\(n\to\infty\)</span> (under our assumptions that <span class="math inline">\(p_{ij}&gt;0\)</span>.)
We can, therefore, write
<span class="math display">\[{\mathbf P}^n\sim \frac{1}{a+b}
\begin{bmatrix}
  b &amp; a \\ b &amp; a
\end{bmatrix}
\text{ for large } n.\]</span> The fact that the rows of the right-hand side
above are equal points to the fact that, for large <span class="math inline">\(n\)</span>, <span class="math inline">\(p^{(n)}_{ij}\)</span> does
not depend (much) on the initial state <span class="math inline">\(i\)</span>. In other words, this Markov
chain forgets its initial condition after a long period of time. This is
a rule more than an exception, and we will study such phenomena in the
following lectures.</p>
</div>
</div>
<div id="mc-sim" class="section level2" number="5.4">
<h2 number="5.4"><span class="header-section-number">5.4</span> How to simulate Markov chains</h2>
<p>One of the (many) reasons Markov chains are a popular modeling tool is the ease with which they can be simulated. When we simulated a random walk, we started at <span class="math inline">\(0\)</span> and built the process by adding independent coin-toss-distributed increments. We obtained the value of the next position of the walk by <em>adding</em> the present position and the value of an independent random variable. For general Markov chain, this procedure works almost verbatim, except that the function that combines the present position
and a value of an independent random variable may be something other than addition.
In general, we collapse the two parts of the process - a simulation of an independent random variable and its combination with the present position - into one. Given our position, we pick the row of the transition matrix that corresponds to it and then use its elements as the probabilities that govern our position tomorrow. It will all be clear once you read through the solution of the following problem.</p>
<div class="problem">
<p>Simulate <span class="math inline">\(1000\)</span> trajectories of a gambler’s ruin Markov chain with <span class="math inline">\(a=3\)</span>, <span class="math inline">\(p=2/3\)</span> and <span class="math inline">\(x=1\)</span> (see subsection @ref(gambler) above for the meaning of these constants). Use the Monte Carlo method to estimate the probability that the gambler will leave the casino with <span class="math inline">\(\$3\)</span> in her pocket in at most <span class="math inline">\(T=100\)</span> time periods.</p>
</div>
<div class="solution">
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true"></a><span class="co"># state space</span></span>
<span id="cb136-2"><a href="#cb136-2" aria-hidden="true"></a>S =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb136-3"><a href="#cb136-3" aria-hidden="true"></a></span>
<span id="cb136-4"><a href="#cb136-4" aria-hidden="true"></a><span class="co"># transition matrix</span></span>
<span id="cb136-5"><a href="#cb136-5" aria-hidden="true"></a>P =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,   <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>,</span>
<span id="cb136-6"><a href="#cb136-6" aria-hidden="true"></a>             <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>,   <span class="dv">2</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>,</span>
<span id="cb136-7"><a href="#cb136-7" aria-hidden="true"></a>             <span class="dv">0</span>,   <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">0</span>,   <span class="dv">2</span><span class="op">/</span><span class="dv">3</span>,</span>
<span id="cb136-8"><a href="#cb136-8" aria-hidden="true"></a>             <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">0</span>,   <span class="dv">1</span>), </span>
<span id="cb136-9"><a href="#cb136-9" aria-hidden="true"></a>           <span class="dt">byrow=</span>T, <span class="dt">ncol=</span><span class="dv">4</span>)</span>
<span id="cb136-10"><a href="#cb136-10" aria-hidden="true"></a></span>
<span id="cb136-11"><a href="#cb136-11" aria-hidden="true"></a>T =<span class="st"> </span><span class="dv">100</span> <span class="co"># number of time periods</span></span>
<span id="cb136-12"><a href="#cb136-12" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">1000</span> <span class="co"># number of simulations</span></span>
<span id="cb136-13"><a href="#cb136-13" aria-hidden="true"></a></span>
<span id="cb136-14"><a href="#cb136-14" aria-hidden="true"></a><span class="co"># simulate the next position of the chain</span></span>
<span id="cb136-15"><a href="#cb136-15" aria-hidden="true"></a>draw_next =<span class="st"> </span><span class="cf">function</span>(s) {</span>
<span id="cb136-16"><a href="#cb136-16" aria-hidden="true"></a>  i =<span class="st"> </span><span class="kw">match</span>(s, S) <span class="co"># the row number of the state s</span></span>
<span id="cb136-17"><a href="#cb136-17" aria-hidden="true"></a>  <span class="kw">sample</span>(S, <span class="dt">prob =</span> P[i, ], <span class="dt">size =</span> <span class="dv">1</span>)</span>
<span id="cb136-18"><a href="#cb136-18" aria-hidden="true"></a>}</span>
<span id="cb136-19"><a href="#cb136-19" aria-hidden="true"></a></span>
<span id="cb136-20"><a href="#cb136-20" aria-hidden="true"></a><span class="co"># simulate a single trajectory of length T</span></span>
<span id="cb136-21"><a href="#cb136-21" aria-hidden="true"></a><span class="co"># from the initial state</span></span>
<span id="cb136-22"><a href="#cb136-22" aria-hidden="true"></a>single_trajectory =<span class="st"> </span><span class="cf">function</span>(initial_state) {</span>
<span id="cb136-23"><a href="#cb136-23" aria-hidden="true"></a>  path =<span class="st"> </span><span class="kw">numeric</span>(T)</span>
<span id="cb136-24"><a href="#cb136-24" aria-hidden="true"></a>  last =<span class="st"> </span>initial_state</span>
<span id="cb136-25"><a href="#cb136-25" aria-hidden="true"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>T) {</span>
<span id="cb136-26"><a href="#cb136-26" aria-hidden="true"></a>    path[n] =<span class="st"> </span><span class="kw">draw_next</span>(last)</span>
<span id="cb136-27"><a href="#cb136-27" aria-hidden="true"></a>    last =<span class="st"> </span>path[n]</span>
<span id="cb136-28"><a href="#cb136-28" aria-hidden="true"></a>  }</span>
<span id="cb136-29"><a href="#cb136-29" aria-hidden="true"></a>  <span class="kw">return</span>(path)</span>
<span id="cb136-30"><a href="#cb136-30" aria-hidden="true"></a>}</span>
<span id="cb136-31"><a href="#cb136-31" aria-hidden="true"></a></span>
<span id="cb136-32"><a href="#cb136-32" aria-hidden="true"></a><span class="co"># simulate the entire chain</span></span>
<span id="cb136-33"><a href="#cb136-33" aria-hidden="true"></a>simulate_chain =<span class="st"> </span><span class="cf">function</span>(initial_state) {</span>
<span id="cb136-34"><a href="#cb136-34" aria-hidden="true"></a>  <span class="kw">data.frame</span>(<span class="dt">X0 =</span> initial_state,</span>
<span id="cb136-35"><a href="#cb136-35" aria-hidden="true"></a>             <span class="kw">t</span>(<span class="kw">replicate</span>(</span>
<span id="cb136-36"><a href="#cb136-36" aria-hidden="true"></a>               nsim, <span class="kw">single_trajectory</span>(initial_state)</span>
<span id="cb136-37"><a href="#cb136-37" aria-hidden="true"></a>             )))</span>
<span id="cb136-38"><a href="#cb136-38" aria-hidden="true"></a>}</span>
<span id="cb136-39"><a href="#cb136-39" aria-hidden="true"></a></span>
<span id="cb136-40"><a href="#cb136-40" aria-hidden="true"></a>df =<span class="st"> </span><span class="kw">simulate_chain</span>(<span class="dv">1</span>)</span>
<span id="cb136-41"><a href="#cb136-41" aria-hidden="true"></a>(<span class="dt">p =</span> <span class="kw">mean</span>(df<span class="op">$</span>X100 <span class="op">==</span><span class="st"> </span><span class="dv">3</span>))</span>
<span id="cb136-42"><a href="#cb136-42" aria-hidden="true"></a><span class="co">## [1] 0.591</span></span></code></pre></div>
<p><em>R.</em> The function <code>draw_next</code> is at the heart of the simulation. Given the current state <code>s</code>, it looks up the row of the transition matrix <code>P</code> which corresponds to <code>s</code>. This is where the function <code>match</code> comes in handy - <code>match(s,S)</code> gives you the position of th element <code>s</code> in the vector <code>S</code>. Of course, if <span class="math inline">\(S = \{ 1,2,3, \dots, n\}\)</span> then we don’t need to use <code>match</code>, as each state is “its own position”. In our case, <code>S</code> is a bit different, namely <span class="math inline">\(S=\{0,1,2,3\}\)</span>, and so <code>match(s,S)</code> is nothing by <code>s+1</code>. This is clearly an overkill in this case, but we still do it for didactical purposes.</p>
<p>Once the row corresponding to the state <code>s</code> is identified, we use its elements as the probabilities to be fed into the command <code>sample</code>, which, in turn, returns our next state and we repeat the procedure over and over (in this case <span class="math inline">\(T=100\)</span> times).</p>
</div>
</div>
<div id="additional-problems-for-chapter-5" class="section level2" number="5.5">
<h2 number="5.5"><span class="header-section-number">5.5</span> Additional problems for Chapter 5</h2>
<!--
  max-roll-so-far
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(\{Y_n\}_{n\in {\mathbb{N}}_0}\)</span> be a sequence
of die-rolls, i.e., a sequence of independent random variables which take values <span class="math inline">\(1,2,\dots, 6\)</span>, each with probability <span class="math inline">\(1/6\)</span>. Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a stochastic process defined by
<span class="math display">\[X_n=\max (Y_0,Y_1,
\dots, Y_n), \ n\in{\mathbb{N}}_0.\]</span> In words, <span class="math inline">\(X_n\)</span> is the maximal value rolled
so far. Is <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> a Markov chain? If it is, find its
transition matrix and the initial distribution. If it is not, give an example
of how the Markov property is violated.</p>
</div>
<div class="solution">
<p>It turns out that <span class="math inline">\(\{X_n\}_{n\in{\mathbb{N}}}\)</span> is, indeed, a Markov chain.
The value of <span class="math inline">\(X_{n+1}\)</span> is either going to be equal to <span class="math inline">\(X_n\)</span> if <span class="math inline">\(Y_{n+1}\)</span>
happens to be less than or equal to it, or it moves up to <span class="math inline">\(Y_{n+1}\)</span>,
otherwise, i.e., <span class="math inline">\(X_{n+1}=\max(X_n,Y_{n+1})\)</span>. Therefore, the
distribution of <span class="math inline">\(X_{n+1}\)</span> depends on the previous values
<span class="math inline">\(X_0,X_1,\dots, X_n\)</span> only through <span class="math inline">\(X_n\)</span>, and, so, <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> is a Markov
chain on the state space <span class="math inline">\(S=\{1,2,3,4,5,6\}\)</span>. The transition matrix
is given by <span class="math display">\[P=\begin{bmatrix} 
1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\
0   &amp; 1/3 &amp; 1/6 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\
0   &amp; 0   &amp; 1/2 &amp; 1/6 &amp; 1/6 &amp; 1/6 \\
0   &amp; 0   &amp; 0   &amp; 2/3 &amp; 1/6 &amp; 1/6 \\
0   &amp; 0   &amp; 0   &amp; 0   &amp; 5/6 &amp; 1/6 \\
0   &amp; 0   &amp; 0   &amp; 0   &amp; 0   &amp; 1 \\
\end{bmatrix}\]</span></p>
</div>
<!-- 
  Y-Z-Markov-chains
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a simple symmetric random walk. For <span class="math inline">\(n\in{\mathbb{N}}_0\)</span>, define
<span class="math inline">\(Y_n = 2X_n+1\)</span>, and let <span class="math inline">\(Z_n\)</span> be the amount of time <span class="math inline">\(X_n\)</span> spent
strictly above <span class="math inline">\(0\)</span> up to (and including) time <span class="math inline">\(n\)</span>, i.e.
<span class="math display">\[Z_0=0, Z_{n+1} - Z_n = \begin{cases} 1, &amp; X_{n+1}&gt;0 \\ 0, &amp; X_
    {n+1}\leq 0 \end{cases} , \text{ for }n\in{\mathbb{N}}_0.\]</span>
Is <span class="math inline">\(Y\)</span> a Markov chain? Is <span class="math inline">\(Z\)</span>?</p>
</div>
<div class="solution">
<p><span class="math inline">\(Y\)</span> is a Markov chain because it is just a
random walk started at <span class="math inline">\(1\)</span> with steps of size <span class="math inline">\(2\)</span> (a more rigorous proof
would follow the same line of reasoning as the proof that random walks are Markov chains).
<span class="math inline">\(Z\)</span> is not a Markov chain because the knowledge of far history (beyond
the present position) affects the likelihood of the next transition as the
following example shows:
<span class="math display">\[\begin{aligned}
    {\mathbb{P}}[ Z_4=2| Z_0=0, Z_1=0, Z_2=0, Z_3=1]=1/2\end{aligned}\]</span> but
<span class="math display">\[\begin{aligned}
    {\mathbb{P}}[ Z_4=2| Z_0=0, Z_1=1, Z_2=1, Z_3=1]= 0.\end{aligned}\]</span></p>
</div>
<!--
  number-of-consecutives
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(\{\delta_n\}_{n\in{\mathbb{N}}}\)</span> be a sequence of independent coin tosses (i.e., random
variables with values <span class="math inline">\(T\)</span> or <span class="math inline">\(H\)</span> with equal probabilities). Let <span class="math inline">\(X_0=0\)</span>,
and, for <span class="math inline">\(n\in{\mathbb{N}}\)</span>, let <span class="math inline">\(X_n\)</span> be the number of times two consecutive
<span class="math inline">\(\delta\)</span>s take the same value in the first <span class="math inline">\(n+1\)</span> tosses. For example, if
the outcome of the coin tosses is TTHHTTTH …, we have <span class="math inline">\(X_0=0\)</span>,
<span class="math inline">\(X_1=1\)</span>, <span class="math inline">\(X_2=1\)</span>, <span class="math inline">\(X_3=2\)</span>, <span class="math inline">\(X_4=2\)</span>, <span class="math inline">\(X_5=3\)</span>, <span class="math inline">\(X_6=4\)</span>, <span class="math inline">\(X_7=4\)</span>, …</p>
<p>Is <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> a Markov chain? If it is,
describe its state space, the transition probabilities and the initial
distribution. If it is not, show exactly how the Markov property is
violated.</p>
</div>
<div class="solution">
<p>Yes, the process <span class="math inline">\(X\)</span> is a Markov chain, on the state space <span class="math inline">\(S={\mathbb{N}}_0\)</span>.
To show that we make the following simple observation: we have
<span class="math inline">\(X_{n}-X_{n-1}=1\)</span> if <span class="math inline">\(\delta_n=\delta_{n+1}\)</span> and <span class="math inline">\(X_n-X_{n-1}=0\)</span>, otherwise
(for <span class="math inline">\(n\in{\mathbb{N}}\)</span>). Therefore,
<span class="math display">\[{\mathbb{P}}[ X_{n+1}=i_n+1 | X_{n}=i_n, \dots, X_1=i_1, X_0=0] =
{\mathbb{P}}[ \delta_{n+2}=\delta_{n+1} | X_{n}=i_n, \dots, X_1=i_1,X_0=0].\]</span> Even if
we knew the exact values of all <span class="math inline">\(\delta_1,\dots, \delta_n,\delta_{n+1}\)</span>, the
(conditional) probability that <span class="math inline">\(\delta_{n+2}=\delta_{n+1}\)</span> would still be
<span class="math inline">\(1/2\)</span>, regardless of these values. Therefore,
<span class="math display">\[{\mathbb{P}}[ X_{n+1}=i_n+1| X_n=i_n,\dots, X_1=i_1, X_0=0] = \tfrac{1}{2},\]</span> and,
similarly, <span class="math display">\[{\mathbb{P}}[ X_{n+1}=i_n| X_n=i_n,\dots, X_1=i_1, X_0=0] = \tfrac{1}{2}.\]</span>
Therefore, the conditional probability given all the past depends on the
past only through the value of <span class="math inline">\(X_n\)</span> (the current position), and we
conclude that <span class="math inline">\(X\)</span> is, indeed, a Markov process. Its initial distribution
is deterministic <span class="math inline">\({\mathbb{P}}[X_0=0]=1\)</span>, and the transition probabilities, as
computed above, are <span class="math display">\[p_{ij}={\mathbb{P}}[ X_{n+1}=j| X_n=i] = \begin{cases}
  1/2, &amp;\text{ if } j=i+1, \\
  1/2, &amp;\text{ if } j=i, \\
  0, &amp;\text{ otherwise.}
  \end{cases}\]</span> In fact, <span class="math inline">\(2 X_n - n\)</span> is a simple symmetric random walk.</p>
</div>
<!--
  lazy-chain
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(X\)</span> be a Markov chain on <span class="math inline">\(N\)</span> states, with the <span class="math inline">\(N\times N\)</span> transition
matrix <span class="math inline">\(P\)</span>. We construct a new Markov chain <span class="math inline">\(Y\)</span> from the transition mechanism of <span class="math inline">\(X\)</span> as
follows: at
each point in time, we toss a biased coin (probability of <em>heads</em>
<span class="math inline">\(p\in (0,1)\)</span>), independently of everything else.
If it shows <em>heads</em> we move according to the transition matrix of <span class="math inline">\(X\)</span>. If it shows <em>tails</em>, we remain in the same state. What is the transition matrix of <span class="math inline">\(Y\)</span>?</p>
</div>
<div class="solution">
<p>Let <span class="math inline">\(Q=(q_{ij})\)</span> denote the transition probability for the chain <span class="math inline">\(Y\)</span>.
When <span class="math inline">\(i\ne j\)</span>, the chain <span class="math inline">\(Y\)</span> will go from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> in one step if and only if the
coin shows <em>heads</em> and the chain <span class="math inline">\(X\)</span> wants to jump from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>. Since the
two events are independent, the
probability of the former is <span class="math inline">\(p\)</span>, and of the later is <span class="math inline">\(p_{ij}\)</span>, we have
<span class="math inline">\(q_{ij} = p p_{ij}\)</span>.</p>
<p>In the case <span class="math inline">\(i=j\)</span>, the chain <span class="math inline">\(Y\)</span> will transition from <span class="math inline">\(i\)</span> to <span class="math inline">\(i\)</span> (i.e.,
stay in <span class="math inline">\(i\)</span>) if either the coin shows <em>heads</em>, or if the coin shows
<em>tails</em> and the chain <span class="math inline">\(X\)</span> decides to stay in <span class="math inline">\(i\)</span>. Therefore,
<span class="math inline">\(q_{ii} = p + (1-p) p_{ij}\)</span>, i.e.,
<span class="math display">\[ Q = p \operatorname{Id}+(1-p) P,\]</span>
where <span class="math inline">\(\operatorname{Id}\)</span> denotes <span class="math inline">\(N\times N\)</span> identity matrix.</p>
</div>
<!--
  blue-red-100
  ------------------------------------------------
-->
<div class="problem">
<p>The red container has 100 red balls, and
the blue container has 100 blue balls. In each step</p>
<p>- a container is selected (with equal probabilities),</p>
<p>- a ball is selected from it (all balls in the container are equally
likely to be selected), and</p>
<p>- the selected ball is placed in the other container. If the
selected container is empty, no ball is transferred.</p>
<p>Once there are 100 blue balls in the red container and 100 red balls in
the blue container, the game stops.</p>
<p>We decide to model the situation as a Markov chain.</p>
<ol style="list-style-type: decimal">
<li><p>What is the state space <span class="math inline">\(S\)</span> we can use? How large is it?</p></li>
<li><p>What is the initial distribution?</p></li>
<li><p>What are the transition probabilities between states? Don’t write
the matrix, it is way too large; just write a general expression for
<span class="math inline">\(p_{ij}\)</span>, <span class="math inline">\(i,j\in S\)</span>.</p></li>
</ol>
<p>(Note: this is a version of the famous Ehrenfest Chain from statistical physics.)</p>
</div>
<div class="solution">
<p>There are many ways in which one can solve this problem. Below is just
one of them.</p>
<p><part> 1. </part></p>
<p>In order to describe the situation being modeled, we need to keep
track of the number of balls of each color in each container.
Therefore, one possibility is to take the set of all quadruplets
<span class="math inline">\((r,b,R,B)\)</span>, <span class="math inline">\(r,b,R,b\in \{0,1,2,\dots, 100\}\)</span> and this state
space would have <span class="math inline">\(101^4\)</span> elements. We know, however, that the total
number of red balls, and the total number of blue balls is always
equal to 100, so the knowledge of the composition of the red (say)
container is enough to reconstruct the contents of the blue
container. In other words, we can use the number of balls of each
color in the red container only as our state, i.e.
<span class="math display">\[S= \{ (r,b)\, : \, r,b=0,1,\dots, 100\}.\]</span> This state space has
<span class="math inline">\(101\times 101=10201\)</span> elements.</p>
<p><part> 2. </part></p>
<p>The initial distribution is deterministic: <span class="math inline">\({\mathbb{P}}[X_0=(100,0)]=1\)</span>
and <span class="math inline">\({\mathbb{P}}[X_0=i]=0\)</span>, for <span class="math inline">\(i\in  S\setminus\{(100,0)\}\)</span>. In the vector notation,
<span class="math display">\[{a}^{(0)}=(0,0, \dots, 0, 1, 0, \dots,
0),\]</span> where <span class="math inline">\(1\)</span> is at the place corresponding to <span class="math inline">\((100,0)\)</span>.</p>
<p><part> 3. </part></p>
<p>Let us consider several separate cases, with the understanding that
<span class="math inline">\(p_{ij}=0\)</span>, for all <span class="math inline">\(i,j\)</span> not mentioned explicitly below:</p>
<ol style="list-style-type: decimal">
<li><p><em>One of the containers is empty.</em> In that case, we are either in
<span class="math inline">\((0,0)\)</span> or in <span class="math inline">\((100,100)\)</span>. Let us describe the situation for
<span class="math inline">\((0,0)\)</span> first. If we choose the red container - and that happens
with probability <span class="math inline">\(\tfrac{1}{2}\)</span> - we stay in <span class="math inline">\((0,0)\)</span>:
<span class="math display">\[p_{(0,0),(0,0)}=\tfrac{1}{2}.\]</span> If the blue container is chosen, a
ball of either color will be chosen with probability
<span class="math inline">\(\tfrac{100}{200}=\tfrac{1}{2}\)</span>, so
<span class="math display">\[p_{(0,0),(1,0)}=p_{(0,0),(0,1)}=\tfrac{1}{4}.\]</span> By the same
reasoning, <span class="math display">\[p_{(100,100),(0,0)}=\tfrac{1}{2}\text{ and } 
p_{(100,100),(99,100)}=p_{(100,100),(100,99)}=\tfrac{1}{4}.\]</span></p></li>
<li><p><em>We are in the state</em> <span class="math inline">\((0,100)\)</span>. By the description of the
model, this is an absorbing state, so <span class="math inline">\(p_{(0,100),(0,100)}=1.\)</span></p></li>
<li><p><em>All other cases</em> Suppose we are in the state <span class="math inline">\((r,b)\)</span> where
<span class="math inline">\((r,b)\not\in\{(0,100),(0,0),(100,100)\}\)</span>. If the red
container is chosen, then the probability of getting a red ball
is <span class="math inline">\(\tfrac{r}{r+b}\)</span>, so
<span class="math display">\[p_{(r,b),(r-1,b)}= \tfrac{1}{2}\tfrac{r}{r+b}.\]</span> Similarly,
<span class="math display">\[p_{(r,b),(r,b-1)}= \tfrac{1}{2}\tfrac{b}{r+b}.\]</span> In the blue
container there are <span class="math inline">\(100-r\)</span> red and <span class="math inline">\(100-b\)</span> blue balls. Thus,
<span class="math display">\[p_{(r,b),(r+1,b)}= \tfrac{1}{2}\tfrac{100-r}{200-r-b},\]</span> and
<span class="math display">\[p_{(r,b),(r,b+1)}= \tfrac{1}{2}\tfrac{100-b}{200-r-b}.\]</span></p></li>
</ol>
</div>
<!--
  deck-2-2
  ------------------------------------------------
-->
<div class="problem">
<p>A “deck” of cards starts with 2 red and 2 black cards. A “move” consists
of the following:</p>
<p>- pick a random card from the deck (if the deck is empty, do nothing),</p>
<p>- if the card is black <em>and</em> the card drawn on the previous move was also black, return it back to the deck,</p>
<p>- otherwise, throw the card away (this, in particular, applies to any card drawn on the first move, since there is no “previous” move at that time).</p>
<ol style="list-style-type: decimal">
<li><p>Model the situation using a Markov chain: find an appropriate state
space, and sketch the transition graph with transition
probabilities. How small can you make the state space?</p></li>
<li><p>What is the probability that the deck will be empty after exactly
<span class="math inline">\(4\)</span> moves? What is the probability that the deck will be empty
eventually?</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<p>We need to keep track of the number of remaining cards of each color
in the deck, as well as the color of the last card we picked (except
at the beginning or when the deck is empty, when it does not
matter). Therefore, the initial state will be <span class="math inline">\((2,2)\)</span>, the
empty-deck state will be <span class="math inline">\((0,0)\)</span> and the other states will be
triplets of the form <span class="math inline">\((\#r, \#b, c)\)</span>, where <span class="math inline">\(\#r\)</span> and <span class="math inline">\(\#b\)</span> denote
the number of cards (red and black) in the deck, and <span class="math inline">\(c\)</span> is the
color, <span class="math inline">\(R\)</span> or <span class="math inline">\(B\)</span>, of the last card we picked. This way, the initial
guess for the state space would be <span class="math display">\[\begin{aligned}
        S_0  = \{&amp;(2,2), (0,0),\\
            &amp; (2,1,B), (2,1,R), (1,2,B), (1,2,R),\\
            &amp; (1,1,B), (1,1,R), 
            (0,2,B), (2,0,R), (2,0,B), (0,2,R),\\
            &amp; (0,1,B), (0,1,R), (1,0,B), (1,0,R) 
          \}
      \end{aligned}\]</span></p>
<p>In order to decrease the size of the state space, we start the chain at
<span class="math inline">\((2,2)\)</span> and consider all trajectories it is possible to take from there.
It turns out that states <span class="math inline">\((2,1,R), (1,2,B), (0,2,B), (2,0,R),  (2,0,B)\)</span> and <span class="math inline">\((1,0,R)\)</span> can never be reached from <span class="math inline">\((2,2)\)</span>, so
we might as well leave them out of the state space. That reduces the
initial guess <span class="math inline">\(S_0\)</span> to a smaller <span class="math inline">\(10\)</span>-state, version
<span class="math display">\[\begin{equation}
S  = \{(2,2), (0,0),
             (2,1,B),  (1,2,R),
             (1,1,B),  (1,1,R), 
             (0,2,R),
             (0,1,B), (0,1,R), (1,0,B)  
         \}
\end{equation}\]</span>
with the following transition graph:</p>
<p>You could further reduce the number of states to <span class="math inline">\(9\)</span> by removing the initial state <span class="math inline">\((2,2)\)</span>
and choosing a non-deterministic distribution over the states that
can be reached from them. There is something unsatisfying about that, though.</p>
<p><part> 2. </part></p>
<p>To get from <span class="math inline">\((2,2)\)</span> to <span class="math inline">\((0,0)\)</span> in exactly four steps, we need to
follow one of the following three paths: <span class="math display">\[\begin{aligned}
                &amp; (2,2) \to (2,1,B) \to (1,1,R) \to (1,0,B) \to (0,0), \\
                &amp; (2,2) \to (2,1,B) \to (1,1,R) \to (0,1,R) \to (0,0), \text{
                or }\\
                &amp; (2,2) \to (1,2,R) \to (1,1,B) \to (0,1,R) \to (0,0). \\
             \end{aligned}\]</span>
Their respective probabilities happen to be
the same, namely
<span class="math inline">\(\tfrac{1}{2}\times \tfrac{2}{3} \times \tfrac{1}{2}\times 1 = \frac{1}{6}\)</span>, so the
probability of hitting <span class="math inline">\((0,0)\)</span> in exactly <span class="math inline">\(4\)</span> steps is
<span class="math inline">\(3 \times \frac{1}{6} = \tfrac{1}{2}\)</span>.</p>
<p>To compute the probability of hitting <span class="math inline">\((0,0)\)</span> eventually, we note
that this is guaranteed to happen sooner or later (see the graph
above) if the first card we draw is black. It is also guaranteed to
happen is the first card we draw is red, but the second one is
black. In fact, the only way for this not to happen is to draw two
red cards on the first two draws. This happens with probability
<span class="math inline">\(\tfrac{1}{2}\times \frac{1}{3} = \frac{1}{6}\)</span>, so the required probability of ending up with an
empty deck is <span class="math inline">\(1 - \frac{1}{6} = \frac{5}{6}\)</span>.</p>
</div>
<!--
  train-m-cities
  ------------------------------------------------
-->
<div class="problemec">
<p>A country has <span class="math inline">\(m+1\)</span> cities (<span class="math inline">\(m\in{\mathbb{N}}\)</span>), one of which is the capital.
There is a direct railway connection between each city and the capital,
but there are no tracks between any two “non-capital” cities. A traveler
starts in the capital and takes a train to a randomly chosen non-capital
city (all cities are equally likely to be chosen), spends a night there
and returns the next morning and immediately boards the train to the
next city according to the same rule, spends the night there, …, etc.
We assume that her choice of the city is independent of the cities
visited in the past. Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be the number of visited non-capital
cities up to (and including) day <span class="math inline">\(n\)</span>, so that <span class="math inline">\(X_0=1\)</span>, but <span class="math inline">\(X_1\)</span> could
be either <span class="math inline">\(1\)</span> or <span class="math inline">\(2\)</span>, etc.</p>
<ol style="list-style-type: decimal">
<li><p>Explain why <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> is a Markov chain on the appropriate state
space <span class="math inline">\({\mathcal{S}}\)</span> and the find the transition probabilities of <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>,
i.e., write an expression for
<span class="math display">\[{\mathbb{P}}[X_{n+1}=j|X_n=i], \text{ for $i,j\in S$.}\]</span></p></li>
<li><p>Let <span class="math inline">\(\tau_m\)</span> be the first time the traveler has visited all <span class="math inline">\(m\)</span>
non-capital cities, i.e. <span class="math display">\[\tau_m=\min \{ n\in{\mathbb{N}}_0\, : \, X_n=m\}.\]</span> What
is the distribution of <span class="math inline">\(\tau_m\)</span>, for <span class="math inline">\(m=1\)</span> and <span class="math inline">\(m=2\)</span>.</p></li>
<li><p>Compute <span class="math inline">\({\mathbb{E}}[\tau_m]\)</span> for general <span class="math inline">\(m\in{\mathbb{N}}\)</span>. What is the
asymptotic behavior of <span class="math inline">\({\mathbb{E}}[\tau_m]\)</span> as <span class="math inline">\(m\to\infty\)</span>? More
precisely, find a simple function <span class="math inline">\(f(m)\)</span> of <span class="math inline">\(m\)</span> (like <span class="math inline">\(m^2\)</span> or
<span class="math inline">\(\log(m)\)</span>) such that <span class="math inline">\({\mathbb{E}}[\tau_m] \sim f(m)\)</span>, i.e.,
<span class="math inline">\(\lim_{m\to\infty} \frac{{\mathbb{E}}[\tau_m]}{f(m)} = 1\)</span>.</p></li>
</ol>
</div>
<div class="solution">
<p><part> 1. </part></p>
<p>The natural state space for <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> is <span class="math inline">\(S=\{1,2,\dots,  m\}\)</span>. It is clear that <span class="math inline">\({\mathbb{P}}[X_{n+1}=j|X_n=i]=0,\)</span> unless, <span class="math inline">\(i=j\)</span>
or <span class="math inline">\(i=j+1\)</span>. If we start from the state <span class="math inline">\(i\)</span>, the process will remain
in <span class="math inline">\(i\)</span> if the traveler visits one of the already-visited cities, and
move to <span class="math inline">\(i+1\)</span> is the visited city has never been visited before.
Thanks to the uniform distribution in the choice of the next city,
the probability that a never-visited city will be selected is
<span class="math inline">\(\tfrac{m-i}{m}\)</span>, and it does not depend on the (names of the)
cities already visited, or on the times of their first visits; it
only depends on their number. Consequently, the extra information
about <span class="math inline">\(X_1,X_2,\dots, X_{n-1}\)</span> will not change the probability of
visiting <span class="math inline">\(j\)</span> in any way, which is exactly what the Markov property
is all about. Therefore, <span class="math inline">\(\{X_n\}_{n\in{\mathbb{N}}}\)</span> is Markov and its transition
probabilities are given by <span class="math display">\[p_{ij}={\mathbb{P}}[X_{n+1}=j|X_{n}=i]=
\begin{cases}
  0, &amp; j\not \in \{i,i+1\}\\
  \tfrac{m-i}{m}, &amp; j=i+1\\
  \tfrac{i}{m}, &amp; j=i.
\end{cases}\]</span> (<em>Note:</em> the situation would not be nearly as nice if
the distribution of the choice of the next city were non-uniform. In
that case, the list of the (names of the) already-visited cities
would matter, and it is not clear that the described process has the
Markov property (does it?). )</p>
<p><part> 2. </part></p>
<p>For <span class="math inline">\(m=1\)</span>, <span class="math inline">\(\tau_m=0\)</span>, so its distribution is deterministic and
concentrated on <span class="math inline">\(0\)</span>. The case <span class="math inline">\(m=2\)</span> is only slightly more
complicated. After having visited his first city, the visitor has a
probability of <span class="math inline">\(\tfrac{1}{2}\)</span> of visiting it again, on each consecutive day.
After a geometrically distributed number of days, he will visit
another city and <span class="math inline">\(\tau_2\)</span> will be realized. Therefore the
distribution <span class="math inline">\(\{p_n\}_{n\in {\mathbb{N}}_0}\)</span> of <span class="math inline">\(\tau_2\)</span> is given by
<span class="math display">\[p_0=0, p_1=\tfrac{1}{2}, p_2=(\tfrac{1}{2})^2, p_3=(\tfrac{1}{2})^3,\dots\]</span></p>
<p><part> 3. </part></p>
<p>For <span class="math inline">\(m&gt;1\)</span>, we can write <span class="math inline">\(\tau_m\)</span> as
<span class="math display">\[\tau_m=\tau_1+(\tau_2-\tau_1)+\dots +(\tau_m-\tau_{m-1}),\]</span> so
that
<span class="math display">\[{\mathbb{E}}[\tau_m]={\mathbb{E}}[\tau_1]+{\mathbb{E}}[\tau_2-\tau_1]+\dots+{\mathbb{E}}[\tau_m-\tau_{m-1}].\]</span>
We know that <span class="math inline">\(\tau_1=0\)</span> and for <span class="math inline">\(k=1,2,\dots, m-1\)</span>, the difference
<span class="math inline">\(\tau_{k+1}-\tau_{k}\)</span> denotes the waiting time before a
never-before-visited city is visited, given that the number of
already-visited cities is <span class="math inline">\(k\)</span>. This random variable is geometric
with success probability given by <span class="math inline">\(\tfrac{m-k}{m}\)</span>, so its
expectation is given by
<span class="math display">\[{\mathbb{E}}[\tau_{k+1}-\tau_k]= \frac{1}{ \tfrac{m-k}{m}}=\frac{m}{m-k}.\]</span>
Therefore, <span class="math display">\[{\mathbb{E}}[\tau_m]=\sum_{k=1}^{m-1} \frac{m}{m-k}= m
(1+\tfrac{1}{2}+\tfrac{1}{3}+\dots+\tfrac{1}{m-1}).\]</span> By comparing it with
the integral <span class="math inline">\(\int_1^m \frac{1}{x}\, dx\)</span>, it is possible to conclude that
<span class="math inline">\(H_m=1+\tfrac{1}{2}+\dots+\tfrac{1}{m-1}\)</span> behaves like <span class="math inline">\(\log m\)</span>, i.e., that
<span class="math display">\[\lim_{m\to\infty} \frac{H_m}{\log m} = 1.\]</span> Therefore
<span class="math inline">\({\mathbb{E}}[\tau_m] \sim f(m)\)</span>, where <span class="math inline">\(f(m) = m \log m\)</span>.</p>
</div>
<!-- multi-step -->
<!--
  glass-milk
  ------------------------------------------------
-->
<div class="problem">
<p>We start with two cups, call them <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Cup <span class="math inline">\(A\)</span> contains <span class="math inline">\(12\)</span> oz of milk, and cup <span class="math inline">\(B\)</span> <span class="math inline">\(12\)</span> oz of water. The
following procedure is then performed <em>twice</em>: first, half of the
content of the glass <span class="math inline">\(A\)</span> is transferred into class <span class="math inline">\(B\)</span>. Then, the
contents of glass <span class="math inline">\(B\)</span> are thoroughly mixed, and a third of its entire
content transferred back to <span class="math inline">\(A\)</span>. Finally, the contents of the glass <span class="math inline">\(A\)</span>
are thoroughly mixed. What is the final amount of milk in glass A? What
does this have to do with Markov chains?</p>
</div>
<div class="solution">
<p>If there are <span class="math inline">\(a\)</span> oz of milk and <span class="math inline">\(b\)</span> oz of water in the glass <span class="math inline">\(A\)</span> at time
<span class="math inline">\(n\)</span> (with <span class="math inline">\(a+b=12\)</span>), then there are <span class="math inline">\(b\)</span> oz of milk and <span class="math inline">\(a\)</span> oz of water
in the glass <span class="math inline">\(B\)</span>. After half of the content of glass <span class="math inline">\(A\)</span> is moved to
<span class="math inline">\(B\)</span>, it will contain <span class="math inline">\(b+\tfrac{1}{2}a\)</span> oz of milk and <span class="math inline">\(a+\tfrac{1}{2}b\)</span> oz of water.
Transferring a third of that back to <span class="math inline">\(a\)</span> leaves <span class="math inline">\(B\)</span> with
<span class="math inline">\((2/3 b + 1/3 a)\)</span> oz of milk and <span class="math inline">\((2/3 a + 1/3 b)\)</span> oz of water.
Equivalently, <span class="math inline">\(A\)</span> contains <span class="math inline">\((2/3 a + 1/3 b)\)</span> oz of milk and
<span class="math inline">\((1/3 a + 2/3 b)\)</span> oz of water. This corresponds to the action of a
Markov chain with the transition matrix
<span class="math inline">\(P = \begin{bmatrix} 2/3 &amp; 1/3 \\ 1/3 &amp; 2/3 \end{bmatrix}\)</span>. We get the required amounts by
computing <span class="math display">\[\begin{aligned}
    (12,0) P^2 = (12,0) \begin{bmatrix} 5/9 &amp; 4/9 \\ 4/9 &amp; 5/9\end{bmatrix} = (20/3, 16/3).\end{aligned}\]</span></p>
</div>
<!--
  manual-multi-step
  ------------------------------------------------
-->
<div class="problem">
<p>The state space of a Markov chain is <span class="math inline">\(S = \{1,2,3,4,5\}\)</span>, and the
non-zero transition probabilities are given by <span class="math inline">\(p_{11} = 1/2\)</span>,
<span class="math inline">\(p_{12}=1/2\)</span>, <span class="math inline">\(p_{23}=p_{34}=p_{45}=p_{51}=1\)</span>. Compute
<span class="math inline">\(p^{(6)}_{12}\)</span> without using software.</p>
</div>
<div class="solution">
<p>As you can see from the transition graph below</p>
<p><img src="_main_files/figure-html/unnamed-chunk-317-1.png" width="672" style="margin-top:-5%; margin-bottom: -10%" style="display: block; margin: auto;" /></p>
<p>You can go from <span class="math inline">\(1\)</span> to <span class="math inline">\(2\)</span> in <span class="math inline">\(6\)</span> steps in
exactly two ways: <span class="math display">\[1 \to 2 \to 3 \to 4 \to 5 \to 1 \to 2\]</span> and
<span class="math display">\[1 \to 1 \to 1 \to 1 \to 1 \to 1 \to 2\]</span>
The probability of the first
path is <span class="math inline">\(2^{-2}\)</span> and the probability of the second path is <span class="math inline">\(2^{-6}\)</span> -
they add up to <span class="math inline">\(\tfrac{17}{64}\)</span>.</p>
</div>
<!--
  gambler-multi-step
  ------------------------------------------------
-->
<div class="problem">
<p>In a <em>Gambler’s ruin</em> problem with the state space <span class="math inline">\(S=\{0,1,2,3,4\}\)</span>
and the probability <span class="math inline">\(p=1/3\)</span> of winning in a single game, compute the <span class="math inline">\(4\)</span>-step
transition probabilities
<span class="math display">\[p^{(4)}_{2 2} = {\mathbb{P}}[ X_{n+4}=2| X_n =2] \text{ and } p^{(4)}_{2 4} = {\mathbb{P}}[ X_{n+4}=4| X_n =2].\]</span></p>
</div>
<div class="solution">
<p>There are four <span class="math inline">\(4\)</span>-step trajectories that
start in <span class="math inline">\(2\)</span> and end in <span class="math inline">\(2\)</span>, with positive probabilities (remember, once
you hit <span class="math inline">\(0\)</span> or <span class="math inline">\(4\)</span> you get stuck there), namely <span class="math display">\[\begin{aligned}
    &amp; 2 \to 1 \to 2 \to 1 \to 2, \quad 
    2 \to 1 \to 2 \to 3 \to 2, \quad  \\
    &amp; 2 \to 3 \to 2 \to 1 \to 2, \quad
    2 \to 3 \to 2 \to 3 \to 2.\end{aligned}\]</span> Each has probability
<span class="math inline">\((1/3)\times(2/3)\times(1/3)\times(2/3) = 4/81\)</span> so the total probability
is <span class="math inline">\(16/81\)</span>.</p>
<p>The (possible) trajectories that go from <span class="math inline">\(2\)</span> to <span class="math inline">\(4\)</span> in exactly 4 steps
are <span class="math display">\[\begin{aligned}
    2 \to 1 \to 2 \to 3 \to 4, \quad 
    2 \to 3 \to 2 \to 3 \to 4\   \text{ and }\
    2 \to 3 \to 4 \to 4 \to 4.\end{aligned}\]</span> The first two have the
same probability, namely
<span class="math inline">\((2/3)\times(1/3)\times(2/3)\times(2/3) = 8/81\)</span>, and the third one
<span class="math inline">\((1/3)\times(2/3)\times(1)\times(1) = 18/81\)</span> so <span class="math inline">\(p^{(4)}_{24} = 26/81\)</span>.</p>
</div>
<!--
  car-insurance
  ------------------------------------------------
-->
<div class="problem">
<p>A car-insurance company classifies drivers in three categories: <em>bad</em>,
<em>neutral</em> and <em>good</em>. The reclassification is done in January of each
year and the probabilities for transitions between different categories
is given by
<span class="math display">\[P= \begin{bmatrix} 1/2 &amp; 1/2 &amp; 0 \\ 1/5 &amp; 2/5 &amp; 2/5 \\ 1/5 &amp; 1/5 &amp; 3/5\end{bmatrix},\]</span> where
the first row/column corresponds to the <em>bad</em> category, the second to
<em>neutral</em> and the third to <em>good</em>. The company started in January 1990
with 1400 drivers in each category. Estimate the number of drivers in
each category in 2090. Assume that the total number of drivers does not
change in time and use R for your computations.</p>
</div>
<div class="solution">
<p>Equal numbers of drivers in each category corresponds to the uniform
initial distribution, <span class="math inline">\(a^{(0)}=(1/3,1/3,1/3)\)</span>. The
distribution of drivers in 2090 is given by the distribution
<span class="math inline">\(a^{(100)}\)</span> of <span class="math inline">\(X_{100}\)</span> which is, in turn, given by
<span class="math display">\[a^{(100)}= a^{(0)} P^{100}.\]</span> Finally, we need to compute the <em>number</em> of drivers in
each category, so we multiply the result by the total number of drivers, i.e., <span class="math inline">\(3 \times 1400 = 4200\)</span>:</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true"></a>P =<span class="st"> </span><span class="kw">matrix</span>(</span>
<span id="cb137-2"><a href="#cb137-2" aria-hidden="true"></a>  <span class="kw">c</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span> , <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> , <span class="dv">0</span>,  </span>
<span id="cb137-3"><a href="#cb137-3" aria-hidden="true"></a>    <span class="dv">1</span><span class="op">/</span><span class="dv">5</span> , <span class="dv">2</span><span class="op">/</span><span class="dv">5</span> , <span class="dv">2</span><span class="op">/</span><span class="dv">5</span> , </span>
<span id="cb137-4"><a href="#cb137-4" aria-hidden="true"></a>    <span class="dv">1</span><span class="op">/</span><span class="dv">5</span> , <span class="dv">1</span><span class="op">/</span><span class="dv">5</span> , <span class="dv">3</span><span class="op">/</span><span class="dv">5</span>), </span>
<span id="cb137-5"><a href="#cb137-5" aria-hidden="true"></a>  <span class="dt">byrow=</span>T, <span class="dt">ncol=</span><span class="dv">3</span>)</span>
<span id="cb137-6"><a href="#cb137-6" aria-hidden="true"></a></span>
<span id="cb137-7"><a href="#cb137-7" aria-hidden="true"></a><span class="co"># a0 needs to be a row matrix </span></span>
<span id="cb137-8"><a href="#cb137-8" aria-hidden="true"></a>a0 =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">3</span>), <span class="dt">nrow=</span><span class="dv">1</span>) </span>
<span id="cb137-9"><a href="#cb137-9" aria-hidden="true"></a></span>
<span id="cb137-10"><a href="#cb137-10" aria-hidden="true"></a>P100 =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">3</span>) <span class="co"># the 3x3 identity matrix</span></span>
<span id="cb137-11"><a href="#cb137-11" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>)</span>
<span id="cb137-12"><a href="#cb137-12" aria-hidden="true"></a>  P100 =<span class="st"> </span>P100 <span class="op">%*%</span><span class="st"> </span>P</span>
<span id="cb137-13"><a href="#cb137-13" aria-hidden="true"></a></span>
<span id="cb137-14"><a href="#cb137-14" aria-hidden="true"></a>(a0 <span class="op">%*%</span><span class="st"> </span>P100) <span class="op">*</span><span class="st"> </span><span class="dv">4200</span></span>
<span id="cb137-15"><a href="#cb137-15" aria-hidden="true"></a><span class="co">##      [,1] [,2] [,3]</span></span>
<span id="cb137-16"><a href="#cb137-16" aria-hidden="true"></a><span class="co">## [1,] 1200 1500 1500</span></span></code></pre></div>
<p>Note: if you think that computing matrix powers using for loops is in poor taste, there are several R packages you can use. Have a look at <a href="https://stats.stackexchange.com/questions/4320/compute-the-power-of-a-matrix-in-r/187477">this post</a> if you are curious.</p>
</div>
<!--
  basil
  ------------------------------------------------
-->
<div class="problem">
<p>A zoologist, Dr. Gurkensaft,
claims to have trained Basil the Rat so that it can avoid being shocked
and find food, even in highly confusing situations. Another scientist,
Dr. Hasenpfeffer does not agree. She says that Basil is stupid and
cannot tell the difference between food and an electrical shocker until
it gets very close to either of them.</p>
<p>The two decide to see who is right by performing the following
experiment. Basil is put in the compartment <span class="math inline">\(3\)</span> of a maze that looks
like this:</p>
<center>
<p><img src="pics/basil-the-rat.png" width="40%" style="display: block; margin: auto;" /></p>
</center>
<p>Dr. Gurkensaft’s hypothesis is that, once in a compartment with <span class="math inline">\(k\)</span> exits, Basil
will prefer the exits that lead him closer to the food. Dr. Hasenpfeffer’s claim
is that every time there are <span class="math inline">\(k\)</span> exits from a compartment, Basil chooses each
one with probability <span class="math inline">\(1/k\)</span>.</p>
<p>After repeating the experiment 100 times, Basil got shocked before getting to
food <span class="math inline">\(52\)</span> times and he reached food before being shocked <span class="math inline">\(48\)</span> times.</p>
<ol style="list-style-type: decimal">
<li><p>Create an Markov chain that models this situation (draw a transition graph and mark the edges with their probabilities).</p></li>
<li><p>Use Monte Carlo to estimate the probability of being
shocked before
getting to food, under the assumption that Basil is stupid
(all exits are equally likely).</p></li>
</ol>
<p>Btw, who do you think is right? Whose side is the evidence (48 vs. 52) on? If you know how to perform an appropriate statistical test here, do it. If you don’t simply state what you think.</p>
</div>
<div class="solution">
<p><part> 1. </part>
Basil’s behavior can be modeled by a Markov Chain with states
corresponding to compartments, and transitions to their adjacency.
The graph of such a chain, on the state space <span class="math inline">\(S=\{1,2,3,4,5,F,S\}\)</span>
would look like this (with black = <span class="math inline">\(1\)</span>, orange = <span class="math inline">\(1/2\)</span> and green=<span class="math inline">\(1/3\)</span>)</p>
<center>
<p><img src="_main_files/figure-html/unnamed-chunk-320-1.png" width="672" style="margin-top:-10%; margin-bottom: -15%" style="display: block; margin: auto;" /></p>
</center>
<p><part> 2. </part>
To be able to do Monte Carlo, we need to construct its transition
matrix. Since there are far fewer transitions than pairs of states, it is a good idea to start with a matrix of <span class="math inline">\(0\)</span>s and then fill in the non-zero values. We also decide that <span class="math inline">\(F\)</span> and <span class="math inline">\(S\)</span> will be given the last two rows/columns, i.e., numbers <span class="math inline">\(6\)</span> and <span class="math inline">\(7\)</span>:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1" aria-hidden="true"></a>P =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow =</span><span class="dv">7</span>, <span class="dt">ncol=</span><span class="dv">7</span> )</span>
<span id="cb138-2"><a href="#cb138-2" aria-hidden="true"></a>P[<span class="dv">1</span>,<span class="dv">2</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>; P[<span class="dv">1</span>,<span class="dv">3</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>;</span>
<span id="cb138-3"><a href="#cb138-3" aria-hidden="true"></a>P[<span class="dv">2</span>,<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>; P[<span class="dv">2</span>,<span class="dv">4</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>; P[<span class="dv">2</span>,<span class="dv">6</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>;</span>
<span id="cb138-4"><a href="#cb138-4" aria-hidden="true"></a>P[<span class="dv">3</span>,<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>; P[<span class="dv">3</span>,<span class="dv">4</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>; P[<span class="dv">3</span>,<span class="dv">7</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>;</span>
<span id="cb138-5"><a href="#cb138-5" aria-hidden="true"></a>P[<span class="dv">4</span>,<span class="dv">2</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>; P[<span class="dv">4</span>,<span class="dv">3</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>; P[<span class="dv">4</span>,<span class="dv">5</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>;</span>
<span id="cb138-6"><a href="#cb138-6" aria-hidden="true"></a>P[<span class="dv">5</span>,<span class="dv">4</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>; P[<span class="dv">5</span>,<span class="dv">6</span>] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>;</span>
<span id="cb138-7"><a href="#cb138-7" aria-hidden="true"></a>P[<span class="dv">6</span>,<span class="dv">6</span>] =<span class="st"> </span><span class="dv">1</span></span>
<span id="cb138-8"><a href="#cb138-8" aria-hidden="true"></a>P[<span class="dv">7</span>,<span class="dv">7</span>] =<span class="st"> </span><span class="dv">1</span></span></code></pre></div>
<p>We continue by simulating <code>nsim = 1000</code> trajectories of this chain, starting from the state <span class="math inline">\(3\)</span>. We compress and reuse the code from section @ref(mc-sim) above:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1000</span>)</span>
<span id="cb139-2"><a href="#cb139-2" aria-hidden="true"></a>T =<span class="st"> </span><span class="dv">100</span>  <span class="co"># number of time periods</span></span>
<span id="cb139-3"><a href="#cb139-3" aria-hidden="true"></a>nsim =<span class="st"> </span><span class="dv">1000</span>  <span class="co"># number of simulations</span></span>
<span id="cb139-4"><a href="#cb139-4" aria-hidden="true"></a>single_trajectory =<span class="st"> </span><span class="cf">function</span>(i) {</span>
<span id="cb139-5"><a href="#cb139-5" aria-hidden="true"></a>    path =<span class="st"> </span><span class="kw">numeric</span>(T)</span>
<span id="cb139-6"><a href="#cb139-6" aria-hidden="true"></a>    last =<span class="st"> </span>i</span>
<span id="cb139-7"><a href="#cb139-7" aria-hidden="true"></a>    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>T) {</span>
<span id="cb139-8"><a href="#cb139-8" aria-hidden="true"></a>        path[n] =<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>, <span class="dt">prob =</span> P[last, ], <span class="dt">size =</span> <span class="dv">1</span>)</span>
<span id="cb139-9"><a href="#cb139-9" aria-hidden="true"></a>        last =<span class="st"> </span>path[n]</span>
<span id="cb139-10"><a href="#cb139-10" aria-hidden="true"></a>    }</span>
<span id="cb139-11"><a href="#cb139-11" aria-hidden="true"></a>    <span class="kw">return</span>(path)</span>
<span id="cb139-12"><a href="#cb139-12" aria-hidden="true"></a>}</span>
<span id="cb139-13"><a href="#cb139-13" aria-hidden="true"></a>df =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">X0 =</span> <span class="dv">3</span>, <span class="kw">t</span>(<span class="kw">replicate</span>(nsim, <span class="kw">single_trajectory</span>(<span class="dv">3</span>))))</span>
<span id="cb139-14"><a href="#cb139-14" aria-hidden="true"></a></span>
<span id="cb139-15"><a href="#cb139-15" aria-hidden="true"></a>(<span class="dt">p_shocked =</span> <span class="kw">mean</span>(df<span class="op">$</span>X100 <span class="op">==</span><span class="st"> </span><span class="dv">7</span>))</span>
<span id="cb139-16"><a href="#cb139-16" aria-hidden="true"></a><span class="co">## [1] 0.581</span></span></code></pre></div>
<p>So, the probability of being shocked first is about <span class="math inline">\(0.58\)</span>. To be honest, what we computed up here is not <span class="math inline">\({\mathbb{P}}[X_{\tau_{S,F}} = S]\)</span>, as the problem required, but the probability <span class="math inline">\({\mathbb{P}}[ X_{100} = S]\)</span>. In general, these are not the same, but because both <span class="math inline">\(S\)</span> and <span class="math inline">\(F\)</span> are absorbing states, the events <span class="math inline">\(X_{100}=S\)</span> and <span class="math inline">\(X_{\tau_{S,F}} = S\)</span> differ only on the event where <span class="math inline">\(\tau_{F,S}&gt;100\)</span>, i.e., when Basil has not been either shocked or fed after <span class="math inline">\(100\)</span> steps.</p>
<p>To see what kind of an error we are making, we can examine the empirical distribution of <span class="math inline">\(X_{100}\)</span> across our <span class="math inline">\(1000\)</span> samples:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true"></a><span class="kw">table</span>(df<span class="op">$</span>X100)</span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true"></a><span class="co">## </span></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true"></a><span class="co">##   6   7 </span></span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true"></a><span class="co">## 419 581</span></span></code></pre></div>
<p>and conclude that, on this particular set of simulations, <span class="math inline">\(\tau_{S,F}\leq 100\)</span>, so no error has been made at all. In general, approximations like this are very useful in cases where we can expect the probability of non-absorption within a given time interval to be negligible. On the other hand, if you examine a typical trajectory of <code>df</code>, you will see that most of the time it takes the value <span class="math inline">\(6\)</span> of <span class="math inline">\(7\)</span>, so a lot of the computational effort goes to waste. But don’t worry about such things in this course.</p>
<p><br></p>
<p>So, is this enough evidence to conclude that Basil is, in fact, a smart rat? On one hand,
the obtained probability <span class="math inline">\(0.58\)</span> is somewhat higher than Basil’s observed shock rate of
<span class="math inline">\(52\%\)</span>, but it is not clear just from those numbers are not due to
simple luck of the draw, and not Basil’s alleged intelligence. It is hard to tell
without doing any further statistical analysis.</p>
<p>For those of you who know a bit of statistics: one can apply the
binomial test (or, more precisely, its large-sample
approximation) to test against the null hypothesis that Basil is
stupid. Under the null, the number of times Basil will get shocked
in 100 experiments is binomial, with parameters <span class="math inline">\(n=100\)</span> and
<span class="math inline">\(p=0.581\)</span>. Its normal approximation is
<span class="math inline">\(N(np, \sqrt{np(1-p)}) = N(58.1, 4.934)\)</span>, so the <span class="math inline">\(z\)</span>-score of the observed value, i.e., <span class="math inline">\(52\)</span>,
is <span class="math inline">\(z = \tfrac{ 52 - 58.1}{ 4.934} = -1.236\)</span>.
The standard normal CDF at <span class="math inline">\(z=-1.236\)</span> is about <span class="math inline">\(0.11\)</span>, i.e., the
<span class="math inline">\(p\)</span>-value is about <span class="math inline">\(0.11\)</span>. That means
that a truly stupid rat would appear at least as smart
as Basil in about <span class="math inline">\(11\%\)</span> of experiments identical to the one described
above by chance alone.
This kind of evidence is usually not considered sufficient to
make a robust conclusion about Basil’s intelligence.</p>
</div>
<!--
  professor
  ------------------------------------------------
-->
<div class="problem">
<p>A math professor has <span class="math inline">\(4\)</span> umbrellas. He keeps some of them at home and some in
the office. Every morning, when he leaves home, he checks the weather and takes
an umbrella with him if it rains. In case all the umbrellas are in the office,
he gets wet. The same procedure is repeated in the afternoon when he leaves the
office to go home. The professor lives in a tropical region, so the chance of
rain in the afternoon is higher than in the morning; it is <span class="math inline">\(1/5\)</span> in the
afternoon and <span class="math inline">\(1/20\)</span> in the morning. Whether it rains of not is independent of
whether it rained the last time he checked.</p>
<p>On day <span class="math inline">\(0\)</span>, there are <span class="math inline">\(2\)</span> umbrellas at home, and <span class="math inline">\(2\)</span> in the office.</p>
<ol style="list-style-type: decimal">
<li><p>Construct a Markov chain that models the situation.</p></li>
<li><p>Use Monte Carlo to give an approximate answer the following questions:</p>
<ol style="list-style-type: lower-alpha">
<li>What is the expected number of trips the professor will manage before he gets wet?</li>
<li>What is the probability that the first time he gets wet it is on his way home from the office?</li>
</ol></li>
</ol>
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/02_Basic_Chains/professor_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->
<!--chapter:end:05-Markov-chains.Rmd-->
</div>
</div>
<div id="classification-of-states" class="section level1" number="6">
<h1 number="6"><span class="header-section-number">6</span> Classification of States</h1>
<div style="counter-reset: thechapter 6;">

</div>
<p>There will be a lot of definitions and some theory before we get to
examples. You might want to peek ahead as notions are being introduced;
it will help your understanding.</p>
<div id="the-communication-relation" class="section level2" number="6.1">
<h2 number="6.1"><span class="header-section-number">6.1</span> The Communication Relation</h2>
<p>Let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a Markov chain on the state space <span class="math inline">\(S\)</span>. For a given set
<span class="math inline">\(B\)</span> of states, define the <strong>(first) hitting time <span class="math inline">\(\tau_B\)</span></strong> (or <span class="math inline">\(\tau(B)\)</span> if subscripts are
impractical) <strong>of the set <span class="math inline">\(B\)</span></strong> as
<span class="math display">\[\begin{equation}
   \tau_B=\min \{ n\in{\mathbb{N}}_0\, : \, X_n\in B\}.
\end{equation}\]</span>
We know that <span class="math inline">\(\tau_B\)</span> is, in fact, a stopping time with
respect to <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>. When <span class="math inline">\(B\)</span> consists of only one element
, e.g. <span class="math inline">\(B=\{i\}\)</span>, we simply write <span class="math inline">\(\tau_{i}\)</span> for <span class="math inline">\(\tau_{\{i\}}\)</span>; <span class="math inline">\(\tau_{i}\)</span>
is the first time the Markov chain <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> “hits” the state <span class="math inline">\(i\)</span>. As
always, we allow <span class="math inline">\(\tau_{B}\)</span> to take the value <span class="math inline">\(\infty\)</span>; it means that
no state in <span class="math inline">\(B\)</span> is ever hit.</p>
<p>The hitting times are important both for applications, and for better
understanding of the structure of Markov chains in general.
For example, let <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be the chain which models a game of tennis (from the
previous lecture). The probability of winning for Player 1 can be
phrased in terms of hitting times: <span class="math display">\[{\mathbb{P}}[ \text{Player 1 wins}]={\mathbb{P}}[ 
\tau_{i_{1}}&lt;\tau_{i_{2}}],\]</span> where <span class="math inline">\(i_{1}=\)</span> “Player 1 wins” and <span class="math inline">\(i_{2}=\)</span>“Player 2
wins” (the two absorbing states of the chain). We will learn how to
compute such probabilities in the subsequent lectures.</p>
<p>Having introduced the hitting times <span class="math inline">\(\tau_B\)</span>, let us give a few more
definitions. It will be very convenient to consider the same Markov
chain with different initial distributions. Most often, these
distributions will correspond to starting from a fixed state (as opposed
to choosing the initial state at random). We use the notation <span class="math inline">\({\mathbb{P}}_i[A]\)</span>
to mean <span class="math inline">\({\mathbb{P}}[A|X_0=i]\)</span> (for any event <span class="math inline">\(A\)</span>), and <span class="math inline">\({\mathbb{E}}_i[A]={\mathbb{E}}[A|X_0=i]\)</span>
(for any random variable <span class="math inline">\(X\)</span>). In practice, we use <span class="math inline">\({\mathbb{P}}_i\)</span> and <span class="math inline">\({\mathbb{E}}_i\)</span>
to signify that we are starting the chain from the state <span class="math inline">\(i\)</span>, i.e.,
<span class="math inline">\({\mathbb{P}}_i\)</span> corresponds to a Markov chain whose transition matrix is the
same as the one of <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>, but the initial distribution is given by
<span class="math inline">\({\mathbb{P}}_i[X_0=j]=0\)</span> if <span class="math inline">\(j\not = i\)</span> and <span class="math inline">\({\mathbb{P}}_i[X_0=i]=1\)</span>. Note also that
<span class="math inline">\({\mathbb{P}}_i[X_1=j] = p_{ij}\)</span> and that <span class="math inline">\({\mathbb{P}}_i[X_n=j] =p^{(n)}_{ij}\)</span>, for any <span class="math inline">\(n\)</span>.</p>
<p>A state <span class="math inline">\(i\in S\)</span> is said to <strong>communicate</strong> with the state <span class="math inline">\(j\in S\)</span>,
denoted by <span class="math inline">\(i\to j\)</span> if <span class="math display">\[{\mathbb{P}}_i[\tau_{j}&lt;\infty]&gt;0.\]</span></p>
<p>Intuitively, <span class="math inline">\(i\)</span> communicates with <span class="math inline">\(j\)</span> if there is a non-zero chance
that the Markov chain <span class="math inline">\(X\)</span> will eventually visit <span class="math inline">\(j\)</span> if it starts from
<span class="math inline">\(i\)</span>. Sometimes we also say that <span class="math inline">\(j\)</span> is <strong>a consequent of</strong> <span class="math inline">\(i\)</span>, that <span class="math inline">\(j\)</span>
<strong>is accessible from</strong> <span class="math inline">\(i\)</span>, or that <span class="math inline">\(j\)</span> <strong>follows</strong> <span class="math inline">\(i\)</span>.</p>
<p>In the “tennis” example of the previous chapter,
every state is accessible from <span class="math inline">\((0,0)\)</span> (the fact
that <span class="math inline">\(p\in (0,1)\)</span> is important here), but <span class="math inline">\((0,0)\)</span> is not accessible from
any other state. The consequents of <span class="math inline">\((0,0)\)</span> are not only <span class="math inline">\((15,0)\)</span> and
<span class="math inline">\((0,15)\)</span>, but also <span class="math inline">\((30,15)\)</span> or <span class="math inline">\((40,40)\)</span>. In fact, all states
are consequents of <span class="math inline">\((0,0)\)</span>. The consequents of <span class="math inline">\((40,40)\)</span> are <span class="math inline">\((40,40)\)</span> itself, <span class="math inline">\((40,Adv)\)</span>,
<span class="math inline">\((Adv, 40)\)</span>, “P1 wins” and “P2 wins”.</p>
<div class="problem">
<p>Explain why
<span class="math inline">\(i \to j\)</span> if and only if <span class="math inline">\(p^{(n)}_{ij}&gt;0\)</span> for some <span class="math inline">\(n\in{\mathbb{N}}_0\)</span>.</p>
</div>
<div class="solution">
<p>Leaving a rigorous mathematical proof aside, we note that the statement
is intuitively easy to understand. If <span class="math inline">\(i\to j\)</span> then there must exist
some time <span class="math inline">\(n\)</span> such that <span class="math inline">\({\mathbb{P}}_i[\tau_j = n]&gt;0\)</span>. This, in turn, implies
that it is possible to go from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> in exactly <span class="math inline">\(n\)</span> steps, where
“possible” means “with positive probability”. In our notation, that is
exactly what <span class="math inline">\(p^{(n)}_{ij}&gt;0\)</span> means.</p>
<p>Conversely, if <span class="math inline">\(p^{(n)}_{ij}&gt;0\)</span> then
<span class="math inline">\({\mathbb{P}}_i[ \tau_j &lt;\infty] \geq {\mathbb{P}}_i[\tau_j \leq n] \geq {\mathbb{P}}_i[ X_n = j]=p^{(n)}_{ij}&gt;0.\)</span></p>
</div>
<p>Two immediate properties of the relation <span class="math inline">\(\to\)</span> are listed in the problem below:</p>
<div class="problem">
<p>Explain why the following statements are true for all states <span class="math inline">\(i,j,k\)</span> of a Markov chain.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(i\to i\)</span>,</p></li>
<li><p><span class="math inline">\(i\to j, j\to k\)</span> implies <span class="math inline">\(i \to k\)</span>.</p></li>
</ol>
</div>
<div class="solution">
<ol style="list-style-type: decimal">
<li><p>If we start from state <span class="math inline">\(i\in S\)</span> we are already there! More rigorously, note that <span class="math inline">\(0\)</span>
is allowed as a value for <span class="math inline">\(\tau_{B}\)</span> in its definition above, i.e., <span class="math inline">\(\tau_i=0\)</span> when <span class="math inline">\(X_0=i\)</span>.</p></li>
<li><p>Intuitively, if you can follow a path (sequence of arrows) from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span>, and then another path <span class="math inline">\(j\)</span> to <span class="math inline">\(k\)</span>,
you can do the same from <span class="math inline">\(i\)</span> to <span class="math inline">\(k\)</span> by concatenating two paths. More rigorously, by the previous problem,
it will be enough to show that <span class="math inline">\(p^{(n)}_{ik}&gt;0\)</span> for some <span class="math inline">\(n\in{\mathbb{N}}\)</span>. By the same
Proposition, we know that <span class="math inline">\(p^{(n_1)}_{ij}&gt;0\)</span> and <span class="math inline">\(p^{(n_2)}_{jk}&gt;0\)</span>
for some <span class="math inline">\(n_1,n_2\in{\mathbb{N}}_0\)</span>. By the Chapman-Kolmogorov relations, with
<span class="math inline">\(n=n_1+n_2\)</span>, we have
<span class="math display">\[\begin{equation}
  p^{(n)}_{ik} =\sum_{l\in S} p^{(n_1)}_{il} p^{(n_2)}_{lk}\geq  
  p^{(n_1)}_{ij} p^{(n_2)}_{jk}&gt;0.
\end{equation}\]</span>
Note that the inequality <span class="math inline">\(p^{(n)}_{ik}\geq p^{(n_1)}_{il}p^{(n_2)}_{lk}\)</span> is valid for
all <span class="math inline">\(i,l,k\in S\)</span>, as long as <span class="math inline">\(n_1+n_2=n\)</span>. It will come in handy later.</p></li>
</ol>
</div>
<p>Remember that the <strong>greatest common divisor (gcd)</strong> of a set <span class="math inline">\(A\)</span> of
natural numbers if the largest number <span class="math inline">\(d\in{\mathbb{N}}\)</span> such that <span class="math inline">\(d\)</span> divides
each <span class="math inline">\(k\in A\)</span>, i.e., such that each <span class="math inline">\(k\in A\)</span> is of the form <span class="math inline">\(k=l d\)</span> for
some <span class="math inline">\(l\in{\mathbb{N}}\)</span>.</p>
<p>A <strong>period</strong> <span class="math inline">\(d(i)\)</span> of a state <span class="math inline">\(i\in S\)</span> is the greatest common
divisor of the <strong>return set</strong> <span class="math display">\[R(i)= \{ n\in{\mathbb{N}}\, : \,  p^{(n)}_{ii}&gt;0\}\]</span>
of the state <span class="math inline">\(i\)</span>. When <span class="math inline">\(R(i)=\emptyset\)</span>, we set <span class="math inline">\(d(i)=1\)</span>. A state
<span class="math inline">\(i\in S\)</span> is called <strong>aperiodic</strong> if <span class="math inline">\(d(i)=1\)</span>.</p>
<div class="problem">
<p>Consider two Markov chains with three states and the transition matrices
<span class="math display">\[P_1=\begin{bmatrix}
 0 &amp; 1 &amp; 0 \\
 0 &amp; 0 &amp; 1 \\
 1 &amp; 0 &amp; 0 
\end{bmatrix}, \quad
P_2=\begin{bmatrix}
 0 &amp; 1 &amp; 0 \\
 0 &amp; 0 &amp; 1 \\
 \tfrac{1}{2} &amp; 0 &amp; \tfrac{1}{2} 
\end{bmatrix}\]</span></p>
<p>Find return sets and periods of each state <span class="math inline">\(i\)</span> of each chain.</p>
</div>
<div class="solution">
<p>For the first chain, with transition graph</p>
<p><img src="_main_files/figure-html/unnamed-chunk-226-1.png" width="672" style="margin-top:-10%; margin-bottom: -10%" style="display: block; margin: auto;" /></p>
<p>the return set for each state <span class="math inline">\(i\in\{1,2,3\}\)</span> is
given by <span class="math inline">\(R(i)= \{3,6,9,12,\dots\}\)</span>, so <span class="math inline">\(d(i)=3\)</span> for all
<span class="math inline">\(i\in\{1,2,3\}\)</span>.</p>
<p>Even though the transition graph of the second chain looks very similar to the first one</p>
<p><img src="_main_files/figure-html/unnamed-chunk-227-1.png" width="672" style="margin-top:-10%; margin-bottom: -10%" style="display: block; margin: auto;" /></p>
<p>the situation changes drastically:
<span class="math display">\[\begin{align}
  R(1) &amp; =\{ 3,4,5,6, \dots \},\\
  R(2) &amp; =\{ 2,3,4,5,6, \dots \},\\
  R(3) &amp; =\{ 1,2,3,4,5,6, \dots \},
\end{align}\]</span>
so that <span class="math inline">\(d(i)=1\)</span> for <span class="math inline">\(i\in\{1,2,3\}\)</span>.</p>
</div>
</div>
<div id="classes" class="section level2" number="6.2">
<h2 number="6.2"><span class="header-section-number">6.2</span> Classes</h2>
<p>We say that the states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in <span class="math inline">\(S\)</span> <strong>intercommunicate</strong>, denoted
by <span class="math inline">\(i\leftrightarrow j\)</span> if <span class="math inline">\(i\to  j\)</span> <em>and</em> <span class="math inline">\(j\to i\)</span>. A set <span class="math inline">\(B\subseteq S\)</span> of states is called
<strong>irreducible</strong> if <span class="math inline">\(i\leftrightarrow j\)</span> for all <span class="math inline">\(i,j\in S\)</span>.</p>
<p>Unlike the relation of communication, the relation of intercommunication
is symmetric. Moreover, we have the following immediate property:
the relation <span class="math inline">\(\leftrightarrow\)</span> is an <em>equivalence relation</em> on <span class="math inline">\(S\)</span>, i.e., for all
<span class="math inline">\(i,j,k\in S\)</span>, we have</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(i\leftrightarrow i\)</span> (<em>reflexivity</em>) ,</p></li>
<li><p><span class="math inline">\(i\leftrightarrow j\)</span> implies <span class="math inline">\(j\leftrightarrow i\)</span> (<em>symmetry</em>), and</p></li>
<li><p><span class="math inline">\(i\leftrightarrow j, j\leftrightarrow k\)</span> implies <span class="math inline">\(i\leftrightarrow k\)</span> (<em>transitivity</em>).</p></li>
</ol>
<p>The fact that <span class="math inline">\(\leftrightarrow\)</span> is an equivalence relation allows us to split the
state-space <span class="math inline">\(S\)</span> into equivalence classes with respect to <span class="math inline">\(\leftrightarrow\)</span>. In
other words, we can write <span class="math display">\[S=S_1\cup S_2\cup S_3\cup \dots,\]</span> where
<span class="math inline">\(S_1, S_2, \dots\)</span> are mutually exclusive (disjoint) and all states in a
particular <span class="math inline">\(S_n\)</span> intercommunicate, while no two states from different
equivalence classes <span class="math inline">\(S_n\)</span> and <span class="math inline">\(S_m\)</span> do. The sets <span class="math inline">\(S_1, S_2, \dots\)</span> are
called <strong>classes</strong> of the chain <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span>. Equivalently, one can say
that classes are <em>maximal irreducible sets</em>, in the sense that they are
irreducible and no class is a subset of a (strictly larger) irreducible
set. A cookbook algorithm for class identification would involve the
following steps:</p>
<ol style="list-style-type: decimal">
<li><p>Start from an arbitrary state (call it <span class="math inline">\(1\)</span>).</p></li>
<li><p>Identify <em>all</em> states <span class="math inline">\(j\)</span> that intercommunicate with it (<span class="math inline">\(1\)</span>,
itself, always does).</p></li>
<li><p>That is your first class, call it <span class="math inline">\(C_1\)</span>. If there are no elements
left, then there is only one class <span class="math inline">\(C_1=S\)</span>. If there is an element
in <span class="math inline">\(S\setminus C_1\)</span>, repeat the procedure above starting from that
element.</p></li>
</ol>
<p>The notion of a class is especially useful in relation to another
natural concept: A set <span class="math inline">\(B\subseteq S\)</span> of states is said to be <strong>closed</strong> if <span class="math inline">\(i  \not\to j\)</span> for all <span class="math inline">\(i\in B\)</span> and all <span class="math inline">\(j\in S\setminus B\)</span>. In words, <span class="math inline">\(B\)</span> is closed if it is
impossible to get out of. A state
<span class="math inline">\(i\in S\)</span> such that the set <span class="math inline">\(\{i\}\)</span> is closed is called <strong>absorbing</strong>.</p>
<div class="problem">
<p>Show that a set <span class="math inline">\(B\)</span> of
states is closed if and only if <span class="math inline">\(p_{ij}=0\)</span> for all <span class="math inline">\(i\in B\)</span> and all
<span class="math inline">\(j\in B^c=S\setminus B\)</span>.</p>
</div>
<div class="solution">
<p>Suppose, first, that <span class="math inline">\(B\)</span> is closed. Then for <span class="math inline">\(i\in B\)</span> and <span class="math inline">\(j\in  B^c\)</span>, we have <span class="math inline">\(i\not\to j\)</span>, i.e., <span class="math inline">\(p^{(n)}_{ij}=0\)</span> for all <span class="math inline">\(n\in{\mathbb{N}}\)</span>. In
particular, <span class="math inline">\(p_{ij}=0\)</span>.</p>
<p>Conversely, suppose that <span class="math inline">\(p_{ij}=0\)</span> for all <span class="math inline">\(i\in B\)</span>, <span class="math inline">\(j\in B^c\)</span>. We
need to show that <span class="math inline">\(k\not\to l\)</span> (i.e. <span class="math inline">\(p^{(n)}_{kl}=0\)</span> for all <span class="math inline">\(n\in{\mathbb{N}}\)</span>) for
all <span class="math inline">\(k\in B\)</span>, <span class="math inline">\(l\in B^c\)</span>. Suppose, to the contrary, that there exist
<span class="math inline">\(k\in B\)</span> and <span class="math inline">\(l\in B^c\)</span> such that <span class="math inline">\(p^{(n)}_{kl}&gt;0\)</span> for some <span class="math inline">\(n\in {\mathbb{N}}\)</span>. That means that we can find a sequence of states
<span class="math display">\[k=i_0, i_1, \dots, i_n=l \text{ such that } p_{i_{m-1} i_{m}}&gt;0
\text{ forall }m = 1,\dots, n.\]</span> The first state, <span class="math inline">\(k=i_0\)</span> is in <span class="math inline">\(B\)</span> and the
last one, <span class="math inline">\(l=i_n\)</span>, is in <span class="math inline">\(B^c\)</span>. Therefore there must exist an index <span class="math inline">\(m\)</span>
such that <span class="math inline">\(i_{m-1}\in B\)</span> but <span class="math inline">\(i_{m}\in B^c\)</span>. We also know that
<span class="math inline">\(p_{i_m i_{m+1}}&gt;0\)</span>, which is in contradiction with out assumption that
<span class="math inline">\(p_{ij}=0\)</span> for all <span class="math inline">\(i\in B\)</span> and <span class="math inline">\(j\in B^c\)</span>.</p>
</div>
<p>Intuitively, a set of states is closed if it has the property that the
chain <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> stays in it forever, once it enters it. In general, if
<span class="math inline">\(B\)</span> is closed, it does not have to follow that <span class="math inline">\(S\setminus B\)</span> is closed.
Also, a class does not have to be closed, and a closed set does not have
to be a class. Here is an example - consider
the following three sets of states in
the <em>tennis</em> chain of the previous lecture and:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(B=\{\text{``P1 wins&#39;&#39;}\}\)</span>: closed and a class, but
<span class="math inline">\(S\setminus B\)</span> is not closed</p></li>
<li><p><span class="math inline">\(B=S\setminus \{(0,0)\}\)</span>: closed, but not a class, and</p></li>
<li><p><span class="math inline">\(B=\{(0,0)\}\)</span>: class, but not closed.</p></li>
</ol>
<p>Not everything is lost as the following relationship always holds:</p>
<div class="problem">
<p>Show that every closed set <span class="math inline">\(B\)</span> is a union of one or more classes.</p>
</div>
<div class="solution">
<p>Let <span class="math inline">\(\hat{B}\)</span> be the union of all classes <span class="math inline">\(C\)</span> such that <span class="math inline">\(C\cap  B\not=\emptyset\)</span>. In other words, take all the elements of <span class="math inline">\(B\)</span> and
throw in all the states which intercommunicate with at least one of them. I claim that
<span class="math inline">\(\hat{B}=B\)</span>. Clearly, <span class="math inline">\(B\subset \hat{B}\)</span>, so we need to show that
<span class="math inline">\(\hat{B}\subseteq B\)</span>. Suppose, to the contrary, that there exists
<span class="math inline">\(j\in \hat{B}\setminus B\)</span>. By construction, <span class="math inline">\(j\)</span> intercommunicates with
some <span class="math inline">\(i\in B\)</span>. In particular <span class="math inline">\(i\to j\)</span>. By the closedness of <span class="math inline">\(B\)</span>, we must
have <span class="math inline">\(j\in B\)</span>. This is a contradiction with the assumptions that
<span class="math inline">\(j\in \hat{B}\setminus B\)</span>.</p>
<p>Note that the converse is not true: ust take the set
<span class="math inline">\(B=\{ (0,0), (0,15)\}\)</span> in the “tennis” example. It is a union of two
classes, but it is not closed.</p>
</div>
</div>
<div id="transience-and-recurrence" class="section level2" number="6.3">
<h2 number="6.3"><span class="header-section-number">6.3</span> Transience and recurrence</h2>
<p>It is often important to know whether a Markov chain will ever return to
its initial state, and if so, how often. The notions of transience and
recurrence are used to address this questions.</p>
<p>We start by introducing a cousin <span class="math inline">\(T_j(1)\)</span> of the first hitting time <span class="math inline">\(\tau_1\)</span>.
The <strong>(first) visit time</strong> to state <span class="math inline">\(j\)</span>, denoted by <span class="math inline">\(T_j(1)\)</span> is defined
as <span class="math display">\[T_j(1) = \min \{ n\in{\mathbb{N}}\, : \, X_n=j\}.\]</span> As usual <span class="math inline">\(T_j(1)=\infty\)</span> if
<span class="math inline">\(X_n\not = j\)</span> for all <span class="math inline">\(n\in{\mathbb{N}}\)</span>.
Similarly, second, third, etc., visit times are defined as follows:
<span class="math display">\[\begin{aligned}
  T_j(2) &amp;= \min \{ n&gt;T_j(1)\, : \, X_n=j\}, \\
  T_j(3) &amp;= \min \{ n&gt;T_j(2)\, : \, X_n=j\}, \text{ etc., }\end{aligned}\]</span>
with the understanding that if <span class="math inline">\(T_j(n)=\infty\)</span>, then also
<span class="math inline">\(T_j(m)=\infty\)</span> for all <span class="math inline">\(m&gt;n\)</span>.</p>
<p>Note that the definition of the random variable <span class="math inline">\(T_j(1)\)</span> differs from
the definition of <span class="math inline">\(\tau_j\)</span> in that the minimum here is taken over the
set <span class="math inline">\({\mathbb{N}}\)</span> of natural numbers, while the set of non-negative integers
<span class="math inline">\({\mathbb{N}}_0\)</span> is used for <span class="math inline">\(\tau_j\)</span>. When <span class="math inline">\(X_0\not = j\)</span>, the hitting time
<span class="math inline">\(\tau_j\)</span> and the first visit time <span class="math inline">\(T_j(1)\)</span> coincide. The important
difference occurs only when <span class="math inline">\(X_0=j\)</span>. In that case <span class="math inline">\(\tau_j=0\)</span> (we are
already there), but it is always true that <span class="math inline">\(T_j(1)\geq 1\)</span>. It can even
happen that <span class="math inline">\({\mathbb{P}}_j[T_j(1)=\infty]=1\)</span>. If you want an example, take any state in the
deterministically monotone chain.</p>
<p>A state <span class="math inline">\(i\in S\)</span> is said to be</p>
<ol style="list-style-type: decimal">
<li><p><strong>recurrent</strong> if <span class="math inline">\({\mathbb{P}}_i[T_i(1)&lt;\infty]=1\)</span>,</p></li>
<li><p><strong>positive recurrent</strong> if <span class="math inline">\({\mathbb{E}}_i[T_i(1)]&lt;\infty\)</span></p></li>
<li><p><strong>null recurrent</strong> if it is recurrent, but not positive recurrent,</p></li>
<li><p><strong>transient</strong> if it is not recurrent.</p></li>
</ol>
<p>A state is recurrent if we are sure we will come back to it eventually
(with probability 1). It is positive recurrent if it is recurrent and
the time between two consecutive visits has finite expectation. Null
recurrence means the we will return, but the waiting time may be very
long. A state is transient if there is a positive chance (however small)
that the chain will never return to it.</p>
<div id="the-return-theorem" class="section level3" number="6.3.1">
<h3 number="6.3.1"><span class="header-section-number">6.3.1</span> The Return Theorem</h3>
<p>The definition of recurrence from above is conceptually simple, but it
gives us no clue about how to actually go about deciding whether a
particular state in a specific Markov chain is recurrent. A criterion
stated entirely in terms of the transition matrix <span class="math inline">\(P\)</span> would be nice.
Before we give it, we need to introduce some notation. and prove an important theorem.
Given a state
<span class="math inline">\(i\)</span>, let <span class="math inline">\(f_i\)</span> denote the probability that the chain will visit <span class="math inline">\(i\)</span>
again, if it starts there, i.e., <span class="math display">\[f_i = {\mathbb{P}}_i[ T_i(1) &lt; \infty].\]</span>
Clearly, <span class="math inline">\(i\)</span> is recurrent if and only if <span class="math inline">\(f_i=1\)</span>.</p>
<p>The interesting thing is that every time our chain visits the state <span class="math inline">\(i\)</span>,
its future evolution is independent from the past (except for the name
of the current state) and it behaves exactly like a new and independent
chain started from <span class="math inline">\(i\)</span> would. This is a special case of so-called
<strong>strong Markov property</strong> which states that the (usual) Markov property
also holds at stopping times (and not only fixed times <span class="math inline">\(n\)</span>). We will not
prove this property it these notes, but we will gladly use it to prove
the following dichotomy:</p>

<div class="theorem">
<p><span id="thm:unnamed-chunk-228" class="theorem"><strong>(#thm:unnamed-chunk-228) </strong></span>(The “Return” Theorem)
Let
<span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> be a Markov chain on a countable state space <span class="math inline">\(S\)</span>, with the
(deterministic) initial state <span class="math inline">\(X_0=i\)</span>. Then exactly one of the following
two statements hold:</p>
<ol style="list-style-type: decimal">
<li><p>either the chain will return to <span class="math inline">\(i\)</span> infinitely many times, or</p></li>
<li><p>the chain will return to <span class="math inline">\(i\)</span> a finite number <span class="math inline">\(N_i\)</span> of times, where
<span class="math inline">\(N_i\)</span> is geometrically distributed random variable with parameter <span class="math inline">\(f_i\)</span>, where
<span class="math inline">\(f_i={\mathbb{P}}_i[T_i(1)&lt;\infty]\)</span>.</p></li>
</ol>
In the first case, <span class="math inline">\(i\)</span> is recurrent and, in the second, it is transient.
</div>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> If <span class="math inline">\(f_i=1\)</span>, then <span class="math inline">\(X\)</span> is guaranteed to return to <span class="math inline">\(i\)</span> at least once. When
that happens, however, the strong Markov property “deletes” the past,
and the process “renews” itself. This puts us back in the original
situation where we are looking at a chain which starts at <span class="math inline">\(i\)</span> and is
guaranteed to return there at least once. Continuing like that, we get a
whole infinite sequence of stopping times <span class="math display">\[T_i(1) &lt; T_i(2) &lt; \dots\]</span> at
which <span class="math inline">\(X\)</span> finds itself at <span class="math inline">\(i\)</span>.</p>
If <span class="math inline">\(f_i&lt;1\)</span>, a similar story can be told, but with a significant
difference. Every time <span class="math inline">\(X\)</span> returns to <span class="math inline">\(i\)</span>, there is a probability
<span class="math inline">\(1-f_i\)</span> that it will never come back to <span class="math inline">\(i\)</span>, and, this is independent of
the past behavior. If we think of the return to <span class="math inline">\(i\)</span> as a success, the
number of successes before the first failure, i.e., the number of return
visits to <span class="math inline">\(i\)</span>, is nothing but a geometrically distributed random
variable with parameter <span class="math inline">\(f_i\)</span>. Q.E.D.
</div>
<p><br></p>
<p>The following interesting fact follows (almost) directly from the Return Theorem:</p>
<div class="problem">
<p>Suppose that the state space <span class="math inline">\(S\)</span> is finite. Show that there exists at least
one recurrent state.</p>
</div>
<div class="solution">
<p>We argue by contradiction and assume that all the states are transient.
We claim that, in that case, the total number of visits <span class="math inline">\(N_i\)</span> to each
state <span class="math inline">\(i\)</span> is always finite, no matter what state <span class="math inline">\(i_0\)</span> we start from.
Indeed, if <span class="math inline">\(i=i_0\)</span> that is precisely the
conclusion the Return Theorem above. For a state <span class="math inline">\(i\ne i_0\)</span>, the number of
visits is either <span class="math inline">\(0\)</span> - if we never even get to <span class="math inline">\(i\)</span>, or <span class="math inline">\(1+N_{i}\)</span> if we
do. In either case, it is a finite number (not <span class="math inline">\(\infty\)</span>).</p>
<p>Therefore the sum <span class="math inline">\(\sum_{i\in S} N_i\)</span> is also finite - a contradiction
with the fact that there are infinitely many time instances <span class="math inline">\(n\in{\mathbb{N}}_0\)</span>,
and that the chain must be in some state in each one of them.</p>
</div>
<p>If <span class="math inline">\(S\)</span> is not finite, it is not true that recurrent states must exist.
Just think of the Deterministically-Monotone Chain or
the random walk with <span class="math inline">\(p\not=\tfrac{1}{2}\)</span>. All states are transitive there.</p>
</div>
<div id="a-recurrence-criterion" class="section level3" number="6.3.2">
<h3 number="6.3.2"><span class="header-section-number">6.3.2</span> A recurrence criterion</h3>
<p>Perhaps the most important consequence of the Return Theorem is the following
criterion for recurrence of Markov chains on finite or countable state spaces:</p>

<div class="theorem">
<span id="thm:unnamed-chunk-230" class="theorem"><strong>(#thm:unnamed-chunk-230) </strong></span>(The Recurrence Criterion)
A state <span class="math inline">\(i\in S\)</span>
is recurrent if and only if <span class="math display">\[\sum_{n\in{\mathbb{N}}} p^{(n)}_{ii}=\infty.\]</span>
</div>

<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Let <span class="math inline">\(N_i\)</span> denote the total number (finite or <span class="math inline">\(\infty\)</span>) of visits to the state <span class="math inline">\(i\)</span>, with the initial visit at time <span class="math inline">\(0\)</span> not counted.
We can write <span class="math inline">\(N_i\)</span> as an infinite sum as follows
<span class="math display">\[N_i = \sum_{n=1}^{\infty} \mathbf{1}_{\{X_n = i\}}.\]</span>
Taking the
expectation yields
<span class="math display">\[{\mathbb{E}}[N_i] = {\mathbb{E}}_i[ \sum_{n=1}^{\infty} \mathbf{1}_{\{X_n=i\}}] = \sum_{n=1}^{\infty} {\mathbb{E}}_i[ \mathbf{1}_{\{X_n=i\}}] = \sum_{n=1}^{\infty} {\mathbb{P}}_i[
 X_n=i] = \sum_{n=1}^{\infty} p^{(n)}_{ii},\]</span> where we used the intuitively acceptable (but not rigorously proven)
fact that <span class="math inline">\({\mathbb{E}}_i\)</span> and an <em>infinite</em> sum can be switched.</p>
If <span class="math inline">\(i\)</span> is transient, i.e., if
<span class="math inline">\(f_i&lt;1\)</span>, the Return Theorem and the formula for the expected value of a geometric distribution imply that
<span class="math display">\[{\mathbb{E}}_i[N_i] = \frac{f_i}{1-f_i}&lt;\infty, \text{ and so }
 \sum_{n=1}^{\infty} p^{(n)}_{ii} = {\mathbb{E}}_i[N_i]&lt;\infty.\]</span> On the other
hand, if <span class="math inline">\(i\)</span> is recurrent, the Return Theorem states that <span class="math inline">\(N_i=\infty\)</span>. Hence,
<span class="math display">\[\sum_{n=1}^{\infty} p^{(n)}_{ii}={\mathbb{E}}_i[N_i]=\infty. \text{ Q.E.D. }\]</span>
</div>
<p><em>Remark.</em> The central idea behind the proof of the recurrence criterion is the following: we managed tell
whether or not <span class="math inline">\(N_i = \infty\)</span> by checking whether <span class="math inline">\({\mathbb{E}}[N_i]=\infty\)</span> or not.
This is, however, not something that can be done for any old random variable taking values in <span class="math inline">\({\mathbb{N}}_0 \cup \{\infty\}\)</span>.
If <span class="math inline">\({\mathbb{E}}[N]&lt;\infty\)</span>, then, clearly <span class="math inline">\({\mathbb{P}}[N=\infty]=0\)</span> so that <span class="math inline">\(N\)</span> only
takes values in <span class="math inline">\({\mathbb{N}}_0\)</span>. On the other hand, it is not true that
<span class="math inline">\({\mathbb{P}}[N=\infty]=0\)</span> implies that <span class="math inline">\({\mathbb{E}}[N]&lt;\infty\)</span>. It suffices to take a
random variable with the following distribution
<span class="math display">\[{\mathbb{P}}[ N = n] = c/n^2 \text{ for }n\in{\mathbb{N}},\]</span> where the constant <span class="math inline">\(c\)</span> is chosen
so that <span class="math inline">\(\sum_n c/n^2 =1\)</span> (in fact, we can compute that <span class="math inline">\(c=6/\pi^2\)</span>
explicitly in this case). The expected value of <span class="math inline">\(N\)</span> is given by
<span class="math display">\[{\mathbb{E}}[N] = \sum_{n=1}^{\infty} n {\mathbb{P}}[N=n] = c \sum_{n=1}^{\infty} \frac{1}{n}
  = \infty.\]</span> The message is that, in general, you cannot detect
whether something happened infinitely many times or not based only on
its expectation.</p>
<p>Such a detection, however, becomes possible in the
special case when <span class="math inline">\(N=N_i\)</span> denotes the total number of returns to the
state <span class="math inline">\(i\)</span> of a Markov chain. This is exactly the content of proof of
the Return Theorem above: each time the chain leaves <span class="math inline">\(i\)</span>, it
comes back to it (or does not) with the same probability, independently
of the past. This gives us extra information about the random variable
<span class="math inline">\(N\)</span> (namely that it is either infinite with probability <span class="math inline">\(1\)</span> or
geometrically distributed) and allows us to test its finiteness by using
the expected value only.</p>
</div>
<div id="polyas-theorem" class="section level3" number="6.3.3">
<h3 number="6.3.3"><span class="header-section-number">6.3.3</span> Polya’s theorem</h3>
<p>Here is an application of our recurrence criterion - a beautiful and
unexpected result of George Pólya from 1921.</p>
<p>In addition to the simple symmetric random walk on the line (<span class="math inline">\(d=1\)</span>) we
studied before, one can consider random walks whose values are in the
plane (<span class="math inline">\(d=2\)</span>), the space (<span class="math inline">\(d=3\)</span>), etc. These are usually defined as
follows: the random walk in <span class="math inline">\(d\)</span> dimensions is the Markov chain with the
state space <span class="math inline">\(S={\mathbb{Z}}^d\)</span> and the following transitions:<br />
starting from the state <span class="math inline">\((x_1,\dots, x_d)\)</span>, it
picks one of its <span class="math inline">\(2d\)</span> neighbors <span class="math inline">\((x_1+1,\dots, x_d)\)</span>,
<span class="math inline">\((x_1-1,\dots, x_d)\)</span>, <span class="math inline">\((x_1, x_2+1,\dots, x_d)\)</span>,
<span class="math inline">\((x_1, x_2-1,\dots, x_d)\)</span>, …, <span class="math inline">\((x_1,\dots, x_d+1)\)</span>, <span class="math inline">\((x_1,\dots, x_d-1)\)</span> randomly and uniformly and moves there. For
illustration, here is a picture of a path of a two-dimensional random walk; as time progresses, the color of the edges goes from black to orange, edges traversed
multiple times are darker, dots mark the position of the walk at time <span class="math inline">\(n=0\)</span> (the black round dot) and at time <span class="math inline">\(n=1000\)</span> (orange square dot):</p>
<center>
<img src="_main_files/figure-html/unnamed-chunk-232-1.png" width="130%" style="display: block; margin: auto;" />
</center>
<p>Polya’s (and our) goal was to study the recurrence properties of the
<span class="math inline">\(d\)</span>-dimensional random walk. We already
know that the simple symmetric random walk on <span class="math inline">\({\mathbb{Z}}\)</span> is recurrent (i.e.,
every <span class="math inline">\(i\in {\mathbb{Z}}\)</span> is a recurrent state). The easiest way to proceed when
<span class="math inline">\(d\geq 2\)</span> is to use the recurrence criterion we proved above.
We start by estimating the values <span class="math inline">\(p^{(n)}_{ii}\)</span>, for
<span class="math inline">\(n\in{\mathbb{N}}\)</span>. By symmetry, we can focus on the origin, i.e., it is enough to
estimate, for each <span class="math inline">\(n\in{\mathbb{N}}\)</span>, the magnitude of
<span class="math display">\[p^{(n)}= p^{(n)}_{00}= {\mathbb{P}}_{0}[ X_n=(0,0,\dots, 0)].\]</span> As we learned some time ago,
this probability can be computed by counting all “trajectories” from <span class="math inline">\((0,\dots, 0)\)</span>
that return to <span class="math inline">\((0,\dots, 0)\)</span> in <span class="math inline">\(n\)</span> steps. First of all, it is clear that <span class="math inline">\(n\)</span> needs to
be even, i.e., <span class="math inline">\(n=2m\)</span>, for some <span class="math inline">\(m\in{\mathbb{N}}\)</span>. It helps if we think of any
trajectory as a sequence of “increments” <span class="math inline">\(\xi_1,\dots, \xi_n\)</span>, where
each <span class="math inline">\(\xi_i\)</span> takes its value in the set <span class="math inline">\(\{1,-1,2,-2,\dots, d, -d\}\)</span>.
In words, <span class="math inline">\(\xi_i= +k\)</span> if the <span class="math inline">\(k\)</span>-th coordinate increases by <span class="math inline">\(1\)</span> on the
<span class="math inline">\(i\)</span>-th step, and <span class="math inline">\(\xi_i=-k\)</span>, if the <span class="math inline">\(k\)</span>-th coordinate decreases<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p>This way, the problem becomes combinatorial:</p>
<p><em>In how many ways can we put one element of the set
<span class="math inline">\(\{1,-1,2,-2, \dots, d,-d\}\)</span> into each of <span class="math inline">\(n=2m\)</span> boxes so that the
number of boxes with <span class="math inline">\(k\)</span> in them equals to the number of boxes with <span class="math inline">\(-k\)</span>
in them?</em></p>
<p>To get the answer, we start by fixing a possible “count” <span class="math inline">\((i_1,\dots,  i_d)\)</span>, satisfying <span class="math inline">\(i_1+\dots+i_d=m\)</span> of the number of times each
of the values in <span class="math inline">\(\{1,2,\dots, d\}\)</span> occurs. These values have to be
placed in <span class="math inline">\(m\)</span> of the <span class="math inline">\(2m\)</span> slots and their negatives (possibly in a
different order) in the remaining <span class="math inline">\(m\)</span> slots. So, first, we choose the
“positive” slots (in <span class="math inline">\(\binom{2m}{m}\)</span> ways), and then distribute <span class="math inline">\(i_1\)</span>
“ones”, <span class="math inline">\(i_2\)</span> “twos”, etc., in those slots; this can be done in<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>
<span class="math display">\[\binom{ m }{ i_1 i_2 \dots i_d}\]</span> ways. This is also the number of
ways we can distribute the negative “ones”, “twos”, etc., in the
remaining slots. All in all, for fixed <span class="math inline">\(i_1,i_2,\dots, i_d\)</span>, all of this
can be done in <span class="math display">\[\binom{2m}{m} \binom{ m }{ i_1 i_2 \dots i_d}^2\]</span> ways.
Remembering that each path has the probability <span class="math inline">\((2d)^{-2m}\)</span>, and summing
over all <span class="math inline">\(i_1,\dots, i_d\)</span> with <span class="math inline">\(i_1+\dots+i_d=m\)</span>, we get
<span class="math display">\[\begin{equation}
  p^{(2m)} = \frac{1}{(2d)^{2m}} \binom{2m}{m} \sum_{i_1+\dots+i_d=m}
        \binom{ m }{ i_1 i_2 \dots i_d}^2.
(\#eq:p2m)
\end{equation}\]</span>
This expression looks so complicated that we better start examining is for
particular values of <span class="math inline">\(d\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>For <span class="math inline">\(d=1\)</span>, the expression above simplifies to <span class="math inline">\(p^{(2m)} = \frac{1}{4^{m}} \binom{2m}{m}\)</span>. It is
still too complicated sum over all <span class="math inline">\(m\in{\mathbb{N}}\)</span>, but we can simplify it
further by using Stirling’s formula
<span class="math display">\[n! \sim \sqrt{2\pi n} \big(\tfrac{n}{e}\big)^n,\]</span> where <span class="math inline">\(a_n \sim b_n\)</span>
means <span class="math inline">\(\lim_{n\to\infty} a_n/b_n=1\)</span>. Indeed, from there,
<span class="math display">\[\label{equ:binom}
 \begin{split}
\binom{2m}{m} \sim \frac{4^m}{ \sqrt{\pi m}},
 \end{split} \text{ and so } p^{(2m)} \sim  \frac{1}{\sqrt{m\pi}}.\]</span> That means that <span class="math inline">\(p^{(m)}\)</span> behaves
li a <span class="math inline">\(p\)</span>-series with <span class="math inline">\(p=1/2\)</span> which we know is divergent. Therefore,
<span class="math display">\[\sum_{m=1}^{\infty} p^{(2m)} = \infty,\]</span>
and we recover our previous conclusion that the simple symmetric random
walk is, indeed, recurrent.</p></li>
<li><p>Moving on to the case <span class="math inline">\(d= 2\)</span>, we notice that the sum of the multinomial
coefficients in @ref(eq:p2m) no longer equals <span class="math inline">\(1\)</span>; in fact it is given
by<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>
<span class="math display">\[\label{equ:Van}
 \begin{split}
\sum_{i=0}^{m} \binom{m}{i}^2 = \binom{2m}{m},
 \end{split}\]</span> and, so,
<span class="math display">\[p^{(2m)} = \frac{1}{16^m} \Big( \frac{4^m}{\sqrt{\pi m}} \Big)^2 \sim
\frac{1}{\pi m}  \text{ implying that  } \sum_{m=1}^{\infty} p^{(2m)}=\infty,\]</span> which
which, in turn, implies that the two-dimensional random walk is also recurrent.</p></li>
<li><p>How about <span class="math inline">\(d\geq 3\)</span>? Things are even more complicated now. The
multinomial sum in @ref(eq:p2m) above does not admit a nice closed-form expression as in
the case <span class="math inline">\(d=2\)</span>, so
we need to do some estimates; these are a bit tedious so we skip them,
but report the punchline, which is that <span class="math display">\[p^{(2m)} 
\sim C \Big(
\tfrac{3}{m} \Big)^{3/2},\]</span> for some constant <span class="math inline">\(C\)</span>. This is where it gets
interesting: this is a <span class="math inline">\(p\)</span>-series which <strong>converges</strong>:
<span class="math display">\[\sum_{m=1}^{\infty} p^{(2m)}&lt;\infty,\]</span> and, so, the random walk is
transient for <span class="math inline">\(d=3\)</span>. This is enough to conclude that the random walk is
transient for all <span class="math inline">\(d\geq 3\)</span>, too (why?).</p></li>
</ol>
<p>To summarize</p>

<div class="theorem">
<span id="thm:unnamed-chunk-233" class="theorem"><strong>(#thm:unnamed-chunk-233) </strong></span>(Polya)
The simple symmetric random walk is recurrent for <span class="math inline">\(d=1,2\)</span>, but transient
for <span class="math inline">\(d\geq 3\)</span>.
</div>
<p><br>
In the words of Shizuo Kakutani</p>
<blockquote>
<p><em>A drunk man will find his way home, but a drunk bird may get lost
forever.</em></p>
</blockquote>
</div>
</div>
<div id="class-properties" class="section level2" number="6.4">
<h2 number="6.4"><span class="header-section-number">6.4</span> Class properties</h2>
<p>COMING SOON</p>
</div>
<div id="a-few-examples" class="section level2" number="6.5">
<h2 number="6.5"><span class="header-section-number">6.5</span> A few Examples</h2>
<p>COMING SOON</p>
</div>
<div id="additional-problems-for-chapter-6" class="section level2" number="6.6">
<h2 number="6.6"><span class="header-section-number">6.6</span> Additional problems for Chapter 6</h2>
qqq cl-stat-09
<!--
  cl-stat-09
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> be two (different) classes. For each of the following statements either explain
why it is true, or give an example showing that it is false.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(i\to j\)</span> or <span class="math inline">\(j\to i\)</span>, for all <span class="math inline">\(i\in C_1\)</span>, and <span class="math inline">\(j\in C_2\)</span>,</p></li>
<li><p><span class="math inline">\(C_1\cup C_2\)</span> is not a class,</p></li>
<li><p>If <span class="math inline">\(i\to j\)</span> for some <span class="math inline">\(i\in C_1\)</span> and <span class="math inline">\(j\in C_2\)</span>, then <span class="math inline">\(k\not\to l\)</span>
for all <span class="math inline">\(k\in C_2\)</span> and <span class="math inline">\(l\in C_1\)</span>,</p></li>
<li><p>If <span class="math inline">\(i\to j\)</span> for some <span class="math inline">\(i\in C_1\)</span> and <span class="math inline">\(j\in C_2\)</span>, then <span class="math inline">\(k\to l\)</span> for
some <span class="math inline">\(k\in C_2\)</span> and <span class="math inline">\(l\in C_1\)</span>,</p></li>
</ol>
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/03_Classification_of_States/cl-stat-09_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->
<!--
  cl-stat-02
  ------------------------------------------------
-->
<div class="problem">
Consider a Markov Chain whose transition graph is given below (with orange edges having probability <span class="math inline">\(1/2\)</span>, black <span class="math inline">\(1\)</span>, blue <span class="math inline">\(3/4\)</span> and green <span class="math inline">\(1/4\)</span>)
<center>
<img src="_main_files/figure-html/unnamed-chunk-324-1.png" width="672" style="margin-top:-10%; margin-bottom: -20%" style="display: block; margin: auto;" />
</center>
<ol style="list-style-type: decimal">
<li><p>Identify the classes.</p></li>
<li><p>Find transient and recurrent states.</p></li>
<li><p>Find periods of all states.</p></li>
<li><p>Compute <span class="math inline">\(f^{(n)}_{13}\)</span>, for all <span class="math inline">\(n\in{\mathbb{N}}\)</span>, where
<span class="math inline">\(f^{(n)}_{ij} ={\mathbb{P}}_i[T_j(1) = n]\)</span>.</p></li>
<li><p>Using software, we can get that, approximately,
<span class="math display">\[ P^{20}=\left[
\begin{array}{llllllll}
 0 &amp; 0 &amp; 0.15 &amp; 0.14 &amp; 0.07 &amp; 0.14 &amp; 0.21 &amp; 0.29 \\
 0 &amp; 0 &amp; 0.13 &amp; 0.15 &amp; 0.07 &amp; 0.15 &amp; 0.21 &amp; 0.29 \\
 0 &amp; 0 &amp; 0.3 &amp; 0.27 &amp; 0.15 &amp; 0.28 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0.27 &amp; 0.3 &amp; 0.13 &amp; 0.29 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0.29 &amp; 0.28 &amp; 0.15 &amp; 0.28 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0.28 &amp; 0.29 &amp; 0.14 &amp; 0.29 &amp; 0 &amp; 0 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.43 &amp; 0.57 \\
 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0.43 &amp; 0.57
\end{array}
\right],\]</span>
where <span class="math inline">\(P\)</span> is the transition matrix of the chain. Compute the
probability <span class="math inline">\({\mathbb{P}}[X_{20}=3]\)</span>, if the initial distribution (the
distribution of <span class="math inline">\(X_0\)</span>) is given by <span class="math inline">\({\mathbb{P}}[X_0=1]=1/2\)</span> and <span class="math inline">\({\mathbb{P}}[X_0=3]=1/2\)</span>.</p></li>
</ol>
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/03_Classification_of_States/cl-stat-02_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->
<!--
  cl-stat-04
  ------------------------------------------------
-->
<div class="problem">
<p>A fair 6-sided die is rolled repeatedly, and for <span class="math inline">\(n\in{\mathbb{N}}\)</span>, the outcome
of the <span class="math inline">\(n\)</span>-th roll is denoted by <span class="math inline">\(Y_n\)</span> (it is assumed that <span class="math inline">\(\{Y_n\}_{n\in{\mathbb{N}}}\)</span> are
independent of each other). For <span class="math inline">\(n\in{\mathbb{N}}_0\)</span>, let <span class="math inline">\(X_n\)</span> be the remainder
(taken in the set <span class="math inline">\(\{0,1,2,3,4\}\)</span>) left after the sum
<span class="math inline">\(\sum_{k=1}^n Y_k\)</span> is divided by <span class="math inline">\(5\)</span>, i.e. <span class="math inline">\(X_0=0\)</span>, and <span class="math display">\[%\label{}
    \nonumber 
    \begin{split}
X_n= \sum_{k=1}^n Y_k \ (\,\mathrm{mod}\, 5\,),\text{ for } n\in{\mathbb{N}}, 
    \end{split}\]</span> making <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> a Markov chain on the state space
<span class="math inline">\(\{0,1,2,3,4\}\)</span> (no need to prove this fact).</p>
<p>Write down the transition matrix of the chain, classify the states,
separate recurrent from transient ones, and compute the period of each
state.</p>
</div>
<div class="solution">
<p>The outcomes <span class="math inline">\(1,2,3,4,5,6\)</span> leave remainders <span class="math inline">\(1,2,3,4,0,1\)</span>, when divided
by <span class="math inline">\(5\)</span>, so the transition matrix <span class="math inline">\(P\)</span> of the chain is given by
<span class="math display">\[P=\begin{bmatrix}
  \tfrac{1}{6} &amp;   \tfrac{1}{3} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} \\
  \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{3} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} \\
  \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{3} &amp;   \tfrac{1}{6} \\
  \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{3} \\
  \tfrac{1}{3} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} &amp;   \tfrac{1}{6} \\
\end{bmatrix}\]</span> Since <span class="math inline">\(p_{ij}&gt;0\)</span> for all <span class="math inline">\(i,j\in S\)</span>, all the states
belong to the same class, and, because there is at least one recurrent
state in a finite-state-space Markov chain and because recurrence is a
class property, all states are recurrent. Finally, <span class="math inline">\(1\)</span> is in the return
set of every state, so the period of each state is <span class="math inline">\(1\)</span>.</p>
</div>
<!--
  cl-stat-06
  ------------------------------------------------
-->
<div class="problem">
<p>Which of the following
statements is true? Give a short explanation (or a counterexample where
appropriate) for your choice. <span class="math inline">\(\{X_n\}_{n\in {\mathbb{N}}_0}\)</span> is a Markov chain with state
space <span class="math inline">\(S\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>If states <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> intercommunicate, then there exists <span class="math inline">\(n\in{\mathbb{N}}\)</span>
such that <span class="math inline">\(p^{(n)}_{ij}&gt;0\)</span> and <span class="math inline">\(p^{(n)}_{ji}&gt;0\)</span>.</p></li>
<li><p>If all rows of the transition matrix are equal, then all states
belong to the same class.</p></li>
<li><p>If <span class="math inline">\(P^n\to I\)</span>, then all states are recurrent. (<em>Note:</em> We say that a
sequence <span class="math inline">\(\{A_n\}_{n\in{\mathbb{N}}}\)</span> of matrices converges to the matrix <span class="math inline">\(A\)</span>, and we
denote it by <span class="math inline">\(A_n\to A\)</span>, if <span class="math inline">\((A_n)_{ij}\to A_{ij}\)</span>, as <span class="math inline">\(n\to\infty\)</span>,
for all <span class="math inline">\(i,j\)</span>.)</p></li>
</ol>
</div>
<div class="solution">
<ol style="list-style-type: decimal">
<li><p><strong>FALSE</strong>. Consider a Markov chain with the transition matrix
<span class="math display">\[\begin{equation}
P = \begin{bmatrix}
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0
\end{bmatrix}
\end{equation}\]</span>
All states intercommunicate, but <span class="math inline">\(p^{(n)}_{12}&gt;0\)</span> if and
only if <span class="math inline">\(n\)</span> is of the form <span class="math inline">\(n=3k+1\)</span>, for <span class="math inline">\(k\in{\mathbb{N}}_0\)</span>. On the other hand
<span class="math inline">\(p^{(n)}_{21}&gt;0\)</span> if and only if <span class="math inline">\(n=3k+2\)</span> , for some <span class="math inline">\(k\in{\mathbb{N}}_0\)</span>. Thus,
<span class="math inline">\(p^{(n)}_{12}\)</span> and <span class="math inline">\(p^{(n)}_{21}\)</span> are never simultaneously positive.</p></li>
<li><p><strong>FALSE</strong>. Consider a Markov chain with the following transition
matrix: <span class="math display">\[P=
\begin{bmatrix}
1 &amp; 0 \\ 1 &amp; 0
\end{bmatrix}.\]</span> Then <span class="math inline">\(1\)</span> is an absorbing state and it is in a class
of its own, so it is not true that all states belong to the same
class.</p></li>
<li><p><strong>TRUE.</strong> Suppose that there exists a transient state <span class="math inline">\(i\in S\)</span>. Then
<span class="math inline">\(\sum_{n} p^{(n)}_{ii}&lt;\infty\)</span>, and, in particular, <span class="math inline">\(p^{(n)}_{ii}\to 0\)</span>, as
<span class="math inline">\(n\to\infty\)</span>. This is a contradiction with the assumption that
<span class="math inline">\(p^{(n)}_{ii}\to 1\)</span>, for all <span class="math inline">\(i\in S\)</span>.</p></li>
</ol>
</div>
qqq cl-stat-07
<!--
  cl-stat-07
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(C\)</span> be a class in a
Markov chain. For each of the following statements either explain why it
is true, or give an example showing that it is false.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(C\)</span> is closed,</p></li>
<li><p><span class="math inline">\(C^c\)</span> is closed,</p></li>
<li><p>At least one state in <span class="math inline">\(C\)</span> is recurrent,</p></li>
<li><p>For all states <span class="math inline">\(i,j\in C\)</span>, <span class="math inline">\(p_{ij}&gt;0\)</span>,</p></li>
</ol>
</div>
<div class="solution">
<ol style="list-style-type: decimal">
<li><p>False. Take <span class="math inline">\(C=\{(0,0)\}\)</span> in the “Tennis example”.</p></li>
<li><p>False. Take <span class="math inline">\(C=\{\text{Player 1 wins}\}\)</span> in the “Tennis example”.</p></li>
<li><p>False. It is enough to take any transient class in a finite-state
Markov chain as a counterexample. For instance, the class
<span class="math inline">\(\{ (0,0) \}\)</span> consisting of a single element <span class="math inline">\((0,0)\)</span> in the Tennis
chain.</p></li>
<li><p>False. This would be true if it read “for each pair of states
<span class="math inline">\(i,j\in C\)</span>, <em>there exists</em> <span class="math inline">\(n\in{\mathbb{N}}\)</span> such that <span class="math inline">\(p^{(n)}_{ij}&gt;0\)</span>”.
Otherwise, we can use the “Tennis chain” and the states <span class="math inline">\(i=(40,Adv)\)</span>
and <span class="math inline">\(j=(Adv,40)\)</span>. They belong to the same class, but <span class="math inline">\(p_{ij}=0\)</span> (you
need to pass through <span class="math inline">\((40,40)\)</span> to go from one to another).</p></li>
</ol>
</div>
qqq cl-stat-08
<!--
  cl-stat-08
  ------------------------------------------------
-->
<div class="problem">
<p>Consider a Markov chain whose
state space has <span class="math inline">\(n\)</span> elements (<span class="math inline">\(n\in{\mathbb{N}}\)</span>). For each of the following
statements either explain why it is true, or give an example showing
that it is false.</p>
<ol style="list-style-type: decimal">
<li><p>all classes are closed</p></li>
<li><p>at least one state is transient,</p></li>
<li><p>not more than half of all states are transient,</p></li>
<li><p>there are at most <span class="math inline">\(n\)</span> classes,</p></li>
</ol>
</div>
<div class="solution">
<ul>
<li><p>False. In the “Tennis” example, there are classes that are not
closed.</p></li>
<li><p>False. Just take the Regime Switching with <span class="math inline">\(0&lt;p_{01}, p_{10} &lt;1\)</span>. Both of the states are recurrent there. Or, simply take
a Markov chain with only one state (<span class="math inline">\(n=1\)</span>).</p></li>
<li><p>False. In the “Tennis” example, 18 states are transient, but <span class="math inline">\(n=20\)</span>.</p></li>
<li><p>True. Classes form a partition of the state space, and each class
has at least one element. Therefore, there are at most <span class="math inline">\(n\)</span> classes.</p></li>
</ul>
</div>
qqq cl-stat-10
<!--
  cl-stat-10
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(i\)</span> be a recurrent state
with period 5, and let <span class="math inline">\(j\)</span> be another state. For each of the following
statements either explain why it is true, or give an example showing
that it is false.</p>
<ol style="list-style-type: decimal">
<li><p>if <span class="math inline">\(j\to i\)</span>, then <span class="math inline">\(j\)</span> is recurrent,</p></li>
<li><p>if <span class="math inline">\(j\to i\)</span>, then <span class="math inline">\(j\)</span> has period <span class="math inline">\(5\)</span>,</p></li>
<li><p>if <span class="math inline">\(i\to j\)</span>, then <span class="math inline">\(j\)</span> has period <span class="math inline">\(5\)</span>,</p></li>
<li><p>if <span class="math inline">\(j\not\to i\)</span> then <span class="math inline">\(j\)</span> is transient,</p></li>
</ol>
</div>
<div class="solution">
We will use the following chain for all the counterexamples (green edges have probability <span class="math inline">\(1/2\)</span> and black edges <span class="math inline">\(1\)</span>)
<center>
<img src="_main_files/figure-html/unnamed-chunk-325-1.png" width="672" style="margin-top:-15%; margin-bottom: -15%" style="display: block; margin: auto;" />
</center>
<ol style="list-style-type: decimal">
<li><p>False. Take <span class="math inline">\(j=0\)</span> and <span class="math inline">\(i=1\)</span> in the chain in the picture.</p></li>
<li><p>False. Take the same counterexample as above.</p></li>
<li><p>True. We know that <span class="math inline">\(i\)</span> is recurrent, and since all recurrent classes
are closed, and <span class="math inline">\(i\to j\)</span>, <span class="math inline">\(h\)</span> must belong to the same class as <span class="math inline">\(i\)</span>.
Period is a class property, so the period of <span class="math inline">\(j\)</span> is also <span class="math inline">\(5\)</span>.</p></li>
<li><p>False. Take <span class="math inline">\(j=6\)</span>, <span class="math inline">\(i=0\)</span> in the chain in the picture.</p></li>
</ol>
</div>
<!--
  cl-stat-11
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> be two states
such that <span class="math inline">\(i\)</span> is transient and <span class="math inline">\(i\leftrightarrow j\)</span>. For each of the following
statements either explain why it is true, or give an example showing
that it is false.</p>
<ol style="list-style-type: decimal">
<li><p>if <span class="math inline">\(i\to k\)</span>, then <span class="math inline">\(k\)</span> is transient,</p></li>
<li><p>if <span class="math inline">\(k\to i\)</span>, then <span class="math inline">\(k\)</span> is transient,</p></li>
<li><p>period of <span class="math inline">\(i\)</span> must be <span class="math inline">\(1\)</span>,</p></li>
<li><p>(extra credit) <span class="math inline">\(\sum_{n=1}^{\infty} p^{(n)}_{jj} = \sum_{n=1}^{\infty} p^{(n)}_{ii}\)</span>,</p></li>
</ol>
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/03_Classification_of_States/cl-stat-11_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->
qqq cl-stat-12
<!--
  cl-stat-12
  ------------------------------------------------
-->
<div class="problem">
<p>Suppose there exists <span class="math inline">\(n\in{\mathbb{N}}\)</span>
such that <span class="math inline">\(P^n=I\)</span>, where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(P\)</span> is the
transition matrix of a finite-state-space Markov chain. For each of the
following statements either explain why it is true, or give an example
showing that it is false.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P=I\)</span>.</p></li>
<li><p>All states belong to the same class.</p></li>
<li><p>All states are recurrent.</p></li>
<li><p>The period of each state is <span class="math inline">\(n\)</span>.</p></li>
</ol>
</div>
<div class="solution">
<ol style="list-style-type: decimal">
<li><p>False. Take the Regime-switching chain with <span class="math display">\[P=
\begin{bmatrix}
0 &amp; 1 \\ 1 &amp; 0
\end{bmatrix}\]</span> Then <span class="math inline">\(P^2=I\)</span>, but <span class="math inline">\(P\not= I\)</span>.</p></li>
<li><p>False. If <span class="math inline">\(P=I\)</span>, all states are absorbing, and, therefore, each is
in a class of its own.</p></li>
<li><p>True. By the assumption <span class="math inline">\(P^{kn}=(P^n)^k=I^k=I\)</span>, for all <span class="math inline">\(k\in{\mathbb{N}}\)</span>.
Therefore, <span class="math inline">\(p^{(kn)}_{ii}=1\)</span> for all <span class="math inline">\(k\in{\mathbb{N}}\)</span>, and so
<span class="math inline">\(\lim_{m\to\infty} p^{(m)}_{ii}\not= 0\)</span> (maybe it doesn’t even
exist). In any case, the series <span class="math inline">\(\sum_{m=1}^{\infty} p^{(m)}_{ii}\)</span>
cannot be convergent, and so, <span class="math inline">\(i\)</span> is recurrent, for all <span class="math inline">\(i\in S\)</span>.
Alternatively, the condition <span class="math inline">\(P^n=I\)</span> means that the chain will be
coming back to where it started - with certainty - every <span class="math inline">\(n\)</span> steps,
and so, all states must be recurrent.</p></li>
<li><p>False. Any chain satisfying <span class="math inline">\(P^n=I\)</span>, but with the property that the
<span class="math inline">\(n\)</span> above is not unique is a counterexample. For example, if <span class="math inline">\(P=I\)</span>,
then <span class="math inline">\(P^n=I\)</span> for any <span class="math inline">\(n\in{\mathbb{N}}\)</span>.</p></li>
</ol>
</div>
qqq cl-stat-13
<!--
  cl-stat-13
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(i\)</span> be a recurrent state
with period 3, and let <span class="math inline">\(j\)</span> be another state. For each of the following
statements either explain why it is true, or give an example showing
that it is false.</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(j\to i\)</span>, then <span class="math inline">\(j\)</span> is recurrent.</p></li>
<li><p>If <span class="math inline">\(j\to i\)</span>, then <span class="math inline">\(j\)</span> has period <span class="math inline">\(3\)</span>.</p></li>
<li><p>If <span class="math inline">\(i\to j\)</span>, then <span class="math inline">\(j\)</span> has period <span class="math inline">\(3\)</span>.</p></li>
<li><p>If <span class="math inline">\(j\not\to i\)</span> then <span class="math inline">\(j\)</span> is transient.</p></li>
</ol>
</div>
<div class="solution">
<p>The only true statement is (c). Since <span class="math inline">\(i\)</span> is recurrent and <span class="math inline">\(i\to j\)</span>, we
have <span class="math inline">\(j\to i\)</span>, and so, <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> belong to the same class. Periodicity
is a class property so the period of <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are the same, namely
<span class="math inline">\(3\)</span>.</p>
</div>
qqq cl-stat-14
<!--
  cl-stat-14
  ------------------------------------------------
-->
<div class="problem">
<p>Let <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> be two
(different) communication classes of a Markov chain. Exactly one of the
statements below is necessarily true. Which one?</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(i\to j\)</span> or <span class="math inline">\(j\to i\)</span>, for all <span class="math inline">\(i\in C_1\)</span>, and <span class="math inline">\(j\in C_2\)</span></p></li>
<li><p><span class="math inline">\(C_1\cup C_2\)</span> is a class</p></li>
<li><p>if <span class="math inline">\(i\to j\)</span> for some <span class="math inline">\(i\in C_1\)</span> and <span class="math inline">\(j\in C_2\)</span>, then <span class="math inline">\(k\not\to l\)</span> for
all <span class="math inline">\(k\in C_2\)</span> and <span class="math inline">\(l\in C_1\)</span></p></li>
<li><p>if <span class="math inline">\(i\to j\)</span> for some <span class="math inline">\(i\in C_1\)</span> and <span class="math inline">\(j\in C_2\)</span>, then <span class="math inline">\(k\to l\)</span> for
some <span class="math inline">\(k\in C_2\)</span> and <span class="math inline">\(l\in C_1\)</span>,</p></li>
<li><p>Statements 1. - 4. are all false.</p></li>
</ol>
</div>
<div class="solution">
<p>The correct statement is 3. Suppose that <span class="math inline">\(i\to j\)</span> and <span class="math inline">\(k\to l\)</span> for some
<span class="math inline">\(k\in C_2\)</span> and some <span class="math inline">\(l\in C_1\)</span>. Since both <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> are in the same
class (and so are <span class="math inline">\(i\)</span> and <span class="math inline">\(l\)</span>), we have <span class="math inline">\(j\to k\)</span> (and <span class="math inline">\(l\to i\)</span>), and
then, by transitivity <span class="math display">\[i \to j \to k \to l \to i\]</span> which implies that
<span class="math inline">\(j\leftrightarrow k\)</span>. Therefore, <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> are in the same class, which is in
contradiction with the assumption that <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> are <em>different</em>
classes.</p>
</div>
<!--
  four-stmts
  ------------------------------------------------
-->
<div class="problem">
<p>Suppose that all classes of a Markov chain are recurrent, and let <span class="math inline">\(i,j\)</span>
be two states such that <span class="math inline">\(i\to j\)</span>. For each of the 4 statements before,
either explain why it is true, or give an example of a Markov chain in
which it fails.</p>
<ol style="list-style-type: lower-alpha">
<li><p>for each state <span class="math inline">\(k\)</span>, either <span class="math inline">\(i\to k\)</span> or <span class="math inline">\(j\to k\)</span></p></li>
<li><p><span class="math inline">\(j\to i\)</span></p></li>
<li><p><span class="math inline">\(p_{ji}&gt;0\)</span> or <span class="math inline">\(p_{ij}&gt;0\)</span></p></li>
<li><p><span class="math inline">\(\sum_{n=1}^{\infty} p^{(n)}_{jj}&lt;\infty\)</span></p></li>
</ol>
</div>
<div class="solution">
<ol style="list-style-type: lower-alpha">
<li><p>False. Take a chain with two states <span class="math inline">\(1,2\)</span> where <span class="math inline">\(p_{11}=p_{22}=1\)</span>,
and set <span class="math inline">\(i=j=1\)</span>, <span class="math inline">\(k=2\)</span>.</p></li>
<li><p>True. Recurrent classes are closed, so <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> belong to the same class. Therefore <span class="math inline">\(j\to i\)</span>.</p></li>
<li><p>False. Take a chain with <span class="math inline">\(4\)</span> states <span class="math inline">\(1,2,3,4\)</span> where
<span class="math inline">\(p_{12}=p_{23}=p_{34}=p_{41}=1\)</span>, and set <span class="math inline">\(i=1\)</span>, <span class="math inline">\(j=3\)</span>.</p></li>
<li><p>False. That would mean that <span class="math inline">\(j\)</span> is transient.</p></li>
</ol>
</div>
<p>⬇︎ In case you were wondering, the text below belongs to footnotes from somewhere high above.⬇︎</p>
<!--chapter:end:06-Classification.Rmd-->
</div>
</div>
<div id="appendix-appendix" class="section level1 unnumbered">
<h1 class="unnumbered">(APPENDIX) Appendix</h1>
</div>
<div id="dist" class="section level1" number="7">
<h1 number="7"><span class="header-section-number">7</span> Probability Distributions</h1>
<p>Here are the basic facts about the probability distributions we will need in these lecture notes. For a much longer list of important distributions, check this <a href="https://en.wikipedia.org/wiki/List_of_probability_distributions">wikipedia page</a>.</p>
<div id="discrete-distributions" class="section level2" number="7.1">
<h2 number="7.1"><span class="header-section-number">7.1</span> Discrete distributions:</h2>
Note: <span class="math inline">\((q=1-p)\)</span>
<table>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;font-style: italic;">
</td>
<td style="text-align:left;font-style: italic;">
Parameters
</td>
<td style="text-align:left;font-style: italic;">
Notation
</td>
<td style="text-align:left;font-style: italic;">
Support
</td>
<td style="text-align:left;font-style: italic;">
pmf
</td>
<td style="text-align:left;font-style: italic;">
<span class="math inline">\({\mathbb{E}}[X]\)</span>
</td>
<td style="text-align:left;font-style: italic;">
<span class="math inline">\(\operatorname{Var}[X]\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Bernoulli
</td>
<td style="text-align:left;">
<span class="math inline">\(p\in (0,1)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(B(p)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\{0,1\}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\((q,p,0,0,\dots)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(p\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(pq\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Binomial
</td>
<td style="text-align:left;">
<span class="math inline">\(n\in{\mathbb{N}}, p\in (0,1)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(b(n,p)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\{0,1,\dots, n\}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\binom{n}{k} p^k q^{n-k}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(np\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(npq\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Geometric
</td>
<td style="text-align:left;">
<span class="math inline">\(p\in (0,1)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(g(p)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\{0,1,\dots\}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(p q^k\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(q/p\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(q/p^2\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Poisson
</td>
<td style="text-align:left;">
<span class="math inline">\(\lambda\in(0,\infty)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(P(\lambda)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\{0,1,\dots\}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(e^{-\lambda} \tfrac{\lambda^k}{k!}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\lambda\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\lambda\)</span>
</td>
</tr>
</tbody>
</table>
</div>
<div id="continuous-distributions" class="section level2" number="7.2">
<h2 number="7.2"><span class="header-section-number">7.2</span> Continuous distributions:</h2>
Note: the pdf is given by the formula in the table only on its support. It is equal to <span class="math inline">\(0\)</span> outside of it.
<table>
<tbody>
<tr>
<td style="text-align:left;font-weight: bold;font-style: italic;">
</td>
<td style="text-align:left;font-style: italic;">
Parameters
</td>
<td style="text-align:left;font-style: italic;">
Notation
</td>
<td style="text-align:left;font-style: italic;">
Support
</td>
<td style="text-align:left;font-style: italic;">
pdf
</td>
<td style="text-align:left;font-style: italic;">
<span class="math inline">\({\mathbb{E}}[X]\)</span>
</td>
<td style="text-align:left;font-style: italic;">
<span class="math inline">\(\operatorname{Var}[X]\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Uniform
</td>
<td style="text-align:left;">
<span class="math inline">\(a\lt b\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(U(a,b)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\((a,b)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\frac{1}{b-a}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\frac{a+b}{2}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\frac{(b-a)^2}{12}\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Normal
</td>
<td style="text-align:left;">
<span class="math inline">\(\mu\in{\mathbb R},\sigma \gt 0\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(N(\mu,\sigma)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\({\mathbb R}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\frac{1}{\sigma \sqrt{2\pi}} e^{-\tfrac{(x-\mu)^2}{2 \sigma^2}}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\mu\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\sigma^2\)</span>
</td>
</tr>
<tr>
<td style="text-align:left;font-weight: bold;">
Exponential
</td>
<td style="text-align:left;">
<span class="math inline">\(\lambda\gt 0\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\operatorname{Exp}(\lambda)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\((0,\infty)\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\lambda e^{-\lambda x}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\tfrac{1}{\lambda}\)</span>
</td>
<td style="text-align:left;">
<span class="math inline">\(\frac{1}{\lambda^2}\)</span>
</td>
</tr>
</tbody>
</table>
<!-- \mypar{Discrete Random variables:} -->
<!--  \begin{description} -->
<!--  \item[pmf:] $p_X(x) = \PP[ X=x]$, for $x\in \sS_X$ (the support). -->
<!--  \item[expectation:] $\EE[X] = \sum_{x\in\sS_X} x\, p_X(x)$. -->
<!--  \item[variance:] $\Var[X] = \EE[ (X-\EE[X])^2] = \EE[X^2] - -->
<!--  \EE[X]^2$. -->
<!--  \item[expectation of a function:] if $\EE[g(X)]$ exists then -->
<!--    \[ \EE[ g(X)] = \tsum_{x\in \sS_X} g(x) p_X(x).\] -->
<!--  \item[expectation of a linear combination: ] \[ \EE[ \alpha X+\beta Y] = -->
<!--    \alpha \EE[X] + \beta \EE[Y].\] -->
<!--  \item[variance of a linear combination:] if $X,Y$ are independent, -->
<!--    \[ \Var[\alpha X+\beta Y] = \alpha^2 \Var[X] + \beta^2 \Var[Y].\] -->
<!--  \item[expectation of a product:] if $X,Y$ are independent and both -->
<!--    expectations $\EE[X]$ and $\EE[Y]$ exist: -->
<!--    \[ \EE[X Y] = \EE[X]\times \EE[Y].\] -->
<!--  \end{description} -->
<!--  \bigskip -->
<!-- \mypar{Random walks} -->
<!--  \begin{description} -->
<!--    \item[definition:] -->
<!--      $\seqk{\delta}$ are independent with\\ $\PP[ \delta_k = 1]=p$ and -->
<!--      $\PP[\delta_k=-1]=q:=1-p$, \\[1ex] -->
<!-- $X_0=0$,\  $X_n = \delta_1 + \delta_2 + \dots + \delta_n$, -->
<!--    \item[distribution:] $\PP[ X_n = k ] = -->
<!--      \binom{n}{\tf{n+k}{2}} p^{\tf{n+k}{2}} q^{\tf{n-k}{2}}$,\\ if $n$ and $k$ have -->
<!--      the same parity and $-n \leq k \leq n$. -->
<!--    \item[maximum:] $M_n = \max(X_0,X_1,\dots, X_n)$. If -->
<!--      $p=\tot$ then \\ -->
<!--      $\PP[ M_n = k] = \PP[ X_n=k] + \PP[ X_n = k+1]$. -->
<!--  \end{description} -->
<!-- \columnbreak -->
<!--    \mypar{Generating functions ($X$ must be $\Nz$-valued)} -->
<!--  \begin{description} -->
<!--    \item[definition:] $P_X(s) = \sum_{k=0}^{\infty} p_k s^k = \EE[ s^X]$. -->
<!--    \item[probabilities:] $p_k = \PP[X=k] = \oo{k!} P^{(k)}(0)$\\ (where -->
<!--      $P^{(k)}$ is the $k$-th derivative) -->
<!--    \item[moments:] -->
<!--      $\EE[X] = P_X'(1)$, \ $\EE[X(X-1)] = P_X''(1)$,\\[1ex] $\Var[X] = P_X''(1) -->
<!--      + P_X'(1) - (P'_X(1))^2$\\[1ex] -->
<!--      $\EE\left[ \prod_{k=0}^{n-1} (X-k)\right] = P_X^{(k)}(1)$. -->
<!--    \item[convolutions:] the convolution $r=p \ast q$ of $\seqkz{p}$ and $\seqkz{q}$ is -->
<!--      $r_n = \tsum_{k=0}^n p_k q_{n-k}, n\in\Nz$. -->
<!--    \item[sums of independent variables:] if $X$ and $Y$ are $\Nz$-valued and -->
<!--      independent, with pmfs $P_X$ and $P_Y$, and $Z=X+Y$, then -->
<!--      the pmf of $Z$ is the convolution of the pmfs of $X$ and $Y$ and -->
<!--      $P_Z(s) = P_X(s) P_Y(s)$. -->
<!--    \item[random sums:] If $\seq{\xi}$ are iid and independent of $N$, and -->
<!--      all are $\Nz$-valued then $P_Y(s) = P_N(P_{\xi_1}(s))$, where -->
<!--      $Y = \sum_{k=0}^N \xi_k$. Also $\EE[Y] = \EE[N] \times \EE[\xi_1]$. -->
<!--  \end{description} -->
<!--  \mypar{Advanced Random Walks} -->
<!--  \begin{description} -->
<!--    \item[Walambda's formulas:] Let $\seq{\xi}$ be an iid sequence. Then $\EE[ -->
<!--      \sum_{k=1}^T \xi_k] = \EE[T]\, \EE[\xi_1]$, when $\EE[T]<\infty$ and -->
<!--      $\EE[\xi_1]<\infty$ in the following two cases -->
<!--      I) $T$ is independent of $\seq{\xi}$, or -->
<!--      II) $T$ is a stopping time w.r.to $\seq{\xi}$. -->
<!--    \item[Distribution of $T_1$:] The generating function $P_{T_1}$ of $T_1$ (the -->
<!--       first hitting time of the level $1$ for the random walk with -->
<!--       $p=\PP[X_1=1]$) is -->
<!--       $P_{T_1}(s) = \oo{2 q s}(1 - \sqrt{1 - 4 p q s^2} )$ -->
<!--       and it satisfies the equation $P_{T_1}(s) = p s + q s P_{T_1}(s)^2$. -->
<!--  \end{description} -->
<!--  \end{multicols} -->
<!-- \pagebreak -->
<!-- \begin{multicols}{2} -->
<!-- \mypar{Branching processes} -->
<!--  \begin{description} -->
<!--    \item[Distribution of $Z_n$:] The generating function of $Z_n$ is -->
<!--      \[ P_{Z_n}(s) = P(P(\dots P(s))) \text{ ($n$ Ps), }\] -->
<!--      where $P$ is the generating function of -->
<!--      the offspring distribution. -->
<!--    \item[Moments of $Z_n$:] If $\mu = \EE[Z_1]$ and $\sigma^2 = -->
<!--      \Var[Z_1]$, then -->
<!--      \[ \EE[ Z_n ] = \mu^n \eand \Var[Z_n] = \sigma^2 -->
<!--      \mu^n(1+\mu+\dots+\mu^n) .\] -->
<!--    \item[Extinction:] The extinction probability $p_E$ is the smallest -->
<!--      solution of the extinction equation $x=P(x)$ in $[0,1]$. -->
<!--  \end{description} -->
<!-- \columnbreak -->
<!-- \vspace{5ex} -->
<!--  \mypar{Markov Chains} -->
<!--  \begin{description} -->
<!--    \item[Markov property:] $\seqz{X}$ is a Markov chain if -->
<!--      \begin{align*} -->
<!--        \PP[ X_{n+1} = i_{n+1}| , X_n = i_n, \dots, X_0=i_0]  = \\ , = -->
<!--         \PP[ X_{n+1} = i_{n+1} | X_n = i_n], -->
<!--      \end{align*} -->
<!--      for all (possible) $i_{n+1}, i_n,\dots, i_0$. -->
<!--    \item[Transition probabilities:] $p_{ij} = \PP[ X_{k+1} = j| X_k=i]$, -->
<!--      $p^{(n)}_{ij} = \PP[ X_{k+n}=j| X_k=i]$, -->
<!--      $P=(p_{ij})$ is the transition matrix. -->
<!--      \[ P^n = (p^{(n)}_{ij})\] -->
<!--      If is $a^{(n)}$ the (row) vector corresponding to the distribution of $X_n$, then -->
<!--      \[ a^{(n)} = a^{(0)} P^n .\] -->
<!--     \item[Recurrence and transience:] A state $i$ is recurrent if and only -->
<!--       if $\sum_{n\in\N} p^{(n)}_{ii} = \infty$. -->
<!--     \item[Canonical decomposition:] -->
<!--       \[ P = \pmat{ P_C , 0 \\ R , Q},\] -->
<!--       where recurrent states go before transient states. The fundamental -->
<!--       matrix $F$ is given by $F = (I-Q)^{-1}$. -->
<!--     \item[Absorption and reward:] $u_{ij} = (F R)_{ij}$, where $u_{ij}$ is -->
<!--       the probability that the first recurrent state we hit is $j$, given -->
<!--       that we start from the transient state $i$. -->
<!--       Also, $v_i = (F g)_i$, where $g$ is the -->
<!--       reward (column) vector and $v_i$ is the expected reward before -->
<!--       absorption, if we start from the transient state $i$. -->
<!--     \item[Stationary distributions:] The (row) vector $\pi$ is a -->
<!--       stationary distribution if $\pi = \pi P$. -->
<!--       If $X$ is finite and irreducible, a unique stationary distribution -->
<!--       $\pi$ exists and we have -->
<!--       \[ \nu_{ij} = \tf{\pi_j}{\pi_i} \eand m_i = \oo{\pi_i}\] -->
<!--       where $\nu_{ij}$ is the expected number of visits to $j$ between two -->
<!--       consecutive visits to $i$, and $m_i$ is the mean return time to $i$. -->
<!--     \item[Limiting distributions:] -->
<!--       $\pi$ is a limiting -->
<!--       distribution if $\pi_j = \lim_n p^{(n)}_{ij}$ for all $i,j$. Limiting -->
<!--       distributions exist for finite, irreducible and aperiodic chains. -->
<!--     \item[Ergodic theorem:] If $X$ is finite and irreducible and -->
<!--       $f:S\to\R$, we have -->
<!--       \[ \lim_{n\to\infty} \tf{f(X_1)+\dots+f(X_n)}{n} = \EE_{\pi}[f(X_1)] -->
<!--       = \sum_{i\in S} \pi_i f(i).\] -->
<!-- \end{description} -->
<!-- \end{multicols} -->
<!--chapter:end:distributions.Rmd-->
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>it may interfere with your existing installation<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>be careful, though. The expression <code>x = y</code> is not the same as <code>x == y</code>. It does not return a logical value - it assigns the value of <code>y</code> to <code>x</code><a href="#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>There are infinitely many ways random variables can be
distributed. Indeed, in the discrete <span class="math inline">\({\mathbb N}\)</span>-valued case only, any
sequence of nonnegative numbers <span class="math inline">\((p_n)_n\)</span> such that <span class="math inline">\(\sum_n p_n=1\)</span> defines
<em>a</em> probability distribution. It turns out, however, that a small-ish number of
distributions appear in nature much more often then the rest. These
distributions, like the normal, uniform, exponential, binomial, etc. turn out to
be so important that they each get a name (hence <em>named distributions</em>). <a href="#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Some books will define the geometric random variables as the number of <em>tosses</em> (and not Ts) before the first H is obtained. In that case, the final H is included into the count. Clearly, this definition and the one we have given differ by <span class="math inline">\(1\)</span>, and this is really not a big deal, but you have to be careful about what is meant when a geometric random variable is mentioned.<a href="#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The function <code>sum</code> adds up all the components of the vector.
You would not want such a function to be vectorized. If it were, it would return
exactly the same vector it got as input.<a href="#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>It is somewhat unfortunate that the standard notation for the time horizon, namely <span class="math inline">\(T\)</span>, coincides with a
shortcut <code>T</code> for <code>TRUE</code> in R. Our example still works fine because this shortcut is used only if there is no variable named <code>T</code>.<a href="#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>The function <code>apply</code> is often used as a substitute for a <code>for</code> loop because it has several advantages over it. First, the code is much easier to read and understand. Second, <code>apply</code> can easily be parallelized. Third, while this is not such a big issue anymore, <code>for</code> loops used to be orders of magnitude slower than the corresponding <code>apply</code> in the past. R’s <code>for</code> loops got much better recently, but they still lag behind <code>apply</code> in some cases. To be fair, <code>apply</code> is known to use more <em>memory</em> than <code>for</code> in certain cases.<a href="#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>For <span class="math inline">\(d=2\)</span> we could have used the values “up”, “down”, “left” and
“’right”, for <span class="math inline">\(1,-1,2\)</span> or <span class="math inline">\(-2\)</span>, respectively. In dimension <span class="math inline">\(3\)</span>, we
could have added “forward” and “backward”, but we run out of words
for directions for larger <span class="math inline">\(d\)</span>.<a href="#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p> <span class="math inline">\(\binom{m}{i_1  \dots i_d}\)</span> is called the <em>multinomial coefficient</em>. It
counts the number of ways we can color <span class="math inline">\(m\)</span> objects into one of <span class="math inline">\(d\)</span>
colors such that there are <span class="math inline">\(i_1\)</span> objects of color <span class="math inline">\(1\)</span>, <span class="math inline">\(i_2\)</span> of
color <span class="math inline">\(2\)</span>, etc. It is a generalization of the binomial coefficient
and its value is given by
<span class="math display">\[\binom{ m }{ i_1 i_2 \dots i_d} = \frac{m!}{i_1! i_2!\dots
            i_d!}.\]</span><a href="#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>Why is this identity true? Can you give a counting argument?<a href="#fnref10" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
