
Let $\seqz{X}$ be a stochastic process. A random variable $\tau$ taking
values in $\N_0\cup\set{+\infty}$ is said to be a with respect to
$\seqz{X}$ if for each $n\in\N_0$ there exists a function
$G^n:\R^{n+1}\to \set{0,1}$ such that
$$\inds{T=n}=G^n(X_0,X_1,\dots, X_n), \text{ for all } n\in\N_0.$$

The functions $G^n$ are called the , and should be thought of as a black
box which takes the values of the process $\seqz{X}$ observed up to the
present point and outputs either $0$ or $1$. The value $0$ means *keep
going* and $1$ means *stop*. The whole point is that the decision has to
based only on the available observations and not on the future ones.

 

1.  The simplest examples of stopping times are (non-random)
    *deterministic times*. Just set $T=5$ (or $T=723$ or $T=n_0$ for any
    $n_0\in\N_0\cup\set{+\infty}$), no matter what the state of the
    world $\omega\in\Omega$ is. The family of decision rules is easy to
    construct:
    $$G^n(x_0,x_1,\dots, x_n)=\begin{cases} 1,& n=n_0, \\ 0, & n\not=
      n_0.\end{cases}.$$ Decision functions $G^n$ do not depend on the
    values of $X_0,X_1,\dots,
    X_n$ *at all*. A gambler who stops gambling after 20 games, no
    matter of what the winnings or losses are uses such a rule.

2.  Probably the most well-known examples of stopping times are *(first)
    hitting times*. They can be defined for general stochastic
    processes, but we will stick to simple random walks for the purposes
    of this example. So, let $X_n=\sum_{k=0}^n \xi_k$ be a simple random
    walk, and let $T_l$ be the first time $X$ hits the level $l\in\N$e
    More precisely, we use the following slightly non-intuitive but
    mathematically correct definition
    $$T_l=\min\sets{n\in\N_0}{X_n=l}.$$ The set $\sets{n\in\N_0}{X_n=l}$
    is the collection of all time-points at which $X$ visits the level
    $l$. The earliest one - the minimum of that set - is the first
    hitting time of $l$. In states of the world $\omega\in\Omega$ in
    which the level $l$ just never gets reached, i.e., when
    $\sets{n\in\N_0}{X_n=l}$ is an empty set, we set
    $T_l(\omega)=+\infty$. In order to show that $T_l$ is indeed a
    stopping time, we need to construct the decision functions $G^n$,
    $n\in\N_0$. Let us start with $n=0$. We would have $T_l=0$ in the
    (impossible) case $X_0=l$, so we always have $G^0(X_0)=0$. How about
    $n\in\N$. For the value of $T_l$ to be equal to exactly $n$, two
    things must happen:

    1.  $X_n=l$ (the level $l$ must actually be hit at time $n$), and

    2.  $X_{n-1}\not = l$, $X_{n-2}\not= l$, ..., $X_{1}\not=l$,
        $X_0\not=l$ (the level $l$ has not been hit before).

    Therefore, $$G^n(x_0,x_1,\dots, x_n)=\begin{cases}
    1,& x_0\not=l, x_1\not= l, \dots, x_{n-1}\not=l, x_n=l\\
    0,&\text{otherwise}.
    \end{cases}$$ The hitting time $T_2$ of the level $l=2$ for a
    particular trajectory of a symmetric simple random walk is depicted
    below: $$\includegraphics{pics/hit_max_walk.pdf}.$$

3.  How about something that is *not* a stopping time? Let $T\in\N$ be
    an arbitrary time-horizon and let $T_M$ be the last time during
    $\ft{0}{T}$ that the random walk visits its maximum during
    $\ft{0}{T}$ (see picture above, where $T=30$). If you bought a stock
    at time $n=0$, had to sell it some time before or at $T$ and had the
    ability to predict the future, this is one of the points you would
    choose to sell it at. Of course, it is impossible in general to
    decide whether $T_M=n$, for some $n\in\ft{0}{T-1}$ without the
    knowledge of the values of the random walk after $n$. More
    precisely, let us sketch the proof of the fact that $T_M$ is not a
    stopping time. Suppose, to the contrary, that it is, and let $G^n$
    be the family of decision functions. Consider the following two
    trajectories: $(0,1,2,3,\dots,
      T-1,T)$ and $(0,1,2,3,\dots, T-1,T-2)$. They differ only in the
    direction of the last step. They also differ in the fact that
    $T_M=T$ for the first one and $T_M=T-1$ for the second one. On the
    other hand, by the definition of the decision functions, we have
    $$\inds{T_M=T-1}=G^{T-1}(X_0,\dots, X_{T-1}).$$ The right-hand side
    is equal for both trajectories, while the left-hand side equals to
    $0$ for the first one and $1$ for the second one. A contradiction.

Wald's identity and Gambler's Ruin
=====================================

Having defined the notion of a stopping time, let us try to compute
something about it. The random variables $\seq{\xi}$ in the statement of
the theorem below are only assumed to be independent of each other and
identically distributed. To make things simpler, you can think of
$\seq{\xi}$ as increments of a simple random walk. Before we state the
main result, here is an extremely useful identity:

[\[pro:EN-sum\]]{#pro:EN-sum label="pro:EN-sum"} Let $N$ be an
$\N_0$-valued random variable. Then
$$\EE[N]=\sum_{k=1}^{\infty} \PP[N\geq k].$$

Clearly, $\PP[N\geq k]=
\sum_{j\geq k} \PP[N=j]$, so (note what happens to the indices when we
switch the sums) $$\sum_{k=1}^{\infty} \PP[N\geq k]=
\sum_{k=1}^{\infty} \sum_{j\geq k} \PP[N=j]
=
\sum_{j=1}^{\infty} \sum_{k=1}^j \PP[n=j]=
\sum_{j=1}^{\infty} j \PP[N=j]=\EE[N].\qedhere$$

Let $\seq{\xi}$ be a sequence of independent, identically distributed
random variables with $\EE[\abs{\xi_1}]<\infty$. Set
$$X_n=\sum_{k=1}^n \xi_k,\ n\in\N_0.$$ If $T$ is an $\seqz{X}$-stopping
time such that $\EE[T]<\infty$, then $$\EE[X_T]=\EE[\xi_1]\EE[T].$$

Here is another way of writing the sum $\sum_{k=1}^{T} \xi_k$:
$$\sum_{k=1}^T\xi_k=\sum_{k=1}^{\infty} \xi_k \inds{k\leq T}.$$ The idea
behind it is simple: add all the values of $\xi_k$ for $k\leq
T$ and keep adding zeros (since $\xi_k \inds{k\leq T}=0$ for $k>T$)
after that. Taking expectation of both sides and switching $\EE$ and
$\sum$ (this can be justified, but the argument is technical and we omit
it here) yields: $$\begin{aligned}
   \label{equ:orig-wald}
 \EE[\sum_{k=1}^T \xi_k]=\sum_{k=1}^{\infty} \EE[ \inds{k\leq T}\xi_k].
 \end{aligned}$$ Let us examine the term $\EE[\xi_k\inds{k\leq T}]$ in
some detail. We first note that
$$\inds{k\leq T}=1-\inds{k>T}=1-\inds{k-1\geq
  T}=1-\sum_{j=0}^{k-1}\inds{T=j}.$$ Therefore,
$$\EE[\xi_k \inds{k\leq T}]=\EE[\xi_k]-\sum_{j=0}^{k-1}\EE[ \xi_k
\inds{T=j}].$$ By the assumption that $T$ is a stopping time, the
indicator $\inds{T=j}$ can be represented as
$\inds{T=j}=G^j(X_0,\dots, X_j)$, and, because each $X_i$ is just a sum
of the increments, we can actually write $\inds{T=j}$ as a function of
$\xi_1,\dots, \xi_j$ only - say $\inds{T=j}=H^j(\xi_1,\dots, \xi_j)$. By
the independence of $(\xi_1,\dots, \xi_j)$ from $\xi_k$ (because $j<k$)
we have $$\label{equ:693B}
 \begin{split}
    \EE[\xi_k \inds{T=j}]&=\EE[ \xi_k H^j(\xi_1,\dots, \xi_j)]=
   \EE[\xi_k] \EE[ H^j(\xi_1,\dots, \xi_j)]\\ &=\EE[\xi_k] \EE[\inds{T=j}]=
   \EE[\xi_k]\PP[T=j].
 \end{split}$$ Therefore, $$\label{equ:1A78}
 \begin{split}
    \EE[\xi_k \inds{k\leq T}]&=\EE[\xi_k]-\sum_{j=0}^{k-1} \EE[\xi_k]
   \PP[T=j]=\EE[\xi_k] \PP[T\geq k]\\ &=\EE[\xi_1] \PP[T\geq k],
 \end{split}$$ where the last equality follows from the fact that all
$\xi_k$ have the same distribution.

Going back to ([\[equ:orig-wald\]](#equ:orig-wald){reference-type="ref"
reference="equ:orig-wald"}), we get $$\label{equ:F98}
 \begin{split}
    \EE[X_T]&=\EE[\sum_{k=1}^T \xi_k]=\sum_{k=1}^{\infty}
   \EE[\xi_1]\PP[T\geq k]\\&=\EE[\xi_1] \sum_{k=1}^{\infty} \PP[T\geq
   k]=\EE[\xi_1] \EE[T],
 \end{split}$$ where we use Proposition
[\[pro:EN-sum\]](#pro:EN-sum){reference-type="ref"
reference="pro:EN-sum"} for the last equality.

. A gambler starts with $\$x$ dollars and repeatedly plays a game in
which he wins a dollar with probability $\tot$ and loses a dollar with
probability $\tot$. He decides to stop when one of the following two
things happens:

1.  he goes bankrupt, i.e., his wealth hits $0$, or

2.  he makes enough money, i.e., his wealth reaches some level $a>x$.

The classical "Gambler's ruin" (dating at least to 1600s) problem asks
the following question: what is the probability that the gambler will
make $a$ dollars before he goes bankrupt?

\medskip

Gambler's wealth $\seq{W}$ is modeled by a simple random walk starting
from $x$, whose increments $\xi_k=W_{k}-W_{k-1}$ are coin-tosses. Then
$W_n=x+X_n$, where $X_n=\sum_{k=0}^n \xi_k$, $n\in\N_0$. Let $T$ be the
time the gambler stops. We can represent $T$ in two different (but
equivalent) ways. On the one hand, we can think of $T$ as the smaller of
the two hitting times $T_{-x}$ and $T_{a-x}$ of the levels $-x$ and
$a-x$ for the random walk $\seqz{X}$ (remember that $W_n=x+X_n$, so
these two correspond to the hitting times for the process $\seqz{W}$ of
the levels $0$ and $a$). On the other hand, we can think of $T$ as the
first hitting time of the two-element *set* $\set{-x,a-x}$ for the
process $\seqz{X}$. In either case, it is quite clear that $T$ is a
stopping time (can you write down the decision functions?). When we
talked about the maximum of the simple symmetric random walk, we proved
that it hits any value if given enough time. Therefore, the probability
that that the gambler's wealth will remain strictly between $0$ and $a$
forever is zero and so, $\PP[T<\infty]=1$.

\medskip

What can we say about the random variable $X_T$ - the gambler's wealth
(minus $x$) at the *random* time $T$? Clearly, it is either equal to
$-x$ or to $a-x$, and the probabilities $p_0$ and $p_a$ with which it
takes these values are exactly what we are after in this problem. We
know that, since there are no other values $X_T$ can take, we must have
$p_0+p_a=1$. Second Wald's identity gives us the second equation for
$p_0$ and $p_a$: $$\EE[X_T]=\EE[\xi_1] \EE[T]=0\cdot \EE[T]=0,$$ so
$$0 = \EE[X_T]=p_0 (-x)+p_a (a-x).$$ These two linear equations with two
unknowns yield $$p_0= \frac{a-x}{a}, \ p_a=\frac{x}{a}.$$ It is
remarkable that the two probabilities are proportional to the amounts of
money the gambler needs to make (lose) in the two outcomes. The
situation is different when $p\not=\tot$.

The distribution of the first hitting time of a simple symmetric random walk
============================================================================

Let $\seqz{X}$ be a simple random walk, with the probability $p$ of
stepping up. Let $T_1=\min\sets{n\in\N_0}{X_n=1}$ be the first hitting
time of level $l=1$, and let $\seqz{p}$ be its pmf, i.e.,
$p_n=\PP[T_1=n]$, $n\in\N_0$. The goal of this section is to use the
powerful generating-function methods to find $\seqz{p}$.

#### A recursive formula.

We start with a simple observation that you cannot get from $0$ to $1$
for the first time in an even number of steps. Therefore, $p_{2n}=0$,
$n\in\N_0$. Also, $p_1=p$ - you simply have to go up on the first step.

\medskip
What about $n>1$? In order to go from $0$ to $1$ in $n>1$ steps (and not
before!) the first step needs to be *down* and then you need to climb up
from $-1$ to $1$ in $n-1$ steps. Climbing from $-1$ to $1$ is exactly
the same as climbing from $-1$ to $0$ and then climbing from $0$ to $1$.
If it took $j$ steps to go from $-1$ to $0$ it will have to take $n-1-j$
steps to go from $1$ to $2$, where $j$ can be anything from $1$ to
$n-2$, in order to finish the job in exactly $n-1$ steps. So, using
formulas, we have$$\label{equ:rec-hit-one-1}
   % \nonumber 
   \begin{split}
\PP[T_1=n]&=
q \sum_{j=1}^{n-2} \PP\Big[ \text{``exactly $j$ steps to first hit $0$ from
$-1$''}\\
& \quad \text{{\em and} ``exactly $n-1-j$ steps to first hit $1$ from
$0$''}\Big].
   \end{split}$$

But there is nothing special about $0$ as a starting point. Taking $j$
steps from $-1$ to $0$ is exactly the same as taking $j$ steps from $0$
to $1$, so
$$\PP[ \text{``exactly $j$ steps to first hit $0$ from $-1$''}]=\PP[ T_1=j]=p_j.$$
By the same token, $$\label{equ:8E0}
 \begin{split}
    \PP[ \text{``exactly n-1-j steps to first hit 1 from 0''}]
   &=\PP[ T_1=n-1-j]\\ &=p_{n-1-j}.
 \end{split}$$ Finally, I claim that the two events are independent of
each other. Indeed, once we have reached $0$, the future increments of
the random walk behave exactly the same as the increments of a fresh
random walk starting from zero - they are independent of everything that
happened in the past. Equivalently, a knowledge of everything that
happened until the moment the random walk hit $0$ for the first time
does not change our perception (and estimation) of what is going to
happen later (in this case the likelihood of hitting $1$ in exactly
$n-1-j$ steps). This property is called the *regeneration property* or
the *strong Lévy property* of random walks. More precisely (but still
not entirely precise), we can make the following claim:

> Let $\seqz{X}$ be a simple random walk and let $T$ be any
> $\N_0$-valued stopping time. Define the process $\seqz{Y}$ by
> $Y_n=X_{T+n}-X_T$. Then $\seqz{Y}$ is also a simple random walk, and
> it is independent of $X$ up to $T$.

In order to check your understanding, try to convince yourself that the
requirement that $T$ be a stopping time is necessary - find an example
of a random time $T$ which is not a stopping time where the statement
above fails.

We can go back to the distribution of the hitting time $T_1$, and use
our newly-found independence together with
([\[equ:rec-hit-one-1\]](#equ:rec-hit-one-1){reference-type="ref"
reference="equ:rec-hit-one-1"}) to obtain the following recursion
$$\label{equ:recursion-hit}
   % \nonumber 
   \begin{split}
 p_n=q \sum_{j=1}^{n-2} p_j p_{n-j-1}, n>1,\qquad p_0=0, \ p_1=p.
   \end{split}$$

#### Generating-function approach

This is where generating functions step in. We will use
([\[equ:recursion-hit\]](#equ:recursion-hit){reference-type="ref"
reference="equ:recursion-hit"}) to derive an equation for the generating
function $P(s)=\sum_{k=0}^{\infty} p_k s^k$. The sum on the right-hand
side of
([\[equ:recursion-hit\]](#equ:recursion-hit){reference-type="ref"
reference="equ:recursion-hit"}) looks a little bit like a convolution,
so let us compare it to the following expansion of the square $P(s)^2$:
$$P(s)^2=\sum_{k=0}^{\infty} (\sum_{i=0}^k p_i p_{k-i}) s^k.$$ The inner
sum $\sum_{i=0}^k p_i p_{k-i}$ needs to be split into several parts to
get an expression which matches
([\[equ:recursion-hit\]](#equ:recursion-hit){reference-type="ref"
reference="equ:recursion-hit"}): $$\begin{aligned}
\sum_{i=0}^k p_i p_{k-i}&=\underbrace{p_0 p_k}_0+
\sum_{i=1}^{k-1} p_{i} p_{k-i}+ \underbrace{p_k p_0}_0
= \sum_{i=1}^{(k+1)-2} p_i p_{(k+1)-i-1}\\ &= q^{-1} p_{k+1}, \efor k\geq 2. \end{aligned}$$
Therefore, since the coefficients of $P(s)^2$ start at $s^2$, we have
$$q s P(s)^2= q s \sum_{k=2}^{\infty} q^{-1} p_{k+1} s^k= \sum_{k=2}^{\infty}
p_{k+1} s^{k+1}=P(s)-p s,$$ which is nothing but a quadratic equation
for $P$.

[\[rem:186F\]]{#rem:186F label="rem:186F"} Here is another - shorter,
but less rigorous - way of deriving the same equation for $P$. The first
hitting time $T_1$ can be written as follows: $$\begin{aligned}
  T_1 = 
  \begin{cases}
    1, & X_1=1,\\
    1+T_1' + T_1'', & X_1=-1,
  \end{cases}\end{aligned}$$ where $T_1'$ is the time it takes to hit
$0$ from $-1$ and $T_1''$ is the time it takes to hit $1$ from $0$. The
notation $T'_1$ and $T''_1$ is suggestive of the fact that $T'_1$ and
$T''_1$ have the same distribution as $T_1$. Moreover, they are
independent of each other, as argued above. Using the expression
$P_X(s) = \EE[ s^X]$ and the law of total probability with conditioning
on the first step, we get $$\begin{aligned}
  P(s) &= \EE[s^{T_1}] = \EE[ s^{T_1}| X_1=1]\times \PP[ X_1=1] + \EE[ s^{T_2} |
  X_1 = -1] \times \PP[ X_1=-1]\\
       &= p \EE[ s^1|X_1=1] + q \EE[ s^{1+T_1'+T_1''}]
  = p s +q\EE[ s^{1+T_1'+T_1''}]\\ 
       & = ps + q s \EE[s^{T_1'}]\times \EE[ S^{T''_1}] = ps + q s P(s)^2.\end{aligned}$$

Now that we have the following equation for $P$: $$\begin{aligned}
  \label{equ:Ps-walk}
  P(s) = p s + q s P(s)^2,\end{aligned}$$ the first task is to solve it.
It is a quadratic equation in $P(s)$, so so it admits two solutions (for
each $s$): $$P(s)=\frac{1\pm\sqrt{1-4pqs^2}}{2qs}.$$ One of the two
solutions is always greater than $1$ in absolute value, so it cannot
correspond to a value of a generating function. Therefore,
$$P(s)= \frac{1-\sqrt{1-4pqs^2}}{2qs},\text{ for } \abs{s}\leq
\frac{1}{2\sqrt{pq}}.$$ It remains to extract the information about
$\seqz{p}$ from $P$. We will not derive expressions for all $p_{n}$ (but
see the last problem in the Problems section), because we do not need
to. We can get very useful information from $P$ itself.

#### Do we actually hit $1$ sooner or later?

What happens if we try to evaluate $P(1)$? We should get $1$, right? In
fact, what we get is the following: $$P(1)=
\frac{1-\sqrt{1-4pq}}{2q}=\frac{1-\abs{p-q}}{2q}=
\begin{cases}
1, & p\geq \tot\\ \frac{p}{q}, & p<\tot 
\end{cases}$$ Clearly, $P(1)<1$ when $p<q$. The explanation is simple -
the random walk may fail to hit the level $1$ *at all*, if $p<q$. In
that case $P(1)=\sum_{k=0}^{\infty} p_k=\PP[T_1<\infty]<1$, or,
equivalently, $\PP[T_1=+\infty]>0$. It is remarkable that if $p=\tot$,
the random walk *will* always hit $1$ sooner or later, but this does not
need to happen if $p<\tot$. What we have here is an example of a
phenomenon known as *criticality*: many physical systems exhibit
qualitatively different behavior depending on whether the value of
certain parameter $p$ lies above or below certain *critical value*
$p=p_c$.

#### Expected time until we hit $1$?

Another question that generating functions can help answer is the
following one: how long, on average, do we need to wait before $1$ is
hit? When $p<\tot$, $\PP[T_1=+\infty]>0$, so we can immediately conclude
that $\EE[T_1]=+\infty$, by definition. The case $p\geq \tot$ is more
interesting. Following the recipe from the lecture on generating
functions, we compute the derivative of $P(s)$ and get $$P'(s)=
\frac{2p}{\sqrt{1-4pqs^2}}-\frac{1-\sqrt{1-4pqs^2}}{2qs^2}.$$

-   When $p=\tot$, we get
    $$\lim_{s\nearrow 1} P'(s)= \lim_{s\nearrow 1} \Big( 
    \frac{1}{\sqrt{1-s^2}} - \frac{1-\sqrt{1-s^2}}{s^2}\Big)=+\infty,$$
    and conclude that $\EE[T_1]=+\infty$.

-   For $p>\tot$, the situation is less severe:
    $$\lim_{s\nearrow 1} P'(s)= \frac{1}{p-q}.$$

We can summarize the situation in the following table
$$\begin{array}{c||c|c}
 & \PP[T_1<\infty] & \EE[T_1] \\\hline\hline
p<\tabfrac{1}{2} & \tabfrac{p}{q} & +\infty \\\hline
p=\tabfrac{1}{2} & 1           & +\infty \\\hline
p>\tabfrac{1}{2} & 1           & \tabfrac{1}{p-q}
\end{array}$$

Problems
========

Either one of the following $4$ random times is a stopping time for a
simple random walk $\seqz{X}$, or they all are. Choose the one which is
not in the first case, or choose (e) if you think they all are.

\sol{
  The correct answer is (e); first, second, or \dots hitting
    times are stopping times, and so are their minima or maxima. Note
    that for two stopping times $T_1$ and $T_2$, the one that happens
    {\em first} is $\min(T_1,T_2)$ and the one that happens {\em last}
    is $\max(T_1,T_2)$.}
\medskip
Let $T_3$ be the hitting time of the level $3$ for a simple biased
random walk $\seqz{X}$ with $p=2/3$. Then

\sol{ The correct answer is (a) since $T_3$ is the sum of three independent random variables
each of which has the same distribution as $T_1$ (see table in the
notes).}
\medskip
The generating function $P_{T_{-1}}$ of the first hitting time $T_{-1}$
of the level $-1$ for the simple biased random walk with $p=\PP[X_1=1]$
satisfies $P_{T_{-1}}(s)=ps + qs P_{T_{-1}}(s)^2$,
$P_{T_{-1}}(s)=qs + ps P_{T_{-1}}(s)^2$,
$P_{T_{-1}}(s)=ps - qs P_{T_{-1}}(s)^2$,
$P_{T_{-1}}(s)=ps + qs P_{T_{-1}}(s)^{-2}$, none of the above.

\sol{
  The correct answer is (b); hitting $-1$ is the same as hitting
    $1$, if you switch the roles of $p$ and $q$.}
\medskip
Let $\seqz{X}$ be a simple biased random walk with $p=\PP[X_1=1]$, and
Let $T_l$ denote the first hitting time of the level $l$, and $P_{T_l}$
denote its generating function. Then
$P_{T_{2}}(s)=ps + qs P_{T_{2}}(s)^4$,
$P_{T_{2}}(s)=ps^2 + qs P_{T_{2}}(s)$, $P_{T_{2}}(s)=P_{T_{1}}(s)$,
$P_{T_{2}}(s)=P_{T_{1}}(s)^2$, none of the above.

\sol{
  The correct answer is (d); to hit $2$, you must first hit $1$
    and then hit $2$, starting from $1$. The times between these two
    are independent, and distributed like $T_1$. }
\medskip
If $P_{T_a}$ denotes the generating function of the first hitting time
$T_{a}$ of the level $a$ for the simple symmetric random walk, then
$P_{T_{2}}(s)=\tot s P_{T_3}(s) + \tot s P_{T_{1}}(s)$,
$P_{T_{2}}(s)=\tot  P_{T_3}(s) + \tot  P_{T_{1}}(s)$,
$P_{T_{2}}(s)= s P_{T_3}(s) P_{T_{1}}(s)$,
$P_{T_{2}}(s)=\oo{3}( s + P_{T_3}(s) + \tot  P_{T_{1}}(s))$, none of the
above.

\sol{The correct answer is (a). 
  If the first step is ``down'', the random
  walk will need to reach the level $2$ from level $-1$. Let us denote that
  random time by $T'_3$. We know that $T'_3$ is independent of the first
  step, and has the same distribution as $T_3$. 
  
  On the other hand, if the first step is ``up'', the random
  walk will need to reach the level $2$ from level $1$. Let us denote that
  random time by $T'_1$.  Like above, its distribution is the same as that
  of $T_1$ and it is independent of the first step.

  Therefore, 
  \[ T_2 = 
  \begin{cases}
    1+T'_3, & X_1={-1},\\
    1+T'_1, & X_1=1
\end{cases}\]
Therefore,
  \begin{align*}
    P_{T_2}(s) = \EE[ s^{T_2}] &= \EE[ s^{1+T'_3} | X_1=-1] \PP[ X_1=-1]\\ &
    \quad +
  \EE[ s^{1+T'_1} | X_1=1] \PP[ X_1=1]\\
                               &= \tot (s \EE[s^{T'_3}] + s
                               \EE[s^{T'_1}])\\
                               &= \tot ( s P_{T'_3}(s) + s P_{T'_1}(s)).\\
                               &= \tot ( s P_{T_3}(s) + s P_{T_1}(s)).
  \end{align*}  
}
\medskip
(\*) The purpose of this problem is to derive explicit expressions for
the probabilities $p_n = \PP[T_1=n]$, where $T_1$ is the first hitting
time of the level $1$ for a simple biased random walk $\seqz{X}$ with
$p=\PP[X_1=1]$. Our starting point is the expression
$$P(s) = \oo{2 q s} \Big(1 - \sqrt{1-4 p q s^2} \Big)$$ for the
generating function $P(s) = P_{T_1}(s)$ derived in the notes.

1.  Consider the function $f(x) = \sqrt{1-x}$. Find an expression for
    the its $n$-th derivative $f^{(n)}(0)$ at $x=0$.

2.  Use the above to expand $P(s)$ in a power-series expansion around
    $s=0$ and write down the expressions for the coefficients $p_n$,
    $n\in\N_0$.

\sol{
\begin{enumerate}
\item $f(x) = (1-x)^{1/2}$, so 
\begin{align*}
f^{(0)}(x) &= (1-x)^{1/2} \\
f^{(1)}(x) &= -\tot (1-x)^{-1/2}\\
f^{(2)}(x) &= -\tot \times \tot (1-x)^{-3/2}\\
f^{(3)}(x) &= -\tot \times \tot \times \tf{3}{2} (1-x)^{-5/2}\\
f^{(4)}(x) &= -\tot \times \tot \times \tf{3}{2} \times \tf{5}{2}  (1-x)^{-7/2}
\end{align*}
The pattern continues; for $n\geq 1$, 
\[ f^{(n)}(x) =  - 2^{-n} (2n-3)!!
(1-x)^{-(2n-1)/2},\]
where $(2n-3)!! = 1\times 3 \times 5 \times \dots \times (2n-3)$, and we
set $(-1)!! = (1)!! = 1$ to be able to use the same formula for all cases. 
Therefore,
\[ \oo{n!} f^{(n)}(0)  = - \tf{(2n-3)!!}{2^n n!} = -\tf{ (2n-3)!!}{2^n n!}.\]
\item We now know that
\[ \sqrt{1-x} = 1 - \sum_{n=1}^{\infty}  x^n \tf{ (2n-3)!!}{2^n n!},\]
is the Taylor-series expansion of $\sqrt{1-x}$ around $x=0$. We plug in $x=
4 p q s^2$ to get
\[ 1 - \sqrt{1-4 p q s^2} = \sum_{n=1}^{\infty} s^{2n} (4pq)^n \tf{(2n-3)!!}{2^n n!},\]
so that
\[ P(s) = \oo{2 q s} (1-\sqrt{1-4 p q s^2}) = \sum_{n=1}^{\infty}
 s^{2n-1} p^{n} q^{n-1} \tf{ 2^{n-1} (2n-3)!!}{n!}.\]
 It follows that $p_{2n}=0$ for $n\in\Nz$ and 
 \[ p_{2n-1} = p^n q^{n-1} \frac{ 2^{n-1} (2n-3)!!}{n!} \efor n\geq 1.\]
\end{enumerate} 
}
\medskip
