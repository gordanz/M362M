```{r child="mydefs.Rmd"}
```
# More about Random Walks
<div style="counter-reset: thechapter 4;"> </div>

## The reflection principle


Counting trajectories in order to compute probabilities is a powerful method, 
as our next  example shows. It also reveals a potential
weakness of the combinatorial approach: it works best when all $\omega$
are equally likely (i.e., when $p=\tot$ in the case of the random walk).

We start by asking a simple question; what is the typical record value
of the random walk, i.e., how far "up" (or "right" depending on your point of view) 
does it typically get? Clearly,
the largest value it can attain is $T$. This happens only when 
all coin tosses came up $+1$, an extremely unlikely event - 
its probability is $2^{-T}$. On the other hand, this maximal value is at least $0$, since
$X_0=0$, already. A bit of thought reveals that any value between those
two extremes is possible, but it is not at all easy to compute their
probabilities.

More precisely, if $\{X_n\}$ is a simple random walk with time horizon
$T$. We define its **running-maximum process** $\seqz{M}$ by
$$M_n=\max(X_0,\dots, X_n),\ \efor 0 \leq n \leq T,$$ 
and ask what the probabilities $\PP[M_n = k]$ for $k=0,\dots, n$ are? 
An easy numerical solution to this problem can given by simulation. We reuse the function 
`simulate_walk` defined at the beginning of the chapter, but also employ a new function, called `apply` which "applies" a function to each row (or column) of a data frame or a matrix. It seems to be tailor-made for our purpose^[the function `apply` is often used as a substitute for a `for` loop because it has  several advantages over it. First, the code is much easier to read and understand. Second, `apply` can easily be parallelized. Third, while this is not such a big issue anymore, `for` loops used to be orders of magnitude slower than the corresponding `apply` in the past. R's `for` loops got much better recently, but they still lag behind `apply` in some cases. To be fair, `apply` is know to use more *memory* than `for` in certain cases.] because we want to compute the maximum of each row of the simulation matrix (remember - the row means  keep the realization fixed, but vary the time-index $n$). The syntax of `apply` is simple - it needs the data frame, the margin (rows are coded as 1 and columns as 2) and the function to be applied (`max` in our case). The output is a vector of size `nsim` with all row-wise maxima:

```{r echo=-(1:3)}
set.seed(1232)
single_trajectory = function(T, p=0.5) {
    delta = sample(c(-1,1), size=T, replace=TRUE, prob=c(1-p,p))
    x = cumsum(delta)
    return(x)
}
simulate_walk = function(nsim, T, p = 0.5) {
  return(
    data.frame(
      X0 = 0,
      t(replicate(nsim, single_trajectory(T, p)))
    ))
}
walk = simulate_walk(nsim=100000, T=12, p=0.5)
M = apply(walk, 1, max)
hist(M, breaks=seq(-0.5, 12.5, 1), probability = TRUE)
```

The overall shape of the distribution is as we expected; the support is $\{0,1,2,\dots, 12\}$ and the 
probabilities tend to decrease as $k$ gets larger. The unexpected feature is that $\PP[ M_{12} = 1]$ seems
to be the same as $\PP[ M_{12} = 2]$. It drops after that for $k=3$, but it looks like 
$\PP[ M_{12} = 3] = \PP[ M_{12}=4]$ again. Somehow the probability does not seem to change at 
all from $2i-1$ to $2i$. 

Fortunately, there is an explicit formula for the distribution of $M_n$ and we can derive it
by a nice counting trick known as **the reflection principle**. 



As usual, we may assume without loss of generality that $n=T$ since the
values of $\delta_{n+1}, \dots, \delta_T$ do not affect $M_n$ at all.
We start by picking a level $l\in\set{1,\dots, n}$ and first compute
the probability $\PP[M_n\geq l]$ - it will turn out to be easier than
attacking $\PP[ M_n=l]$ directly. The symmetry assumption $p=1/2$ ensures that
all trajectories are equally likely, so we can do this by counting the
number of trajectories whose maximal level reached is at least $l$, and
then multiply by $2^{-n}$.

What makes the computation of $\PP[M_n \geq l]$ a bit easier than that
of $\PP[ M_n = l]$ is the following equivalence

$$M_n\geq l \text{ if and only if } X_k=l \text{ for some } k.$$

In words, the set of trajectories whose maximum is at least $l$ is
exactly the same as the set of trajectories that hit the level $l$ at
some time. Let us denote the set of trajectories $\omega$ with this property by
$A_l$, so that $\PP[ M_n \geq l] = \PP[A_l]$. 
We can further split $A_l$ into three disjoint events $A_l^{>}$, 
$A_l^{=}$ and $A_l^{<}$, depending on whether $X_n<l$, $X_n=l$ or $X_n>l$.
In the picture below, the red trajectory is in $A_l^{>}$, the green trajectory in $A_l^=$ 
 the orange one in $A_l^{<}$, while the blue one is not in $A_l$ at all. 

<center>

```{r echo=FALSE, out.width="80%"}
library(ggplot2)

omegaequal = c(0,-1,0,1,2,3,4,5,4,5,4,3)
omegagreater = c(0,1,2,3,4,5,6,5,4,3,4,5)
omegaless = c( 0,1,0,1,0,1,2,3,2,1,0,-1)
omeganot = c(0,1,2,1,0,1,0,-1,0,1,2,1)

ggplot()+
  geom_line(aes(x=0:11,y=omegaequal), color="darkgreen", size=1, alpha=0.6) +
  geom_point(aes(x=0:11,y=omegaequal), color="black") +
  geom_line(aes(x=0:11,y=omegaless), color="orange3", size=1, alpha=0.6) +
  geom_point(aes(x=0:11,y=omegaless), color="black") +
  geom_line(aes(x=0:11,y=omegagreater), color="darkred", size=1, alpha=0.6) +
  geom_point(aes(x=0:11,y=omegagreater), color="black") +
  geom_line(aes(x=0:11,y=omeganot), color="darkblue", size=1, alpha=0.6) +
  geom_point(aes(x=0:11,y=omeganot), color="black") +
  geom_line(aes(x=c(-0, 11),y=3), color="Brown", linetype="dashed") +
  geom_text(aes(x = 11.5, y=5, label="X[n]>l"),family="serif", parse=TRUE)+
  geom_text(aes(x = 11.5, y=3, label=paste("X[n] == l")),family="serif", parse=TRUE)+
  geom_text(aes(x = 11.5, y=-1, label="X[n]<l"),family="serif", parse=TRUE)+
  xlab("time")+ylab("")+
  theme_bw()+scale_x_continuous(breaks=0:11)+scale_y_continuous(breaks=-1:6)

```

</center>

With the set of all trajectories $\Omega$ partitioned into four disjoint classes, namely $A^>_l, A^=_l, A^<_l$ and $(A_l)^c$, we are ready to reveal the main idea behind the reflection principle:

<center style="margin-bottom: 20px;">
$A_l^<$ and $A_l^>$ have exactly the same number of elements, i.e., $\# A^>_l = \# A_l^<$.
</center>


To see why that is true, start by choosing a trajectory $\omega\in A_l^{>}$ and denoting by
$\tau_l(\omega)$ the *first time* $\omega$ visits the
level $l$. Since $\omega \in A^>$ such a time clearly exists. 
Then we associate to $\omega$ another trajectory, call it $\bar{\omega}$, obtained from $\omega$
in the following way:

1. $\bar{\omega}$ and $\omega$ are the same until the time $\tau_l(\omega)$. 
2. After that, $\bar{\omega}$ is the reflection of $\omega$ around the level $l$. 

Equivalently the increments of $\omega$ and $\bar{\omega}$ are exactly the same up to time $\tau(\omega)$, and exactly the opposite afterwards. In the picture below - the orange trajectory is $\omega$ and the green trajectory is its 
"reflection" $\bar{\omega}$; note that they overlap until time $5$:

<center>

```{r echo=FALSE, out.width = "80%"}
library(ggplot2)
library(latex2exp)

omegagreen = c(0,1,2,1,2,3,4,5,4,3,2,1)
omegaorange = c(0,1,2,1,2,3,2,1,2,3,4,5)

ggplot()+
  geom_line(aes(x=0:11,y=omegagreen), color="darkgreen", size=1, alpha=0.6) +
  geom_point(aes(x=0:11,y=omegagreen), color="black") +
  geom_line(aes(x=0:11,y=omegaorange), color="orange3", size=1, alpha=0.6) +
  geom_point(aes(x=0:11,y=omegaorange), color="black") +
  geom_line(aes(x=c(-0, 11),y=3), color="Brown", linetype="dashed") +
  geom_line(aes(x=c(5, 5),y=c(0.25,3)), color="gray", linetype="dotdash") +
  geom_text( aes(x = 5, y=0, label=paste(TeX("$\\tau_l(\\omega)")) ),family="serif", parse=TRUE)+
  geom_text(aes(x = 11.5, y=5, label=paste("omega")),size = 5, family="serif", parse=TRUE)+
  geom_text(aes(x = 11.5, y=1, label=paste("bar(omega)")),size = 5, family="serif", parse=TRUE)+
  xlab("time")+ylab("")+
  theme_bw()+scale_x_continuous(breaks=0:11)+scale_y_continuous(breaks=0:6)

```

</center>

Convince yourself that this procedure establishes
a bijection between the sets $A_l^{>}$ and $A_l^{<}$, making these two
sets equal in size. 

So why is it important to know that $\# A_l^> = \# A_l^<$? Because the trajectories in 
$A_l^>$ (as well as in $A_l^=$) are easy to count. 
For them, the requirement that the level 
$l$ is hit at a certain point is redundant; if you are at or above $l$
at the very end, you must have hit $l$ at a certain point.  
Therefore, $A_l^{>}$ is simply the family of those trajectories 
$\omega$ whose final positions $X_n(\omega)$ are somewhere strictly above $l$. Hence,
\begin{align}
       \PP[A_l^{>}] &= \PP[ X_n=l+1 \text{ or } X_n = l+2 \text{ or } \dots \text{ or }
     X_n=n]\\ & = \sum_{k=l+1}^n \PP[X_n = k]
\end{align}

Similarly,      $$\begin{aligned}
     \PP[ A_l^{=}] = \PP[X_n=l].\end{aligned}$$ 
Finally, by the reflection principle, 
$$\begin{aligned}
    \PP[ A_l^{<}] = \PP[A_l^{>}] = \sum_{k=l+1}^n \PP[X_n=k].\end{aligned}$$

Putting all of this
together, we get $$\begin{aligned}
    \PP[ A_l ] = \PP[ X_n=l] + 2 \sum_{k=l+1}^n \PP[X_n=k],\end{aligned}$$
so that $$\begin{aligned}
    \PP[ M_n = l ] &= \PP[ M_n \geq l] - \PP[ M_n \geq l+1]\\ & = \PP
    [A_l] - \PP
    [A_{l+1}]\\ & =
    \PP[ X_n = l] + 2 \PP[X_n = l+1] + 2\PP[X_n = l+2]+ \dots + 2\PP[ X_n=n] -\\
    & \qquad \qquad  \quad \  -
    \PP[ X_n = l+1] - 2 \PP[X_n = l+2] - \dots - 2\PP[ X_n=n]\\
    &= \PP[ X_n=l] + \PP[X_n=l+1]
    \end{aligned}$$
    
Now that we have the explicit expression
$$ \PP[ M_n = l ] = \PP[ X_n=l] + \PP[X_n = l+1] \text{ for } l=0,1,\dots, n,$$
we can shed some light on the fact on the shape of the histogram for $M_n$ we plotted above. 
Since $\PP[X_n=l]$ is $0$ if $n$ and $l$ don't have the same parity, it is clear that only
one of the probabilities $\PP[X_n=l]$ and $\PP[X_n=l+1]$ can be positive. It follows that, for
$n$ even, we have
\begin{align}
\PP[ M_n =0] &= \PP[X_n=0] + \PP[X_n=1] = \PP[X_n=0]\\
\PP[M_n=1] &= \PP[ X_n=1] + \PP[X_n=2] = \PP[X_n=2]\\
\PP[M_n=2] &= \PP[ X_n=2] + \PP[X_n=3] = \PP[X_n=2]\\
\PP[M_n=3] &= \PP[ X_n=3] + \PP[X_n=4] = \PP[X_n=4]\\
\PP[M_n=4] &= \PP[ X_n=4] + \PP[X_n=5] = \PP[X_n=4] \text{ etc.}
\end{align}
In a similar way, for $n$ odd, we have
\begin{align}
\PP[ M_n =0] &= \PP[X_n=0] + \PP[X_n=1] = \PP[X_n=1]\\
\PP[M_n=1] &= \PP[ X_n=1] + \PP[X_n=2] = \PP[X_n=1]\\
\PP[M_n=2] &= \PP[ X_n=2] + \PP[X_n=3] = \PP[X_n=3]\\
\PP[M_n=3] &= \PP[ X_n=3] + \PP[X_n=4] = \PP[X_n=3]\\
\PP[M_n=4] &= \PP[ X_n=4] + \PP[X_n=5] = \PP[X_n=5] \text{ etc.}
\end{align}

Here is a example of a typical problem where the reflection principle (i.e., the formula for $\PP[M_n=k]$) is used:

<div class="problem">
Let $X$ be a simple symmetric random walk. 
  What is the probability that $X_n\leq 0$ for all $0\leq n \leq T$? 
</div>

<div class="solution">

  This is really a question about the maximum, but in disguise.  The walk will stay negative or $0$ if and only if its
  its running maximum $M_T$ at time $T$ takes the value $0$. By our formula for $\PP[M_n=l]$ we have
  $$ \PP[M_T=0] = \PP[X_T=0] + \PP[X_T = 1].$$
When $T=2N$ is even this evaluates to $\binom{2N}{N} 2^{-2N}$, and when $T=2N-1$ to
$\binom{2N-1}{N} 2^{-(2N-1)}$. 
</div>

<div class="problem">
  What is the probability that a simple symmetric random walk will reach the level $l=1$ in $T$ steps or fewer?
  What happens when $T\to\infty$?
</div>

<div class="solution">The first question is exactly the opposite of the question in our previous example, so the answer is
  $$ 1 - \PP[M_T=0] = 1- \PP[X_T=0] - \PP[X_T=1].$$
  As above, this evaluates to $\binom{2N}{N} 2^{-2N}$ when $T=2N$ is even (we skip the case of odd $T$ because it is very similar). 
  When $N\to\infty$, we expect $\binom{2N}{N}$ to go to $+\infty$ and $2^{-2N}$ to go to 
  $0$, so it is not immediately clear which term will win. 
  One way to make a guess is to think about it probabilistically: we are looking at the 
  probability $\PP[X_{2N}=0]$ that the random walk takes the 
  value $0$ after exactly $2N$ steps. Even though no  other (single) value is more 
  likely to happen, there are so many other values $X_{2N}$ could take (anything 
  from $-2N$ to $2N$ except for $0$) that we conjecture that
  its probability converges to $0$. A  formal mathematical argument which proves that
  our conjecture is, indeed correct, involves **Stirling's formula**:
  
  $$ N! \sim \sqrt{2 \pi N} \left( \tf{N}{e} \right)^N \text{ where }
   A_N \sim B_N \text{ means that } \lim_{N\to\infty} \tf{A_N}{B_N}=1. $$
  
  We write $\binom{2N}{N} = \tfrac{(2N)!}{N! N!}$ and apply Stirling's formula to each factorial (let's skip the details)
  to conclude that 
  $$ 
  \binom{2N}{N} 2^{-2n}\sim \frac{1}{\sqrt{N \pi}} 
  \text{ so that }  \lim_{N\to\infty}
  \binom{2N}{N} 2^{-2n}
  = 0 $$
</div>

The result of the previous problem implies the following important fact:

> The simple symmetric random walk will reach the level $1$, 
> with certainty, given enough time.

Indeed, we just proved that the probability of this not happening during the first $T$ steps
shrinks down to $0$ as $T\to\infty$.

But wait, there is more! By symmetry, the level $1$ can be replaced by $-1$. Also, once we hit
$1$, the random walk "renews itself" (this property is called the Strong
Markov Property and we will talk about it later), so it will eventually
hit the level $2$, as well. Continuing the same way, we get the
following remarkable result

> **Sooner or later, the symple symmetric random walk will visit any level.** 


We close this chapter with an application of the reflection principle
to a classical problem in probability and combinatorics. Feel free to skip it
if you want to. 

<div class="problemec">
Suppose that two
candidates, Daisy and Oscar, are running for office, and $T \in\N$
voters cast their ballots. Votes are counted the old-fashioned way,
namely by the same official, one by one, until all $T$ of them have been
processed. After each ballot is opened, the official records the number
of votes each candidate has received so far. At the end, the official
announces that Daisy has won by a margin of $k>0$ votes, i.e., that
Daisy got $(T+k)/2$ votes and Oscar the remaining $(T-k)/2$ votes. What
is the probability that at no time during the counting has Oscar been in
the lead?
</div>

<div class="solution">
We assume that the order in which the official counts the votes is
completely independent of the actual votes, and that each voter chooses
Daisy with probability $p\in (0,1)$ and Oscar with probability $q=1-p$.
We don't know a-priori what $p$ is, and, as it turns out, we don't need to!

For $0 \leq n \leq T$, let $X_n$ be the number of votes received by
Daisy *minus* the number of votes received by Oscar in the first $n$
ballots. When the $n+1$-st vote is counted, $X_n$ either increases by
$1$ (if the vote was for Daisy), or decreases by 1 otherwise. The votes
are independent of each other and $X_0=0$, so $X_n$, $0\leq n \leq T$ is
a simple random walk with the time horizon $T$. The probability of an
up-step is $p\in (0,1)$, so this random walk is not necessarily
symmetric. The ballot problem can now be restated as follows:

*For a simple random walk $\set{X_n}_{0\leq n \leq T}$, what is the
probability that $X_n\geq 0$ **for all** $n$ with $0\leq n \leq T$, given that
$X_T=k$?*

The first step towards understanding the solution is the realization
that the exact value of $p$ does not matter. Indeed, we are interested
in the conditional probability $\PP[ F|G]=\PP[F\cap G]/\PP[G]$, where
$F$ denotes the set of $\omega$ whose corresponding trajectories always
stay non-negative, while the trajectories corresponding to $\omega\in G$
reach $k$ at time $n$. Each $\omega \in G$ consists of exactly $(T+k)/2$
up-steps ($1$s) and $(T-k)/2$ down steps ($-1$s), so its probability
weight is equal to $p^{ (T+k)/2} q^{(T-k)/2}$. Therefore, with $\# A$
denoting the number of elements in the set $A$, we get $$\begin{aligned}
 \PP[ F|G]=\frac{\PP[F\cap G]}{\PP[G]}=\frac{\# (F\cap G) \ p^{
    (T+k)/2} q^{(T-k)/2}}{ \# G \ p^{ (T+k)/2}
  q^{(T-k)/2}}=\frac{\#(F\cap G)}{\# G}.\end{aligned}$$ This is quite
amazing in and of itself. This conditional probability does not depend
on $p$ at all!

\medskip
Since we already know how to count the number of elements in $G$ (there
are $\binom{T}{(T+k)/2}$), "all" that remains to be done is to count the
number of elements in $G\cap F$. The elements in $G \cap F$ form a
portion of all the elements in $G$ whose trajectories don't hit the
level $l=-1$; this way, $\#(G\cap F)=\#G-\#H$, where $H$ is the set of
all paths which finish at $k$, but cross (or, at least, touch) the level
$l=-1$ in the process. Can we use the reflection principle to find
$\# H$? Yes, we can. In fact, you can convince yourself that the
reflection of any trajectory corresponding to $\omega \in H$ around the
level $l=-1$ after its last hitting time of that level produces a
trajectory that starts at $0$ and ends at $-k-2$, and vice versa. The
number of paths from $0$ to $-k-2$ is easy to count - it is equal to
$\binom{T}{(T+k)/2+1}$. Putting everything together, we get
$$\PP[ F|G]=\frac{\binom{T}{n_1}-\binom{T}{n_1+1}}
{\binom{T}{n_1}}=\frac{k+1}{n_1+1},\text{ where }n_1=\frac{T+k}{2}.$$
The last equality follows from the definition of binomial coefficients
$\binom{T}{i}=\frac{T!}{i!(T-i)!}$.

The Ballot problem has a long history (going back to at least 1887) and
has spurred a lot of research in combinatorics and probability. In fact,
people still write research papers on some of its generalizations. When
posed outside the context of probability, it is often phrased as "*in
how many ways can the counting be performed ...*" (the difference being
only in the normalizing factor $\binom{T}{n_1}$ appearing in Example
above). A special case $k=0$ seems to be even
more popular - the number of $2n$-step paths from $0$ to $0$ never going
below zero is called the **$n$-th Catalan number** and equals 
 \begin{align}
   C_n=\frac{1}{n+1} \binom{2n}{n}.
 \end{align}
</div>

## Stopping times

COMING SOON

## Wald's formula and Gambler's ruin

COMING SOON

## Additional problems for Chapter 4

<!--
  3-max-problems
  ------------------------------------------------
-->
<div class="problem">
```{r child="problems/01_Random_Walks/3-max-problems_prb.Rmd"}
```
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/01_Random_Walks/3-max-problems_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->


<!--
  time_until_hit
  ------------------------------------------------
-->
<div class="problem">
```{r child="problems/01_Random_Walks/time_until_hit_prb.Rmd"}
```
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/01_Random_Walks/time_until_hit_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->



<!-- 
  Luke_cookies
  -------------------------------
-->
<div class="problem">
```{r child="problems/01_Random_Walks/Luke_cookies_prb.Rmd"}
```
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/01_Random_Walks/Luke_cookies_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->



<!--
  catalan
  ------------------------------------------------
-->
<div class="problemec">
```{r child="problems/01_Random_Walks/catalan_prb.Rmd"}
```
</div>
<!-- <div class="solution"> -->
<!-- ```{r child="problems/01_Random_Walks/catalan_sol.Rmd"} -->
<!-- ``` -->
<!-- </div> -->


⬇︎ In case you were wondering, the text below belongs to footnotes from somewhere high above.⬇︎

